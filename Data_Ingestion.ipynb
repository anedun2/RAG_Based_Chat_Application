{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Python Notebook parses the PDF files from the URLs; uses LlamaIndex to index the parsed content and store the index locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import PyPDF2\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing_extensions import Protocol\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import GPTVectorStoreIndex\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenAI API Key Authentication (The OpenAI API Key will be stored in the config.py file)\n",
    "openai.api_key = config.openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                                URL  \\\n",
      "0   1  https://dl.acm.org/doi/pdf/10.1145/3397271.340...   \n",
      "1   2               https://arxiv.org/pdf/2104.07186.pdf   \n",
      "2   3               https://arxiv.org/pdf/2106.14807.pdf   \n",
      "3   4               https://arxiv.org/pdf/2301.03266.pdf   \n",
      "4   5               https://arxiv.org/pdf/2303.07678.pdf   \n",
      "\n",
      "                                     Research_Papers  \n",
      "0  ColBERT: Efficient and Effective Passage Searc...  \n",
      "1  COIL: Revisit Exact Lexical Match in Informati...  \n",
      "2  A Few Brief Notes on DeepImpact, COIL, and a C...  \n",
      "3  Doc2Query--: When Less is More\\nMitko Gospodin...  \n",
      "4  Query2doc: Query Expansion with Large Language...  \n"
     ]
    }
   ],
   "source": [
    "#Parse PDF files from URLs\n",
    "\n",
    "#Function to parse the PDF Files from URLs\n",
    "def parse_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    with open('temp.pdf', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    pdf_file = open('temp.pdf', 'rb')\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "    text = \"\"\n",
    "    for page_number in range(num_pages):\n",
    "        page = pdf_reader.pages[page_number] \n",
    "        text += page.extract_text()\n",
    "\n",
    "    pdf_file.close()\n",
    "    return text\n",
    "\n",
    "#List of URLs where PDF Files should be extracted\n",
    "urls = [\n",
    "    \"https://dl.acm.org/doi/pdf/10.1145/3397271.3401075\",\n",
    "    \"https://arxiv.org/pdf/2104.07186.pdf\",\n",
    "    \"https://arxiv.org/pdf/2106.14807.pdf\",\n",
    "    \"https://arxiv.org/pdf/2301.03266.pdf\",\n",
    "    \"https://arxiv.org/pdf/2303.07678.pdf\"\n",
    "]\n",
    "\n",
    "i = 0\n",
    "data = []\n",
    "\n",
    "#The for loop parses each PDF File and stores it a dataframe\n",
    "for url in urls:\n",
    "    i+=1\n",
    "    pdf_text = parse_pdf(url)\n",
    "    data.append({'id': i ,'URL': url, 'Research_Papers': pdf_text})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the parsed PDFs to a CSV file\n",
    "df.to_csv('research_papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Data Loader from Llamahub( https://llamahub.ai/ )  to load the csv file \n",
    "PandasCSVReader = download_loader(\"PandasCSVReader\")\n",
    "\n",
    "\n",
    "loader = PandasCSVReader()\n",
    "docs = loader.load_data(file=Path('research_papers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='d3ebb839-f393-431d-b500-d1ce023cf399', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1', text='0, 1, https://dl.acm.org/doi/pdf/10.1145/3397271.3401075, ColBERT: Efficient and Effective Passage Search via\\nContextualized Late Interaction over BERT\\nOmar Khattab\\nStanford University\\nokhattab@stanford.eduMatei Zaharia\\nStanford University\\nmatei@cs.stanford.edu\\nABSTRACT\\nRecent progress in Natural Language Understanding (NLU) is driv-\\ning fast-paced advances in Information Retrieval (IR), largely owed\\nto fine-tuning deep language models (LMs) for document ranking.\\nWhile remarkably effective, the ranking models based on these LMs\\nincrease computational cost by orders of magnitude over prior ap-\\nproaches, particularly as they must feed each query–document pair\\nthrough a massive neural network to compute a single relevance\\nscore. To tackle this, we present ColBERT, a novel ranking model\\nthat adapts deep LMs (in particular, BERT) for efficient retrieval.\\nColBERT introduces a late interaction architecture that indepen-\\ndently encodes the query and the document using BERT and then\\nemploys a cheap yet powerful interaction step that models their fine-\\ngrained similarity. By delaying and yet retaining this fine-granular\\ninteraction, ColBERT can leverage the expressiveness of deep LMs\\nwhile simultaneously gaining the ability to pre-compute document\\nrepresentations offline, considerably speeding up query processing.\\nCrucially, ColBERT’s pruning-friendly interaction mechanism en-\\nables leveraging vector-similarity indexes for end-to-end retrieval\\ndirectly from millions of documents. We extensively evaluate Col-\\nBERT using two recent passage search datasets. Results show that\\nColBERT’s effectiveness is competitive with existing BERT-based\\nmodels (and outperforms every non-BERT baseline), while exe-\\ncuting two orders-of-magnitude faster and requiring up to four\\norders-of-magnitude fewer FLOPs per query.\\nKEYWORDS\\nNeural IR; Efficiency; Deep Language Models; BERT\\nACM Reference Format:\\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Pas-\\nsage Search via Contextualized Late Interaction over BERT. In Proceedings of\\nthe 43rd International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM,\\nNew York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.3401075\\n1 INTRODUCTION\\nOver the past few years, the Information Retrieval (IR) community\\nhas witnessed the introduction of a host of neural ranking models,\\nincluding DRMM [ 7], KNRM [ 4,36], and Duet [ 20,22]. In contrast\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n©2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\\nhttps://doi.org/10.1145/3397271.3401075\\n0.15 0.20 0.25 0.30 0.35 0.40\\nMRR@10101102103104105Query Latency (ms)\\nBM25doc2queryKNRMDuet\\nDeepCTfT+ConvKNRM\\ndocTTTTTqueryBERT-baseBERT-large\\nColBERT (re-rank)ColBERT (full retrieval)Bag-of-Words (BoW) Model\\nBoW Model with NLU Augmentation\\nNeural Matching Model\\nDeep Language Model\\nColBERT (ours)Figure 1: Effectiveness (MRR@10) versus Mean Query La-\\ntency (log-scale) for a number of representative ranking\\nmodels on MS MARCO Ranking [24]. The figure also shows\\nColBERT. Neural re-rankers run on top of the official BM25\\ntop-1000 results and use a Tesla V100 GPU. Methodology and\\ndetailed results are in §4.\\nto prior learning-to-rank methods that rely on hand-crafted fea-\\ntures, these models employ embedding-based representations of\\nqueries and documents and directly model local interactions (i.e.,\\nfine-granular relationships) between their contents. Among them,\\na recent approach has emerged that fine-tunes deep pre-trained\\nlanguage models (LMs) like ELMo [ 29] and BERT [ 5] for estimating\\nrelevance. By computing deeply-contextualized semantic repre-\\nsentations of query–document pairs, these LMs help bridge the\\npervasive vocabulary mismatch [ 21,42] between documents and\\nqueries [ 30]. Indeed, in the span of just a few months, a number\\nof ranking models based on BERT have achieved state-of-the-art\\nresults on various retrieval benchmarks [ 3,18,25,39] and have\\nbeen proprietarily adapted for deployment by Google1and Bing2.\\nHowever, the remarkable gains delivered by these LMs come\\nat a steep increase in computational cost. Hofstätter et al. [9] and\\nMacAvaney et al. [18] observe that BERT-based models in the lit-\\nerature are 100-1000 ×more computationally expensive than prior\\nmodels—some of which are arguably notinexpensive to begin with\\n[13]. This quality–cost tradeoff is summarized by Figure 1, which\\ncompares two BERT-based rankers [ 25,27] against a representative\\nset of ranking models. The figure uses MS MARCO Ranking [ 24],\\na recent collection of 9M passages and 1M queries from Bing’s\\nlogs. It reports retrieval effectiveness (MRR@10) on the official\\nvalidation set as well as average query latency (log-scale) using a\\nhigh-end server that dedicates one Tesla V100 GPU per query for\\nneural re-rankers. Following the re-ranking setup of MS MARCO,\\nColBERT (re-rank), the Neural Matching Models, and the Deep LMs\\nre-rank MS MARCO’s official top-1000 documents per query. Other\\n1https://blog.google/products/search/search-language-understanding-bert/\\n2https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-\\nin-search-experience-using-azure-gpus/\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n39\\nQuery Document\\nMaxSim∑\\nMaxSim MaxSims\\nQueryCNN  /  Match KernelsCNN  /  Match Kernels / MLPMLPs\\nDocument\\n(c) All-to-all Interaction\\n(e.g., BERT)(b) Query-Document Interaction\\n(e.g., DRMM, KNRM, Conv-KNRM)(d) Late Interaction\\n(i.e., the proposed ColBERT)(a) Representation-based Similarity\\n(e.g., DSSM, SNRM)Query Document\\ns\\nQuery Document\\nsFigure 2: Schematic diagrams illustrating query–document matching paradigms in neural IR. The figure contrasts existing\\napproaches (sub-figures (a), (b), and (c)) with the proposed late interaction paradigm (sub-figure (d)).\\nmethods, including ColBERT (full retrieval), directly retrieve the\\ntop-1000 results from the entire collection.\\nAs the figure shows, BERT considerably improves search preci-\\nsion, raising MRR@10 by almost 7% against the best previous meth-\\nods; simultaneously, it increases latency by up to tens of thousands\\nof milliseconds even with a high-end GPU. This poses a challenging\\ntradeoff since raising query response times by as little as 100ms is\\nknown to impact user experience and even measurably diminish\\nrevenue [ 17]. To tackle this problem, recent work has started explor-\\ning using Natural Language Understanding (NLU) techniques to\\naugment traditional retrieval models like BM25 [ 32]. For example,\\nNogueira et al. [26,28] expand documents with NLU-generated\\nqueries before indexing with BM25 scores and Dai & Callan [ 2] re-\\nplace BM25’s term frequency with NLU-estimated term importance.\\nDespite successfully reducing latency, these approaches generally\\nreduce precision substantially relative to BERT.\\nTo reconcile efficiency and contextualization in IR, we propose\\nColBERT , a ranking model based on contextualized late interac-\\ntion over BERT . As the name suggests, ColBERT proposes a novel\\nlate interaction paradigm for estimating relevance between a query\\n𝑞and a document 𝑑. Under late interaction, 𝑞and𝑑are separately\\nencoded into two sets of contextual embeddings, and relevance is\\nevaluated using cheap and pruning-friendly computations between\\nboth sets—that is, fast computations that enable ranking without\\nexhaustively evaluating every possible candidate.\\nFigure 2 contrasts our proposed late interaction approach with\\nexisting neural matching paradigms. On the left, Figure 2 (a) illus-\\ntrates representation-focused rankers, which independently compute\\nan embedding for 𝑞and another for 𝑑and estimate relevance as\\na single similarity score between two vectors [ 12,41]. Moving to\\nthe right, Figure 2 (b) visualizes typical interaction-focused rankers.\\nInstead of summarizing 𝑞and𝑑into individual embeddings, these\\nrankers model word- and phrase-level relationships across 𝑞and𝑑\\nand match them using a deep neural network (e.g., with CNNs/MLPs\\n[22] or kernels [ 36]). In the simplest case, they feed the neural net-\\nwork an interaction matrix that reflects the similiarity between\\nevery pair of words across 𝑞and𝑑. Further right, Figure 2 (c) illus-\\ntrates a more powerful interaction-based paradigm, which models\\nthe interactions between words within as well as across𝑞and𝑑at\\nthe same time, as in BERT’s transformer architecture [25].These increasingly expressive architectures are in tension. While\\ninteraction-based models (i.e., Figure 2 (b) and (c)) tend to be su-\\nperior for IR tasks [ 8,21], a representation-focused model—by iso-\\nlating the computations among 𝑞and𝑑—makes it possible to pre-\\ncompute document representations offline [ 41], greatly reducing\\nthe computational load per query. In this work, we observe that\\nthe fine-grained matching in interaction-based models and the pre-\\ncomputation in representation-based models can be combined by\\nretaining yet judiciously delaying the query–document interaction.\\nFigure 2 (d) illustrates an architecture that precisely does so. As\\nillustrated, every query embedding interacts with all document\\nembeddings via a MaxSim operator, which computes maximum\\nsimilarity (e.g., cosine), and the scalar outputs of these operators\\nare summed across query terms. This paradigm allows ColBERT to\\nexploit deep LM-based representations while shifting the cost of\\nencoding documents offline and amortizing the cost of encoding\\nthe query once across all ranked documents. Crucially, it enables\\nColBERT to leverage vector-similarity search indexes (e.g., [ 1,15])\\nto retrieve the top- 𝑘results directly from a large document collec-\\ntion. This ability substantially improves recall over existing models,\\nwhich only re-rank the output of term-based retrieval.\\nAs Figure 1 illustrates, ColBERT can serve queries in tens or, for\\nend-to-end retrieval from millions of documents, few hundreds of\\nmilliseconds. For instance, when used for re-ranking as in “ColBERT\\n(re-rank)”, it delivers over 170 ×speedup (and requires 14,000 ×fewer\\nFLOPs) relative to existing BERT-based models [ 25,27], while be-\\ning more effective than every non-BERT baseline (§4.2 & 4.3). Col-\\nBERT’s indexing—the only time it needs to feed documents through\\nBERT—is also practical: it can index the MS MARCO collection of\\n9M passages in about 3 hours using a single server with four GPUs\\n(§4.5), retaining its effectiveness with a space footprint of as little\\nas few tens of GiBs. Our ablation study (§4.4) shows that late in-\\nteraction, its implementation via MaxSim operations, and crucial\\ndesign choices within our BERT-based encoders are all essential to\\nColBERT’s effectiveness.\\nOur main contributions are as follows.\\n(1)We propose late interaction (§3.1) as a paradigm for efficient\\nand effective neural ranking.\\n(2)We present ColBERT (§3.2 & 3.3), a highly-effective model\\nthat employs novel BERT-based query and document en-\\ncoders within the late interaction paradigm.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n40(3)We show how to leverage ColBERT both for re-ranking on\\ntop of a term-based retrieval model (§3.5) and for searching\\na full collection using vector similarity indexes (§3.6).\\n(4)We evaluate ColBERT on MS MARCO and TREC CAR, two\\nrecent passage search collections.\\nWe release our reference implementation as open source.3\\n2 RELATED WORK\\nNeural Matching Models. Over the past few years, IR researchers\\nhave introduced numerous neural architectures for ranking. In\\nthis work, we compare against KNRM [ 4,36], Duet [ 20,22], Con-\\nvKNRM [ 4], and fastText+ConvKNRM [ 10]. KNRM proposes a dif-\\nferentiable kernel-pooling technique for extracting matching sig-\\nnals from an interaction matrix, while Duet combines signals from\\nexact-match-based as well as embedding-based similarities for rank-\\ning. Introduced in 2018, ConvKNRM learns to match 𝑛-grams in the\\nquery and the document. Lastly, fastText+ConvKNRM (abbreviated\\nfT+ConvKNRM) tackles the absence of rare words from typical\\nword embeddings lists by adopting sub-word token embeddings.\\nIn 2018, Zamani et al. [41] introduced SNRM, a representation-\\nfocused IR model that encodes each query and each document as\\na single, sparse high-dimensional vector of “latent terms”. By pro-\\nducing a sparse-vector representation for each document, SNRM\\nis able to use a traditional IR inverted index for representing docu-\\nments, allowing fast end-to-end retrieval. Despite highly promising\\nresults and insights, SNRM’s effectiveness is substantially outper-\\nformed by the state of the art on the datasets with which it was\\nevaluated (e.g., see [ 18,38]). While SNRM employs sparsity to al-\\nlow using inverted indexes, we relax this assumption and compare\\na (dense) BERT-based representation-focused model against our\\nlate-interaction ColBERT in our ablation experiments in §4.4. For a\\ndetailed overview of existing neural ranking models, we refer the\\nreaders to two recent surveys of the literature [8, 21].\\nLanguage Model Pretraining for IR. Recent work in NLU\\nemphasizes the importance pre-training language representation\\nmodels in an unsupervised fashion before subsequently fine-tuning\\nthem on downstream tasks. A notable example is BERT [ 5], a bi-\\ndirectional transformer-based language model whose fine-tuning\\nadvanced the state of the art on various NLU benchmarks. Nogueira et\\nal.[25], MacAvaney et al. [18], and Dai & Callan [ 3] investigate\\nincorporating such LMs (mainly BERT, but also ELMo [ 29]) on dif-\\nferent ranking datasets. As illustrated in Figure 2 (c), the common\\napproach (and the one adopted by Nogueira et al. on MS MARCO\\nand TREC CAR) is to feed the query–document pair through BERT\\nand use an MLP on top of BERT’s [CLS] output token to produce a\\nrelevance score. Subsequent work by Nogueira et al. [27] introduced\\nduoBERT, which fine-tunes BERT to compare the relevance of a\\npair of documents given a query. Relative to their single-document\\nBERT, this gives duoBERT about 1% MRR@10 advantage on MS\\nMARCO while increasing the cost by at least 1.4×.\\nBERT Optimizations. As discussed in §1, these rankers can be\\nhighly expensive in practice. Orthogonal to our approach, there are\\nongoing efforts in the NLU literature for distilling [ 14,33], compress-\\ning [ 40], and pruning [ 19] BERT. Other optimizations may trade\\nquality and speed specifically for IR (e.g., re-ranking with a smaller\\n3https://github.com/stanford-futuredata/ColBERT\\nQuery Document\\nQuery Encoder, fQ Document Encoder, fDMaxSim MaxSim MaxSimscore\\nOffline IndexingFigure 3: The general architecture of ColBERT given a query\\n𝑞and a document 𝑑.\\ndepth𝑘(§4.2) or truncating longer documents). While these efforts\\ncan be instrumental in narrowing the efficiency gap, they generally\\nachieve much smaller speedups than our re-designed architecture\\nfor IR, due to their generic nature, and the more aggressive ones\\noften come at the cost of noticeably lower quality.\\nEfficient NLU-based Models. Recently, a direction emerged\\nthat employs expensive NLU computation offline. This includes\\ndoc2query [ 28] and DeepCT [ 2]. The doc2query model expands\\neach document with a pre-defined number of synthetic queries, gen-\\nerated by a seq2seq transformer model trained to generate queries\\ngiven a document. It then relies on a BM25 index for retrieval from\\nthe (expanded) documents. DeepCT uses BERT to produce the term\\nfrequency component of BM25 in a context-aware manner, essen-\\ntially representing a feasible realization of the term-independence\\nassumption with neural networks [ 23]. Lastly, docTTTTTquery [ 26]\\nis identical to doc2query except that it fine-tunes a pre-trained\\nmodel (namely, T5 [31]) for generating the predicted queries.\\nConcurrently with the drafting of this paper, Hofstätter et al. [11]\\npublished their Transformer-Kernel (TK) model. At a high level, TK\\nimproves the KNRM architecture described earlier: while KNRM\\nemploys kernel pooling on top of word-embedding-based inter-\\naction, TK uses a Transformer [ 34] component for contextually\\nencoding queries and documents before kernel pooling. TK estab-\\nlishes a new state-of-the-art for non-BERT models on MS MARCO\\n(Dev); however, the best non-ensemble MRR@10 it achieves is 31%\\nwhile ColBERT reaches up to 36%. Moreover, due to indexing docu-\\nment representations offline and employing a MaxSim-based late\\ninteraction mechanism, ColBERT is much more scalable, enabling\\nend-to-end retrieval which is not supported by TK.\\n3 COLBERT\\nColBERT prescribes a simple framework for balancing the quality\\nand cost of neural IR, particularly deep language models like BERT.\\nAs introduced earlier, delaying the query–document interaction can\\nfacilitate cheap neural re-ranking (i.e., through pre-computation)\\nand even support practical end-to-end neural retrieval (i.e., through\\npruning via vector-similarity search). ColBERT addresses how to\\ndo so while still preserving the effectiveness of state-of-the-art\\nmodels, which condition the bulk of their computations on the joint\\nquery–document pair.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n41Even though ColBERT’s late-interaction framework can be ap-\\nplied to a wide variety of architectures (e.g., CNNs, RNNs, transform-\\ners, etc.), we choose to focus this work on bi-directional transformer-\\nbased encoders (i.e., BERT) owing to their state-of-the-art effective-\\nness yet very high computational cost.\\n3.1 Architecture\\nFigure 3 depicts the general architecture of ColBERT, which com-\\nprises: (a) a query encoder 𝑓𝑄, (b) a document encoder 𝑓𝐷, and (c)\\nthe late interaction mechanism. Given a query 𝑞and document 𝑑,\\n𝑓𝑄encodes𝑞into a bag of fixed-size embeddings 𝐸𝑞while𝑓𝐷en-\\ncodes𝑑into another bag 𝐸𝑑. Crucially, each embeddings in 𝐸𝑞and\\n𝐸𝑑iscontextualized based on the other terms in 𝑞or𝑑, respectively.\\nWe describe our BERT-based encoders in §3.2.\\nUsing𝐸𝑞and𝐸𝑑, ColBERT computes the relevance score be-\\ntween𝑞and𝑑via late interaction, which we define as a summation\\nof maximum similarity (MaxSim) operators. In particular, we find\\nthe maximum cosine similarity of each 𝑣∈𝐸𝑞with vectors in 𝐸𝑑,\\nand combine the outputs via summation. Besides cosine, we also\\nevaluate squared L2 distance as a measure of vector similarity. In-\\ntuitively, this interaction mechanism softly searches for each query\\nterm𝑡𝑞—in a manner that reflects its context in the query—against\\nthe document’s embeddings, quantifying the strength of the “match”\\nvia the largest similarity score between 𝑡𝑞and a document term 𝑡𝑑.\\nGiven these term scores, it then estimates the document relevance\\nby summing the matching evidence across all query terms.\\nWhile more sophisticated matching is possible with other choices\\nsuch as deep convolution and attention layers (i.e., as in typical\\ninteraction-focused models), a summation of maximum similarity\\ncomputations has two distinctive characteristics. First, it stands\\nout as a particularly cheap interaction mechanism, as we examine\\nits FLOPs in §4.2. Second, and more importantly, it is amenable\\nto highly-efficient pruning for top- 𝑘retrieval, as we evaluate in\\n§4.3. This enables using vector-similarity algorithms for skipping\\ndocuments without materializing the full interaction matrix or even\\nconsidering each document in isolation. Other cheap choices (e.g.,\\na summation of average similarity scores, instead of maximum) are\\npossible; however, many are less amenable to pruning. In §4.4, we\\nconduct an extensive ablation study that empirically verifies the ad-\\nvantage of our MaxSim-based late interaction against alternatives.\\n3.2 Query & Document Encoders\\nPrior to late interaction, ColBERT encodes each query or document\\ninto a bag of embeddings, employing BERT-based encoders. We\\nshare a single BERT model among our query and document en-\\ncoders but distinguish input sequences that correspond to queries\\nand documents by prepending a special token [Q]to queries and\\nanother token [D]to documents.\\nQuery Encoder. Given a textual query 𝑞, we tokenize it into its\\nBERT-based WordPiece [ 35] tokens𝑞1𝑞2...𝑞𝑙. We prepend the token\\n[Q]to the query. We place this token right after BERT’s sequence-\\nstart token [CLS] . If the query has fewer than a pre-defined number\\nof tokens𝑁𝑞, we pad it with BERT’s special [mask] tokens up\\nto length𝑁𝑞(otherwise, we truncate it to the first 𝑁𝑞tokens).\\nThis padded sequence of input tokens is then passed into BERT’sdeep transformer architecture, which computes a contextualized\\nrepresentation of each token.\\nWe denote the padding with masked tokens as query augmen-\\ntation , a step that allows BERT to produce query-based embeddings\\nat the positions corresponding to these masks. Query augmentation\\nis intended to serve as a soft, differentiable mechanism for learning\\nto expand queries with new terms or to re-weigh existing terms\\nbased on their importance for matching the query. As we show in\\n§4.4, this operation is essential for ColBERT’s effectiveness.\\nGiven BERT’s representation of each token, our encoder passes\\nthe contextualized output representations through a linear layer\\nwith no activations. This layer serves to control the dimension of\\nColBERT’s embeddings, producing 𝑚-dimensional embeddings for\\nthe layer’s output size 𝑚. As we discuss later, we typically fix 𝑚to\\nbe much smaller than BERT’s fixed hidden dimension.\\nWhile ColBERT’s embedding dimension has limited impact on\\nthe efficiency of query encoding, this step is crucial for controlling\\nthe space footprint of documents, as we show in §4.5. In addition,\\nit can have a significant impact on query execution time, particu-\\nlarly the time taken for transferring the document representations\\nonto the GPU from system memory (where they reside before pro-\\ncessing a query). In fact, as we show in §4.2, gathering, stacking,\\nand transferring the embeddings from CPU to GPU can be the\\nmost expensive step in re-ranking with ColBERT. Finally, the out-\\nput embeddings are normalized so each has L2 norm equal to one.\\nThe result is that the dot-product of any two embeddings becomes\\nequivalent to their cosine similarity, falling in the [−1,1]range.\\nDocument Encoder. Our document encoder has a very similar\\narchitecture. We first segment a document 𝑑into its constituent\\ntokens𝑑1𝑑2...𝑑𝑚, to which we prepend BERT’s start token [CLS] fol-\\nlowed by our special token [D]that indicates a document sequence.\\nUnlike queries, we do not append [mask] tokens to documents. Af-\\nter passing this input sequence through BERT and the subsequent\\nlinear layer, the document encoder filters out the embeddings corre-\\nsponding to punctuation symbols, determined via a pre-defined list.\\nThis filtering is meant to reduce the number of embeddings per doc-\\nument, as we hypothesize that (even contextualized) embeddings\\nof punctuation are unnecessary for effectiveness.\\nIn summary, given 𝑞=𝑞0𝑞1...𝑞𝑙and𝑑=𝑑0𝑑1...𝑑𝑛, we compute\\nthe bags of embeddings 𝐸𝑞and𝐸𝑑in the following manner, where\\n#refers to the [mask] tokens:\\n𝐸𝑞:=Normalize(CNN(BERT(“[𝑄]𝑞0𝑞1...𝑞𝑙##...#”))) (1)\\n𝐸𝑑:=Filter(Normalize(CNN(BERT(“[𝐷]𝑑0𝑑1...𝑑𝑛”)))) (2)\\n3.3 Late Interaction\\nGiven the representation of a query 𝑞and a document 𝑑, the rele-\\nvance score of 𝑑to𝑞, denoted as𝑆𝑞,𝑑, is estimated via late interaction\\nbetween their bags of contextualized embeddings. As mentioned\\nbefore, this is conducted as a sum of maximum similarity computa-\\ntions, namely cosine similarity (implemented as dot-products due\\nto the embedding normalization) or squared L2 distance.\\n𝑆𝑞,𝑑:=Õ\\n𝑖∈[|𝐸𝑞|]max\\n𝑗∈[|𝐸𝑑|]𝐸𝑞𝑖·𝐸𝑇\\n𝑑𝑗(3)\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n42ColBERT is differentiable end-to-end. We fine-tune the BERT\\nencoders and train from scratch the additional parameters (i.e., the\\nlinear layer and the [Q] and [D] markers’ embeddings) using the\\nAdam [ 16] optimizer. Notice that our interaction mechanism has\\nno trainable parameters. Given a triple ⟨𝑞,𝑑+,𝑑−⟩with query 𝑞,\\npositive document 𝑑+and negative document 𝑑−, ColBERT is used\\nto produce a score for each document individually and is optimized\\nvia pairwise softmax cross-entropy loss over the computed scores\\nof𝑑+and𝑑−.\\n3.4 Offline Indexing: Computing & Storing\\nDocument Embeddings\\nBy design, ColBERT isolates almost all of the computations be-\\ntween queries and documents to enable pre-computing document\\nrepresentations offline. At a high level, our indexing procedure is\\nstraight-forward: we proceed over the documents in the collection\\nin batches, running our document encoder 𝑓𝐷on each batch and\\nstoring the output embeddings per document. Although indexing a\\nset of documents is an offline process, we incorporate a few simple\\noptimizations for enhancing its throughput. As we show in §4.5,\\nthese can considerably reduce the offline cost of indexing.\\nTo begin with, we exploit multiple GPUs, if available, for faster\\nencoding of batches of documents in parallel. When batching, we\\npad all documents to the maximum length of a document within\\nthe batch.4To make capping the sequence length on a per-batch\\nbasis effective, our indexer proceeds through documents in large\\ngroups of𝐵(e.g.,𝐵=100,000) documents. It sorts these documents\\nby length and then feeds batches of 𝑏(e.g.,𝑏=128) documents of\\ncomparable length through our encoder. Such length-based bucket-\\ning is sometimes refered to as a BucketIterator in some libraries\\n(e.g., allenNLP). Lastly, while most computations occur on the GPU,\\nwe found that a non-trivial portion of the indexing time is spent on\\npre-processing the text sequences, primarily BERT’s WordPiece to-\\nkenization. Exploiting that these operations are independent across\\ndocuments in a batch, we parallelize the pre-processing across the\\navailable CPU cores.\\nOnce the document representations are produced, they are saved\\nto disk using 32-bit or 16-bit values to represent each dimension.\\nAs we describe in §3.5 and 3.6, these representations are either\\nsimply loaded from disk for ranking or are subsequently indexed\\nfor vector-similarity search, respectively.\\n3.5 Top-𝑘Re-ranking with ColBERT\\nRecall that ColBERT can be used for re-ranking the output of an-\\nother retrieval model, typically a term-based model, or directly\\nfor end-to-end retrieval from a document collection. In this sec-\\ntion, we discuss how we use ColBERT for ranking a small set of\\n𝑘(e.g.,𝑘=1000) documents given a query 𝑞. Since𝑘is small, we\\nrely on batch computations to exhaustively score each document\\n(unlike our approach in §3.6). To begin with, our query serving sub-\\nsystem loads the indexed documents representations into memory,\\nrepresenting each document as a matrix of embeddings.\\nGiven a query 𝑞, we compute its bag of contextualized embed-\\ndings𝐸𝑞(Equation 1) and, concurrently, gather the document repre-\\nsentations into a 3-dimensional tensor 𝐷consisting of 𝑘document\\n4The public BERT implementations we saw simply pad to a pre-defined length.matrices. We pad the 𝑘documents to their maximum length to\\nfacilitate batched operations, and move the tensor 𝐷to the GPU’s\\nmemory. On the GPU, we compute a batch dot-product of 𝐸𝑞and\\n𝐷, possibly over multiple mini-batches. The output materializes a\\n3-dimensional tensor that is a collection of cross-match matrices\\nbetween𝑞and each document. To compute the score of each docu-\\nment, we reduce its matrix across document terms via a max-pool\\n(i.e., representing an exhaustive implementation of our MaxSim\\ncomputation) and reduce across query terms via a summation. Fi-\\nnally, we sort the 𝑘documents by their total scores.\\nRelative to existing neural rankers (especially, but not exclusively,\\nBERT-based ones), this computation is very cheap that, in fact, the\\ncost of a simple implementation is dominated by the gathering\\nand transferring of the pre-computed embeddings. To illustrate,\\nranking𝑘documents via typical BERT rankers requires feeding\\nBERT𝑘different inputs each of length 𝑙=|𝑞|+|𝑑𝑖|for query𝑞and\\ndocuments𝑑𝑖, where attention has quadratic cost in the length of\\nthe sequence. In contrast, ColBERT feeds BERT only a single, much\\nshorter sequence of length 𝑙=|𝑞|. Consequently, ColBERT is not\\nonly cheaper, it also scales much better with 𝑘(§4.2).\\n3.6 End-to-end Top- 𝑘Retrieval with ColBERT\\nAs mentioned before, ColBERT’s late-interaction operator is specifi-\\ncally designed to enable end-to-end retrieval from a large collection,\\nlargely to improve recall relative to term-based retrieval approaches.\\nThis section is concerned with cases where the number of docu-\\nments to be ranked is too large for exhaustive evaluation of each\\npossible candidate document, particularly when we are only in-\\nterested in the highest scoring ones. Concretely, we focus here on\\nretrieving the top- 𝑘results directly from a large document collec-\\ntion with𝑁(e.g.,𝑁=10,000,000) documents, where 𝑘≪𝑁.\\nTo do so, we leverage the pruning-friendly nature of the MaxSim\\noperations at the backbone of late interaction. Instead of applying\\nMaxSim between one of the query embeddings and all of one docu-\\nment’s embeddings, we can use fast vector-similarity data structures\\nto efficiently conduct this search between the query embedding\\nandalldocument embeddings across the full collection. For this,\\nwe employ an off-the-shelf library for large-scale vector-similarity\\nsearch, namely faiss [15] from Facebook.5In particular, at the\\nend of offline indexing (§3.4), we maintain a mapping from each\\nembedding to its document of origin and then index all document\\nembeddings into faiss.\\nSubsequently, when serving queries, we use a two-stage pro-\\ncedure to retrieve the top- 𝑘documents from the entire collection.\\nBoth stages rely on ColBERT’s scoring: the first is an approximate\\nstage aimed at filtering while the second is a refinement stage. For\\nthe first stage, we concurrently issue 𝑁𝑞vector-similarity queries\\n(corresponding to each of the embeddings in 𝐸𝑞) onto our faiss in-\\ndex. This retrieves the top- 𝑘′(e.g.,𝑘′=𝑘/2) matches for that vector\\nover all document embeddings. We map each of those to its docu-\\nment of origin, producing 𝑁𝑞×𝑘′document IDs, only 𝐾≤𝑁𝑞×𝑘′\\nof which are unique. These 𝐾documents likely contain one or more\\nembeddings that are highly similar to the query embeddings. For\\nthe second stage, we refine this set by exhaustively re-ranking only\\nthose𝐾documents in the usual manner described in §3.5.\\n5https://github.com/facebookresearch/faiss\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n43In our faiss -based implementation, we use an IVFPQ index\\n(“inverted file with product quantization”). This index partitions\\nthe embedding space into 𝑃(e.g.,𝑃=1000) cells based on 𝑘-means\\nclustering and then assigns each document embedding to its nearest\\ncell based on the selected vector-similarity metric. For serving\\nqueries, when searching for the top- 𝑘′matches for a single query\\nembedding, only the nearest 𝑝(e.g.,𝑝=10) partitions are searched.\\nTo improve memory efficiency, every embedding is divided into 𝑠\\n(e.g.,𝑠=16) sub-vectors, each represented using one byte. Moreover,\\nthe index conducts the similarity computations in this compressed\\ndomain, leading to cheaper computations and thus faster search.\\n4 EXPERIMENTAL EVALUATION\\nWe now turn our attention to empirically testing ColBERT, address-\\ning the following research questions.\\nRQ1: In a typical re-ranking setup, how well can ColBERT bridge\\nthe existing gap (highlighted in §1) between highly-efficient and\\nhighly-effective neural models? (§4.2)\\nRQ2: Beyond re-ranking, can ColBERT effectively support end-\\nto-end retrieval directly from a large collection? (§4.3)\\nRQ3: What does each component of ColBERT (e.g., late interac-\\ntion, query augmentation) contribute to its quality? (§4.4)\\nRQ4: What are ColBERT’s indexing-related costs in terms of\\noffline computation and memory overhead? (§4.5)\\n4.1 Methodology\\n4.1.1 Datasets & Metrics. Similar to related work [ 2,27,28], we\\nconduct our experiments on the MS MARCO Ranking [ 24] (hence-\\nforth, MS MARCO) and TREC Complex Answer Retrieval (TREC-\\nCAR) [ 6] datasets. Both of these recent datasets provide large train-\\ning data of the scale that facilitates training and evaluating deep\\nneural networks. We describe both in detail below.\\nMS MARCO. MS MARCO is a dataset (and a corresponding\\ncompetition) introduced by Microsoft in 2016 for reading compre-\\nhension and adapted in 2018 for retrieval. It is a collection of 8.8M\\npassages from Web pages, which were gathered from Bing’s re-\\nsults to 1M real-world queries. Each query is associated with sparse\\nrelevance judgements of one (or very few) documents marked as\\nrelevant and no documents explicitly indicated as irrelevant. Per\\nthe official evaluation, we use MRR@10 to measure effectiveness.\\nWe use three query sets in our evaluation. The official develop-\\nment and evaluation sets contain roughly 7k queries. The relevance\\njudgements of the evaluation set are held-out by Microsoft and\\nMRR@10 results can only be obtained by submitting to the com-\\npetition’s organizers. We submitted our main re-ranking ColBERT\\nmodel for §4.2. In addition, the collection includes roughly 55k\\nqueries (with labels) that are provided as additional validation data.\\nWe re-purpose a random sample of 5k queries among those (i.e.,\\nones not in our development or training sets) as a “local” evaluation\\nset. Along with the official development set, we use this held-out\\nset for testing our models as well as baselines in §4.3. We do so to\\navoid submitting multiple variants of the same model at once, as\\nthe organizers discourage too many submissions by the same team.\\nTREC CAR. Introduced by Dietz [ 6]et al. in 2017, TREC CAR is\\na synthetic dataset based on Wikipedia that consists of about 29M\\npassages. Similar to related work [ 25], we dedicate the first fourof five pre-defined folds for training (and the fifth for validation),\\nwhich amounts to roughly 3M queries generated by concatenating\\nthe title of a Wikipedia page with the heading of one of its sections.\\nThat section’s passages are marked as relevant to the corresponding\\nquery. Our evaluation is conducted on the test set used in TREC\\n2017 CAR, which contains 2,254 queries.\\n4.1.2 Implementation. Our ColBERT models are implemented us-\\ning Python 3 and PyTorch 1. We use the popular transformers6\\nlibrary for pre-trained BERT. Similar to [ 25], we fine-tune all Col-\\nBERT models with learning rate 3×10−6with a batch size 32. We fix\\nthe number of embeddings per query at 𝑁𝑞=32. Unless otherwise\\nstated, we set our ColBERT embedding dimension 𝑚to 128; §4.5\\ndemonstrates ColBERT’s robustness to a wide range of dimensions.\\nFor MS MARCO, we initialize the BERT components of the Col-\\nBERT query and document encoders using Google’s official pre-\\ntrained BERT basemodel and train all models for 200k iterations.\\nFor TREC CAR, we follow related work [ 2,25] and use a different\\npre-trained model to the official ones. To explain, the official BERT\\nmodels were pre-trained on Wikipedia, which is the source of TREC\\nCAR’s training and test sets. To avoid leaking test data into train,\\nNogueira and Cho [ 25] pre-train a BERT model on the Wiki pages\\ncorresponding to training subset of TREC CAR. They release their\\nBERT large pre-trained model, which we fine-tune for ColBERT’s ex-\\nperiments on TREC CAR. As BERT large embeddings are larger, we\\nset𝑚to 200, and since fine-tuning this model is significantly slower\\nthan BERT base, we train on TREC CAR for only 125k iterations.\\nIn our re-ranking results, unless otherwise stated, we use 4 bytes\\nper dimension in our embeddings and employ cosine as our vector-\\nsimilarity function. For end-to-end ranking, we use (squared) L2\\ndistance, as we found our faiss index was faster at L2-based re-\\ntrieval. For our faiss index, we set the number of partitions to\\n𝑃=2,000, and search the nearest 𝑝=10to each query embedding to\\nretrieve𝑘′=𝑘=1000 document vectors per query embedding. We\\ndivide each embedding into 𝑠=16sub-vectors, each encoded using\\none byte. To represent the index used for the second stage of our\\nend-to-end retrieval procedure, we use 16-bit values per dimension.\\n4.1.3 Hardware & Time Measurements. To evaluate the latency of\\nneural re-ranking models in §4.2, we use a single Tesla V100 GPU\\nthat has 32 GiBs of memory on a server with two Intel Xeon Gold\\n6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469\\nGiBs of RAM. For the mostly CPU-based retrieval experiments in\\n§4.3 and the indexing experiments in §4.5, we use another server\\nwith the same CPU and system memory specifications but which\\nhas four Titan V GPUs attached, each with 12 GiBs of memory.\\nAcross all experiments, only one GPU is dedicated per query for\\nretrieval (i.e., for methods with neural computations) but we use\\nup to all four GPUs during indexing.\\n4.2 Quality–Cost Tradeoff: Top- 𝑘Re-ranking\\nIn this section, we examine ColBERT’s efficiency and effectiveness\\nat re-ranking the top- 𝑘results extracted by a bag-of-words retrieval\\nmodel, which is the most typical setting for testing and deploying\\nneural ranking models. We begin with the MS MARCO dataset. We\\n6https://github.com/huggingface/transformers\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n44Method MRR@10 (Dev) MRR@10 (Eval) Re-ranking Latency (ms) FLOPs/query\\nBM25 (official) 16.7 16.5 - -\\nKNRM 19.8 19.8 3 592M (0.085×)\\nDuet 24.3 24.5 22 159B (23×)\\nfastText+ConvKNRM 29.0 27.7 28 78B (11×)\\nBERT base[25] 34.7 - 10,700 97T (13,900×)\\nBERT base(our training) 36.0 - 10,700 97T (13,900×)\\nBERT large [25] 36.5 35.9 32,900 340T (48,600×)\\nColBERT (over BERT base) 34.9 34.9 61 7B (1×)\\nTable 1: “Re-ranking” results on MS MARCO. Each neural model re-ranks the official top-1000 results produced by BM25.\\nLatency is reported for re-ranking only. To obtain the end-to-end latency in Figure 1, we add the BM25 latency from Table 2.\\nMethod MRR@10 (Dev) MRR@10 (Local Eval) Latency (ms) Recall@50 Recall@200 Recall@1000\\nBM25 (official) 16.7 - - - - 81.4\\nBM25 (Anserini) 18.7 19.5 62 59.2 73.8 85.7\\ndoc2query 21.5 22.8 85 64.4 77.9 89.1\\nDeepCT 24.3 - 62(est.) 69 [2] 82 [2] 91 [2]\\ndocTTTTTquery 27.7 28.4 87 75.6 86.9 94.7\\nColBERT L2(re-rank) 34.8 36.4 - 75.3 80.5 81.4\\nColBERT L2(end-to-end) 36.0 36.7 458 82.9 92.3 96.8\\nTable 2: End-to-end retrieval results on MS MARCO. Each model retrieves the top-1000 documents per query directly from the\\nentire 8.8M document collection.\\ncompare against KNRM, Duet, and fastText+ConvKNRM, a repre-\\nsentative set of neural matching models that have been previously\\ntested on MS MARCO. In addition, we compare against the adapta-\\ntion of BERT for ranking by Nogueira and Cho [ 25], in particular,\\ntheir BERT baseand its deeper counterpart BERT large.7\\nWe report the competition’s official metric, namely MRR@10, on\\nthe validation set (Dev) and the evaluation set (Eval). We also report\\nthe re-ranking latency, which we measure using a single Tesla V100\\nGPU, and the FLOPs per query for each neural ranking model. To do\\nso, we adapt the baselines’ publicly-available reference implemen-\\ntations into our pytorch testbed. For ColBERT, our reported latency\\nsubsumes the entire computation from gathering the document\\nrepresentations, moving them to the GPU, tokenizing then encod-\\ning the query, and applying late interaction to compute document\\nscores. For the baselines, we measure the scoring computations on\\nthe GPU and exclude the CPU-based text preprocessing (similar\\nto [9]). In principle, the baselines can pre-compute most of this\\npreprocessing (e.g., document tokenization) offline. We estimate\\nthe FLOPs using the torchprofile8library.\\nWe now proceed to study the results, which are reported in Ta-\\nble 1. To begin with, we notice the fast progress from KNRM in\\n2017 to the BERT-based models in 2019, manifesting itself in over\\n16% increase in MRR@10. As described in §1, the simultaneous\\nincrease in computational cost is difficult to miss. Judging by their\\nrather monotonic pattern of increasingly larger cost and higher ef-\\nfectiveness, these results appear to paint a picture where expensive\\nmodels are necessary for high-quality ranking.\\nIn contrast with this trend, ColBERT (which employs late in-\\nteraction over BERT base) performs competitively with the original\\n7https://github.com/nyu-dl/dl4marco-bert/\\n8https://github.com/mit-han-lab/torchprofileadaptation of BERT baseand BERT large for ranking by Nogueira\\nand Cho [ 25,27]. Interestingly, ColBERT appears no worse than\\nBERT basein MRR@10—although the latter uses a different loss func-\\ntion to ColBERT’s (§3.3). To confirm the intuition that ColBERT’s\\nlate interaction does trade away some of BERT’s quality, the table\\nalso reports results of “BERT base(our training)”, which is based on\\nNogueira and Cho’s model of the same size but is optimized with\\npairwise softmax cross-entropy loss. We train it with learning rate\\n3×10−6and batch size 16 for 200k iterations. Unlike the original\\nBERT baseranker, results show that this model does in fact have an\\nedge over ColBERT’s effectiveness.\\nWhile highly competitive in retrieval quality, ColBERT is orders\\nof magnitude cheaper than BERT base, in particular, by over 170 ×in\\nlatency and 13,900×in FLOPs. This highlights the expressiveness\\nof our proposed late interaction mechanism when coupled with a\\npowerful pre-trained LM like BERT. While ColBERT’s re-ranking\\nlatency is slightly higher than the non-BERT models shown (i.e., by\\n10s of milliseconds), this difference is explained by the time it takes\\na simple Python implementation to gather, stack, and transfer the\\ndocument embeddings to the GPU. In particular, the query encoding\\nand interaction in ColBERT consume only 13 milliseconds of its\\ntotal execution time.\\nDiving deeper into the quality–cost tradeoff between BERT and\\nColBERT, Figure 4 demonstrates the relationships between FLOPs\\nand effectiveness (MRR@10) as a function of the re-ranking depth\\n𝑘when re-ranking the top- 𝑘results by BM25, comparing ColBERT\\nand BERT base(our training). We conduct this experiment on MS\\nMARCO (Dev). We note here that as the official top-1000 ranking\\ndoes not provide the BM25 order (and also lacks documents beyond\\nthe top-1000 per query), the models in this experiment re-rank the\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n450.27 0.29 0.31 0.33 0.35 0.37\\nMRR@10103104105106107108109Million FLOPs (log-scale)\\nk=10205010020050010002000\\nk=10 20 50 100\\n200500\\n10002000\\nBERTbase (our training)\\nColBERTFigure 4: FLOPs (in millions) and MRR@10 as functions\\nof the re-ranking depth 𝑘. Since the official BM25 ranking\\nis not ordered, the initial top- 𝑘retrieval is conducted with\\nAnserini’s BM25.\\nAnserini [ 37] toolkit’s BM25 output. Consequently, both MRR@10\\nvalues at𝑘=1000 are slightly higher from those reported in Table 1.\\nStudying the results in Figure 4, we notice that not only is Col-\\nBERT much cheaper than BERT for the same model size (i.e., 12-\\nlayer “base” transformer encoder), it also scales better with the\\nnumber of ranked documents. In part, this is because ColBERT\\nonly needs to process the query once, irrespective of the number of\\ndocuments evaluated. For instance, at 𝑘=10, BERT requires nearly\\n180×more FLOPs than ColBERT; at 𝑘=1000, BERT’s overhead\\njumps to 13,900×. It then reaches 23,000× at𝑘=2000.\\nWe observe that this orders-of-magnitude reduction in FLOPs\\nmakes it practical to run ColBERT entirely on the CPU. In fact, sub-\\nsequent informal experimentation suggests that ColBERT’s latency\\nand FLOPs can be considerably reduced further by a number of\\noptimizations, some entailing a controllable quality tradeoff. These\\ninclude using smaller vector dimensions (whose MRR@10 is tested\\nin §4.5), padding queries to shorter 𝑁𝑞, processing documents in\\na lengths-aware fashion, and distilling/quantizing the encoder(s)\\n(§2), the final two of which are also applicable to the baseline BERT\\nreference implementation by Nogueira and Cho [ 25]. Addition-\\nally, caching the document embeddings on the GPU(s)—if sufficient\\nGPU memory exists—can significantly reduce ColBERT’s latency.\\nLastly, batch-processing of multiple queries can enhance ColBERT’s\\nthroughput by improving the GPU utilization of query encoding.\\nWe leave exploring these opportunities for future work.\\nMethod MAP MRR@10\\nBM25 (Anserini) 15.3 -\\ndoc2query 18.1 -\\nDeepCT 24.6 33.2\\nBM25 + BERT base 31.0 -\\nBM25 + BERT large 33.5 -\\nBM25 + ColBERT 31.3 44.2\\nTable 3: Results on TREC CAR.\\nHaving studied our results on MS MARCO, we now consider\\nTREC CAR, whose official metric is MAP. Similar to Table 1, we\\nalso report MRR@10. The results are summarized in Table 3, which\\nincludes a number of important baselines (BM25, doc2query, and\\nDeepCT) in addition to re-ranking baselines that have previouslybeen tested on this dataset. As the table shows, the results mirror\\nthose seen with MS MARCO.\\n4.3 End-to-end Top- 𝑘Retrieval\\nBeyond cheap re-ranking, ColBERT is amenable to top- 𝑘retrieval di-\\nrectly from a full collection. Table 2 considers full retrieval, wherein\\neach model retrieves the top-1000 documents directly from MS\\nMARCO’s 8.8M documents per query. In addition to MRR@10 and\\nlatency in milliseconds, the table reports Recall@50, Recall@200,\\nand Recall@1000, important metrics for a full-retrieval model that\\nessentially filters down a large collection on a per-query basis.\\nWe compare against BM25, in particular MS MARCO’s official\\nBM25 ranking as well as a well-tuned baseline based on the Anserini\\ntoolkit.9While many other traditional models exist, we are not\\naware of any that substantially outperform Anserini’s BM25 im-\\nplementation (e.g., see RM3 in [ 28], LMDir in [ 2], or Microsoft’s\\nproprietary feature-based RankSVM on the leaderboard).\\nWe also compare against doc2query, DeepCT, and docTTTTT-\\nquery. All three rely on a traditional bag-of-words model (primarily\\nBM25) for retrieval. Crucially, however, they re-weigh the frequency\\nof terms per document and/or expand the set of terms in each doc-\\nument before building the BM25 index. In particular, doc2query\\nexpands each document with a pre-defined number of synthetic\\nqueries generated by a seq2seq transformer model (which docTTT-\\nTquery replaced with a pre-trained language model, T5 [ 31]). In\\ncontrast, DeepCT uses BERT to produce the term frequency com-\\nponent of BM25 in a context-aware manner.\\nFor the latency of Anserini’s BM25, doc2query, and docTTTT-\\nquery, we use the authors’ [ 26,28] Anserini-based implementation.\\nWhile this implementation supports multi-threading, it only utilizes\\nparallelism across different queries. We thus report single-threaded\\nlatency for these models, noting that simply parallelizing their com-\\nputation over shards of the index can substantially decrease their\\nalready-low latency. For DeepCT, we only estimate its latency us-\\ning that of BM25 (as denoted by (est.) in the table), since DeepCT\\nre-weighs BM25’s term frequency without modifying the index\\notherwise.10As discussed in §4.1, we use ColBERT L2for end-to-\\nend retrieval, which employs negative squared L2 distance as its\\nvector-similarity function. For its latency, we measure the time for\\nfaiss -based candidate filtering and the subsequent re-ranking. In\\nthis experiment, faiss uses all available CPU cores.\\nLooking at Table 2, we first see Anserini’s BM25 baseline at 18.7\\nMRR@10, noticing its very low latency as implemented in Anserini\\n(which extends the well-known Lucene system), owing to both\\nvery cheap operations and decades of bag-of-words top- 𝑘retrieval\\noptimizations. The three subsequent baselines, namely doc2query,\\nDeepCT, and docTTTTquery, each brings a decisive enhancement\\nto effectiveness. These improvements come at negligible overheads\\nin latency, since these baselines ultimately rely on BM25-based\\nretrieval. The most effective among these three, docTTTTquery,\\ndemonstrates a massive 9% gain over vanilla BM25 by fine-tuning\\nthe recent language model T5.\\n9http://anserini.io/\\n10In practice, a myriad of reasons could still cause DeepCT’s latency to differ\\nslightly from BM25’s. For instance, the top- 𝑘pruning strategy employed, if any, could\\ninteract differently with a changed distribution of scores.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n46Shifting our attention to ColBERT’s end-to-end retrieval effec-\\ntiveness, we see its major gains in MRR@10 over all of these end-\\nto-end models. In fact, using ColBERT in the end-to-end setup is su-\\nperior in terms of MRR@10 to re-ranking with the same model due\\nto the improved recall. Moving beyond MRR@10, we also see large\\ngains in Recall@ 𝑘for𝑘equals to 50, 200, and 1000. For instance, its\\nRecall@50 actually exceeds the official BM25’s Recall@1000 and\\neven all but docTTTTTquery’s Recall@200, emphasizing the value\\nof end-to-end retrieval (instead of just re-ranking) with ColBERT.\\n4.4 Ablation Studies\\n0.220.240.260.280.300.320.340.36\\nMRR@10BERT [CLS]-based dot-product (5-layer)  [A]\\nColBERT via average similarity (5-layer)  [B]\\nColBERT without query augmentation (5-layer)  [C]\\nColBERT (5-layer)  [D]\\nColBERT (12-layer)  [E]\\nColBERT + e2e retrieval (12-layer)  [F]\\nFigure 5: Ablation results on MS MARCO (Dev). Between\\nbrackets is the number of BERT layers used in each model.\\nThe results from §4.2 indicate that ColBERT is highly effective\\ndespite the low cost and simplicity of its late interaction mechanism.\\nTo better understand the source of this effectiveness, we examine a\\nnumber of important details in ColBERT’s interaction and encoder\\narchitecture. For this ablation, we report MRR@10 on the validation\\nset of MS MARCO in Figure 5, which shows our main re-ranking\\nColBERT model [E], with MRR@10 of 34.9%.\\nDue to the cost of training all models, we train a copy of our\\nmain model that retains only the first 5 layers of BERT out of 12\\n(i.e., model [D]) and similarly train all our ablation models for\\n200k iterations with five BERT layers. To begin with, we ask if the\\nfine-granular interaction in late interaction is necessary. Model [A]\\ntackles this question: it uses BERT to produce a single embedding\\nvector for the query and another for the document, extracted from\\nBERT’s [CLS] contextualized embedding and expanded through a\\nlinear layer to dimension 4096 (which equals 𝑁𝑞×128=32×128).\\nRelevance is estimated as the inner product of the query’s and the\\ndocument’s embeddings, which we found to perform better than\\ncosine similarity for single-vector re-ranking. As the results show,\\nthis model is considerably less effective than ColBERT, reinforcing\\nthe importance of late interaction.\\nSubsequently, we ask if our MaxSim-based late interaction is bet-\\nter than other simple alternatives. We test a model [B] that replaces\\nColBERT’s maximum similarity with average similarity. The results\\nsuggest the importance of individual terms in the query paying\\nspecial attention to particular terms in the document. Similarly,\\nthe figure emphasizes the importance of our query augmentation\\nmechanism: without query augmentation [C], ColBERT has a no-\\nticeably lower MRR@10. Lastly, we see the impact of end-to-end\\nretrieval not only on recall but also on MRR@10. By retrieving\\ndirectly from the full collection, ColBERT is able to retrieve to the\\ntop-10 documents missed entirely from BM25’s top-1000.\\n4.5 Indexing Throughput & Footprint\\nLastly, we examine the indexing throughput and space footprint\\nof ColBERT. Figure 6 reports indexing throughput on MS MARCO\\n0 10000 20000 30000 40000 50000\\nThroughput (documents/minute)Basic ColBERT Indexing\\n+multi-GPU document processing\\n+per-batch maximum sequence length\\n+length-based bucketing\\n+multi-core pre-processingFigure 6: Effect of ColBERT’s indexing optimizations on the\\noffline indexing throughput.\\ndocuments with ColBERT and four other ablation settings, which\\nindividually enable optimizations described in §3.4 on top of basic\\nbatched indexing. Based on these throughputs, ColBERT can index\\nMS MARCO in about three hours. Note that any BERT-based model\\nmust incur the computational cost of processing each document\\nat least once. While ColBERT encodes each document with BERT\\nexactly once, existing BERT-based rankers would repeat similar\\ncomputations on possibly hundreds of documents for each query.\\nSetting Dimension( 𝑚) Bytes/Dim Space(GiBs) MRR@10\\nRe-rank Cosine 128 4 286 34.9\\nEnd-to-end L2 128 2 154 36.0\\nRe-rank L2 128 2 143 34.8\\nRe-rank Cosine 48 4 54 34.4\\nRe-rank Cosine 24 2 27 33.9\\nTable 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.\\nTable 4 reports the space footprint of ColBERT under various\\nsettings as we reduce the embeddings dimension and/or the bytes\\nper dimension. Interestingly, the most space-efficient setting, that\\nis, re-ranking with cosine similarity with 24-dimensional vectors\\nstored as 2-byte floats, is only 1% worse in MRR@10 than the most\\nspace-consuming one, while the former requires only 27 GiBs to\\nrepresent the MS MARCO collection.\\n5 CONCLUSIONS\\nIn this paper, we introduced ColBERT, a novel ranking model that\\nemploys contextualized late interaction over deep LMs (in particular,\\nBERT) for efficient retrieval. By independently encoding queries\\nand documents into fine-grained representations that interact via\\ncheap and pruning-friendly computations, ColBERT can leverage\\nthe expressiveness of deep LMs while greatly speeding up query\\nprocessing. Crucially, doing so allows scaling ColBERT to end-\\nto-end neural retrieval directly from a large document collection,\\nwhich can greatly improve recall over existing models. Our results\\nshow that ColBERT is two orders-of-magnitude faster than existing\\nBERT-based models, all while only minimally impacting re-ranking\\nquality and while outperforming every non-BERT baseline.\\nAcknowledgments. OK was supported by the Eltoukhy Family\\nGraduate Fellowship at the Stanford School of Engineering. This\\nresearch was supported in part by affiliate members and other\\nsupporters of the Stanford DAWN project—Ant Financial, Facebook,\\nGoogle, Infosys, NEC, and VMware—as well as Cisco, SAP, and the\\nNSF under CAREER grant CNS-1651570. Any opinions, findings,\\nand conclusions or recommendations expressed in this material are\\nthose of the authors and do not necessarily reflect the views of the\\nNational Science Foundation.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n47REFERENCES\\n[1]Firas Abuzaid, Geet Sethi, Peter Bailis, and Matei Zaharia. 2019. To Index or Not\\nto Index: Optimizing Exact Maximum Inner Product Search. In 2019 IEEE 35th\\nInternational Conference on Data Engineering (ICDE). IEEE, 1250–1261.\\n[2]Zhuyun Dai and Jamie Callan. 2019. Context-Aware Sentence/Passage Term\\nImportance Estimation For First Stage Retrieval. arXiv preprint arXiv:1910.10687\\n(2019).\\n[3]Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with\\nContextual Neural Language Modeling. arXiv preprint arXiv:1905.09217 (2019).\\n[4]Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional\\nneural networks for soft-matching n-grams in ad-hoc search. In Proceedings of the\\neleventh ACM international conference on web search and data mining. 126–134.\\n[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\\nPre-training of deep bidirectional transformers for language understanding. arXiv\\npreprint arXiv:1810.04805 (2018).\\n[6]Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. 2017. TREC\\nComplex Answer Retrieval Overview.. In TREC.\\n[7]Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance\\nmatching model for ad-hoc retrieval. In Proceedings of the 25th ACM International\\non Conference on Information and Knowledge Management. ACM, 55–64.\\n[8]Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen\\nWu, W Bruce Croft, and Xueqi Cheng. 2019. A deep look into neural ranking\\nmodels for information retrieval. arXiv preprint arXiv:1903.06902 (2019).\\n[9]Sebastian Hofstätter and Allan Hanbury. 2019. Let’s measure run time! Extending\\nthe IR replicability infrastructure to include performance aspects. arXiv preprint\\narXiv:1907.04614 (2019).\\n[10] Sebastian Hofstätter, Navid Rekabsaz, Carsten Eickhoff, and Allan Hanbury. 2019.\\nOn the effect of low-frequency terms on neural-IR models. In Proceedings of\\nthe 42nd International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval. 1137–1140.\\n[11] Sebastian Hofstätter, Markus Zlabinger, and Allan Hanbury. 2019. TU Wien@\\nTREC Deep Learning’19–Simple Contextualization for Re-ranking. arXiv preprint\\narXiv:1912.01385 (2019).\\n[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry\\nHeck. 2013. Learning deep structured semantic models for web search using\\nclickthrough data. In Proceedings of the 22nd ACM international conference on\\nInformation & Knowledge Management. 2333–2338.\\n[13] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Efficient Interaction-based Neural\\nRanking with Locality Sensitive Hashing. In The World Wide Web Conference.\\nACM, 2858–2864.\\n[14] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\\nand Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.\\narXiv preprint arXiv:1909.10351 (2019).\\n[15] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity\\nsearch with GPUs. arXiv preprint arXiv:1702.08734 (2017).\\n[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\\nmization. arXiv preprint arXiv:1412.6980 (2014).\\n[17] Ron Kohavi, Alex Deng, Brian Frasca, Toby Walker, Ya Xu, and Nils Pohlmann.\\n2013. Online controlled experiments at large scale. In SIGKDD.\\n[18] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. Cedr:\\nContextualized embeddings for document ranking. In Proceedings of the 42nd\\nInternational ACM SIGIR Conference on Research and Development in Information\\nRetrieval. ACM, 1101–1104.\\n[19] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really\\nBetter than One?. In Advances in Neural Information Processing Systems . 14014–\\n14024.\\n[20] Bhaskar Mitra and Nick Craswell. 2019. An Updated Duet Model for Passage\\nRe-ranking. arXiv preprint arXiv:1903.07666 (2019).\\n[21] Bhaskar Mitra, Nick Craswell, et al .2018. An introduction to neural information\\nretrieval. Foundations and Trends® in Information Retrieval 13, 1 (2018), 1–126.\\n[22] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using\\nlocal and distributed representations of text for web search. In Proceedings of the26th International Conference on World Wide Web. International World Wide Web\\nConferences Steering Committee, 1291–1299.\\n[23] Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz,\\nand Emine Yilmaz. 2019. Incorporating query term independence assumption\\nfor efficient retrieval and ranking using deep neural networks. arXiv preprint\\narXiv:1907.03693 (2019).\\n[24] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A Human-Generated MAchine\\nReading COmprehension Dataset. (2016).\\n[25] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\\narXiv preprint arXiv:1901.04085 (2019).\\n[26] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to\\ndocTTTTTquery. (2019).\\n[27] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-Stage\\nDocument Ranking with BERT. arXiv preprint arXiv:1910.14424 (2019).\\n[28] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\\nExpansion by Query Prediction. arXiv preprint arXiv:1904.08375 (2019).\\n[29] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word\\nrepresentations. arXiv preprint arXiv:1802.05365 (2018).\\n[30] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding\\nthe Behaviors of BERT in Ranking. arXiv preprint arXiv:1904.07531 (2019).\\n[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim-\\nits of transfer learning with a unified text-to-text transformer. arXiv preprint\\narXiv:1910.10683 (2019).\\n[32] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\\nMike Gatford, et al. 1995. Okapi at TREC-3. NIST Special Publication (1995).\\n[33] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.\\n2019. Distilling task-specific knowledge from BERT into simple neural networks.\\narXiv preprint arXiv:1903.12136 (2019).\\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in neural information processing systems. 5998–6008.\\n[35] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,\\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al .\\n2016. Google’s neural machine translation system: Bridging the gap between\\nhuman and machine translation. arXiv preprint arXiv:1609.08144 (2016).\\n[36] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017.\\nEnd-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th\\nInternational ACM SIGIR conference on research and development in information\\nretrieval. 55–64.\\n[37] Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini: Reproducible ranking\\nbaselines using Lucene. Journal of Data and Information Quality (JDIQ) 10, 4\\n(2018), 1–20.\\n[38] Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin. 2019. Critically Examining\\nthe\" Neural Hype\" Weak Baselines and the Additivity of Effectiveness Gains\\nfrom Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR\\nConference on Research and Development in Information Retrieval. 1129–1132.\\n[39] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019.\\nCross-domain modeling of sentence-level evidence for document retrieval. In\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\\ncessing and the 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP). 3481–3487.\\n[40] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert:\\nQuantized 8bit bert. arXiv preprint arXiv:1910.06188 (2019).\\n[41] Hamed Zamani, Mostafa Dehghani, W Bruce Croft, Erik Learned-Miller, and\\nJaap Kamps. 2018. From neural re-ranking to neural ranking: Learning a sparse\\nrepresentation for inverted indexing. In Proceedings of the 27th ACM International\\nConference on Information and Knowledge Management. ACM, 497–506.\\n[42] Le Zhao. 2012. Modeling and solving term mismatch for full-text retrieval. Ph.D.\\nDissertation. Carnegie Mellon University.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n48\\n1, 2, https://arxiv.org/pdf/2104.07186.pdf, COIL: Revisit Exact Lexical Match in Information Retrieval\\nwith Contextualized Inverted List\\nLuyu Gao, Zhuyun Dai, Jamie Callan\\nLanguage Technologies Institute\\nCarnegie Mellon University\\n{luyug, zhuyund, callan}@cs.cmu.edu\\nAbstract\\nClassical information retrieval systems such as\\nBM25 rely on exact lexical match and carry\\nout search efﬁciently with inverted list index.\\nRecent neural IR models shifts towards soft\\nsemantic matching all query document terms,\\nbut they lose the computation efﬁciency of\\nexact match systems. This paper presents\\nCOIL, a contextualized exact match retrieval\\narchitecture that brings semantic lexical match-\\ning. COIL scoring is based on overlapping\\nquery document tokens’ contextualized repre-\\nsentations. The new architecture stores con-\\ntextualized token representations in inverted\\nlists, bringing together the efﬁciency of exact\\nmatch and the representation power of deep\\nlanguage models. Our experimental results\\nshow COIL outperforms classical lexical re-\\ntrievers and state-of-the-art deep LM retrievers\\nwith similar or smaller latency.1\\n1 Introduction\\nWidely used, bag-of-words (BOW) information re-\\ntrieval (IR) systems such as BM25 rely on exact\\nlexical match2between query and document terms.\\nRecent study in neural IR takes a different approach\\nand compute soft matching between all query and\\ndocument terms to model complex matching.\\nThe shift to soft matching in neural IR models\\nattempts to address vocabulary mismatch problems,\\nthat query and the relevant documents use differ-\\nent terms, e.g. cat v.s. kitty, for the same con-\\ncept (Huang et al., 2013; Guo et al., 2016; Xiong\\net al., 2017). Later introduction of contextualized\\nrepresentations (Peters et al., 2018) from deep lan-\\nguage models (LM) further address semantic mis-\\nmatch , that the same term can refer to different\\nconcepts, e.g., bank of river vs. bank in ﬁnance.\\nFine-tuned deep LM rerankers produce token rep-\\nresentations based on context and achieve state-of-\\n1Our code is available at https://github.com/\\nluyug/COIL .\\n2Exact match up to morphological changes.the-art in text ranking with huge performance leap\\n(Nogueira and Cho, 2019; Dai and Callan, 2019b).\\nThough the idea of soft matching all tokens is\\ncarried through the development of neural IR mod-\\nels, seeing the success brought by deep LMs, we\\ntake a step back and ask: how much gain can we get\\nif we introduce contextualized representations back\\nto lexical exact match systems? In other words, can\\nwe build a system that still performs exact query-\\ndocument token matching but compute matching\\nsignals with contextualized token representations\\ninstead of heuristics? This may seem a constraint\\non the model, but exact lexical match produce more\\nexplainable and controlled patterns than soft match-\\ning. It also allows search to focus on only the\\nsubset of documents that have overlapping terms\\nwith query, which can be done efﬁciently with in-\\nverted list index. Meanwhile, using dense contex-\\ntualized token representations enables the model\\nto handle semantic mismatch, which has been a\\nlong-standing problem in classic lexical systems.\\nTo answer the question, we propose a new lexi-\\ncal matching scheme that uses vector similarities\\nbetween query-document overlapping term contex-\\ntualized representations to replace heuristic scor-\\ning used in classical systems. We present COn-\\ntextualized Inverted List (COIL), a new exact lex-\\nical match retrieval architecture armed with deep\\nLM representations. COIL processes documents\\nwith deep LM ofﬂine and produces representations\\nfor each document token. The representations are\\ngrouped by their surface tokens into inverted lists.\\nAt search time, we build representation vectors\\nfor query tokens and perform contextualized ex-\\nact match: use each query token to look up its\\nown inverted list and compute vector similarity\\nwith document vectors stored in the inverted list\\nas matching scores. COIL enables efﬁcient search\\nwith rich-in-semantic matching between query and\\ndocument.\\nOur contributions include 1) introduce a novelarXiv:2104.07186v1  [cs.IR]  15 Apr 2021retrieval architecture, contextualized inverted\\nlists (COIL) that brings semantic matching into\\nlexical IR systems, 2) show matching signals in-\\nduced from exact lexical match can capture com-\\nplicated matching patterns, 3) demonstrate COIL\\nsigniﬁcantly outperform classical and deep LM\\naugmented lexical retrievers as well as state-of-the-\\nart dense retrievers on two retrieval tasks.\\n2 Related Work\\nLexical Retriever Classical IR systems rely on\\nexact lexical match retrievers such as Boolean\\nRetrieval, BM25 (Robertson and Walker, 1994)\\nand statistical language models (Lafferty and Zhai,\\n2001). This type of retrieval model can process\\nqueries very quickly by organizing the documents\\ninto inverted index, where each distinct term has\\nan inverted list that stores information about docu-\\nments it appears in. Nowadays, they are still widely\\nused in production systems. However, these re-\\ntrieval models fall short of matching related terms\\n(vocabulary mismatch) or modeling context of the\\nterms (semantic mismatch). Much early effort\\nwas put into improving exact lexical match retriev-\\ners, such as matching n-grams (Metzler and Croft,\\n2005) or expanding queries with terms from related\\ndocuments (Lavrenko and Croft, 2001). However,\\nthese methods still use BOW framework and have\\nlimited capability of modeling human languages.\\nNeural Ranker In order to deal with vocab-\\nulary mismatch, neural retrievers that rely on\\nsoft matching between numerical text represen-\\ntations are introduced. Early attempts compute\\nsimilarity between pre-trained word embedding\\nsuch as word2vec (Mikolov et al., 2013) and\\nGLoVe (Pennington et al., 2014) to produce match-\\ning score (Ganguly et al., 2015; Diaz et al., 2016).\\nOne more recent approach encodes query and doc-\\nument each into a vector and computes vector sim-\\nilarity (Huang et al., 2013). Later researches real-\\nized the limited capacity of a single vector to en-\\ncode ﬁne-grained information and introduced full\\ninteraction models to perform soft matching be-\\ntween all term vectors (Guo et al., 2016; Xiong\\net al., 2017). In these approaches, scoring is\\nbased on learned neural networks and the hugely\\nincreased computation cost limited their use to\\nreranking a top candidate list generated by a lexical\\nretriever.Deep LM Based Ranker and Retriever Deep\\nLM made a huge impact on neural IR. Fine-\\ntuned Transformer (Vaswani et al., 2017) LM\\nBERT (Devlin et al., 2019) achieved state-of-the-\\nart reranking performance for passages and docu-\\nments (Nogueira and Cho, 2019; Dai and Callan,\\n2019b). As illustrated in Figure 1a, the common\\napproach is to feed the concatenated query docu-\\nment text through BERT and use BERT’s [CLS]\\noutput token to produce a relevance score. The\\ndeep LM rerankers addressed both vocabulary and\\nsemantic mismatch by computing full cross atten-\\ntion between contextualized token representations.\\nLighter deep LM rankers are developed (MacA-\\nvaney et al., 2020; Gao et al., 2020), but their cross\\nattention operations are still too expensive for full-\\ncollection retrieval.\\nLater research therefore resorted to augment-\\ning lexical retrieval with deep LMs by expanding\\nthe document surface form to narrow the vocab-\\nulary gap, e.g., DocT5Query (Nogueira and Lin,\\n2019), or altering term weights to emphasize impor-\\ntant terms, e.g., DeepCT (Dai and Callan, 2019a).\\nSmartly combining deep LM retriever and reranker\\ncan offer additive gain for end performance (Gao\\net al., 2021a). These retrievers however still suffer\\nfrom vocabulary and semantic mismatch as tradi-\\ntional lexical retrievers.\\nAnother line of research continues the work on\\nsingle vector representation and build dense retriev-\\ners, as illustrated in Figure 1b. They store docu-\\nment vectors in a dense index and retrieve them\\nthrough Nearest Neighbours search. Using deep\\nLMs, dense retrievers have achieved promising re-\\nsults on several retrieval tasks (Karpukhin et al.,\\n2020). Later researches show that dense retrieval\\nsystems can be further improved by better train-\\ning (Xiong et al., 2020; Gao et al., 2021b).\\nSingle vector systems have also been extended\\nto multi-vector representation systems. Poly-\\nencoder (Humeau et al., 2020) encodes queries\\ninto a set of vectors. Similarly, Me-BERT (Luan\\net al., 2020) represents documents with a set of vec-\\ntors. A concurrent work ColBERT (Figure 1c) use\\nmultiple vectors to encode both queries and docu-\\nments (Khattab and Zaharia, 2020). In particular, it\\nrepresents a documents with all its terms’ vectors\\nand a query with an expanded set of term vectors.\\nIt then computes all-to-all (Cartesian) soft match\\nbetween the tokens. ColBERT performs interaction\\nas dot product followed pooling operations, whichCLS bank account SEP bank river bankCLS bank account SEP bank river bankCLS bank account SEP bank river bankCLS bank account SEP bank river bankscore(a) Cross-Attention Model (e.g., BERT reranker)\\nCLS bank account CLS bank river bankCLS bank accountCLS bank account\\nCLS bank river bankCLS bank river bankscore (b) Dense Retrievers (e.g., DPR)\\nCLS bank account CLS bank river bankCLS bank accountCLS bank account\\nCLS bank river bankCLS bank river bank\\nEXP EXPscore\\nmax max max\\nEXP\\nEXPEXP\\nEXPmax max\\n(c) ColBERT: All-to-All Match\\nCLS bank account CLS bank river bankCLS bank accountCLS bank account\\nCLS bank river bankCLS bank river bankdot maxsum (d) COIL: Contextualized Exact Match\\nFigure 1: An illustration of reranking/retrieval mechanisms with deep LM, including our proposed model, COIL.\\nBank\\nRiver\\nAccountBank\\nAccount\\nTraditional Inverted Lists Querydocid: 3\\ntf: 2docid: 9\\ntf: 1docid: 1\\ntf: 1docid: 2\\ntf: 1docid: 4\\ntf: 1docid: 5\\ntf: 2docid: 1\\ntf: 1docid: 3\\ntf: 1docid: 6\\ntf: 1....\\n....BM25\\xa0\\nscoringBM25\\xa0\\nscoring\\nBM25\\xa0\\nscoring\\nFigure 2: An illustration of traditional inverted lists.\\nThe inverted list maps a term to the list of documents\\nwhere the term occurs. Retriever looks up query terms’\\ninverted lists and scores those documents with stored\\nstatistics such as term frequency (tf).\\nallows it to also leverage a dense index to do full\\ncorpus retrieval. However, since ColBERT encodes\\na document with all tokens, it adds another order\\nof magnitude of index complexity to all aforemen-\\ntioned methods: document tokens in the collection\\nneed to be stored in a single huge index and con-\\nsidered at query time. Consequently, ColBERT is\\nengineering and hardware demanding.\\n3 Methodologies\\nIn this section, we ﬁrst provide some preliminaries\\non exact lexical match systems. Then we discuss\\nCOIL’s contextualized exact match design and how\\nits search index is organized. We also give a com-\\nparison between COIL and other popular retrievers.\\nBank\\nRiver\\nAccountBank\\nAccount\\nContextualized Inverted Lists Querydocid\\xa0 [1 3 6 7]\\ndocid\\xa0 [1 2 4 5 5 9]\\ndocid\\xa0 [3 3 9]vectors\\nvectors\\nvectorsCLSdocid\\xa0 [1 2 3 4 .............C]\\nvectors ...CLSmatrix\\nproduct\\nmatrix\\nproduct\\nmatrix\\nproductFigure 3: COIL’s index and retrieval architecture.\\nCOIL-tok relies on the exact token matching (lower).\\nCOIL-full includes in addition CLS matching (upper).\\n3.1 Preliminaries\\nClassic lexical retrieval system relies on overlap-\\nping query document terms under morphological\\ngeneralization like stemming, in other words, exact\\nlexical match , to score query document pair. A\\nscoring function is deﬁned as a sum of matched\\nterm scores. The scores are usually based on statis-\\ntics like term frequency ( tf). Generally, we can\\nwrite,\\ns=X\\nt2q\\\\d\\x1bt(hq(q;t);hd(d;t)) (1)\\nwhere for each overlapping term tbetween query q\\nand document d, functionshqandhdextract terminformation and a term scoring function \\x1btcom-\\nbines them. A popular example is BM25, which\\ncomputes,\\nsBM25 =X\\nt2q\\\\didf(t)hBM25\\nq(q;t)hBM25\\nd(d;t)\\nhBM25\\nq(q;t) =tft;q(1 +k2)\\ntft;q+k2\\nhBM25\\nd(d;t) =tft;d(1 +k1)\\ntft;d+k1(1\\n2, 3, https://arxiv.org/pdf/2106.14807.pdf, A Few Brief Notes on DeepImpact, COIL, and a Conceptual\\nFramework for Information Retrieval Techniques\\nJimmy Lin andXueguang Ma\\nDavid R. Cheriton School of Computer Science\\nUniversity of Waterloo\\nAbstract\\nRecent developments in representational learn-\\ning for information retrieval can be organized\\nin a conceptual framework that establishes two\\npairs of contrasts: sparse vs. dense representa-\\ntions and unsupervised vs. learned representa-\\ntions. Sparse learned representations can fur-\\nther be decomposed into expansion and term\\nweighting components. This framework al-\\nlows us to understand the relationship between\\nrecently proposed techniques such as DPR,\\nANCE, DeepCT, DeepImpact, and COIL, and\\nfurthermore, gaps revealed by our analysis\\npoint to “low hanging fruit” in terms of tech-\\nniques that have yet to be explored. We present\\na novel technique dubbed “uniCOIL”, a simple\\nextension of COIL that achieves to our knowl-\\nedge the current state-of-the-art in sparse re-\\ntrieval on the popular MS MARCO passage\\nranking dataset. Our implementation using\\nthe Anserini IR toolkit is built on the Lucene\\nsearch library and thus fully compatible with\\nstandard inverted indexes.\\n1 Introduction\\nWe present a novel conceptual framework for un-\\nderstanding recent developments in information re-\\ntrieval that organizes techniques along two dimen-\\nsions. The ﬁrst dimension establishes the contrast\\nbetween sparse and dense vector representations\\nfor queries and documents.1The second dimen-\\nsion establishes the contrast between unsupervised\\nand learned (supervised) representations. Figure 1\\nillustrates our framework.\\nRecent proposals for dense retrieval, exempliﬁed\\nby DPR (Karpukhin et al., 2020) and ANCE (Xiong\\net al., 2021), but also encompassing many other\\ntechniques (Gao et al., 2021b; Hofstätter et al.,\\n2020; Qu et al., 2021; Hofstätter et al., 2021; Lin\\n1Consistent with parlance in information retrieval, we use\\n“document” throughout this paper in a generic sense to refer to\\nthe unit of retrieved text. To be more precise, our experiments\\nare in fact focused on passage retrieval.Dense Sparse\\nSupervised DPR, ANCE DeepImpact, COIL\\nUnsupervised LSI, LDA BM25, tf–idf\\nTable 1: Our conceptual framework for organizing re-\\ncent developments in information retrieval.\\net al., 2021), can be understood as learned dense\\nrepresentations for retrieval. This is formulated\\nas a representational learning problem where the\\ntask is to learn (transformer-based) encoders that\\nmap queries and documents into dense ﬁxed-width\\nvectors (768 dimensions is typical) in which inner\\nproducts between queries and relevant documents\\nare maximized, based on supervision signals from\\na large dataset such as the MS MARCO passage\\nranking test collection (Bajaj et al., 2018). See Lin\\net al. (2020) for a survey.\\nDense retrieval techniques are typically com-\\npared against a bag-of-words exact match ranking\\nmodel such as BM25, which in this context can be\\nunderstood as unsupervised sparse retrieval. Al-\\nthough it may be unnatural to describe BM25 in\\nthis way, it is technically accurate: each document\\nis represented by a sparse vector where each dimen-\\nsion corresponds to a unique term in the vocabulary,\\nand the scoring function assigns a weight to each di-\\nmension. As with dense retrieval, query–document\\nscores are computed via inner products.\\nWhat about learned sparse retrieval? The most\\nprominent recent example of this in the literature\\nis DeepCT (Dai and Callan, 2019), which uses\\na transformer to learn term weights based on a re-\\ngression model, with the supervision signal coming\\nfrom the MS MARCO passage ranking test collec-\\ntion.2DeepCT has an interesting “quirk”: in truth,\\nit only learns the term frequency (tf) component\\nof term weights, but still relies on the remaining\\n2Learning sparse representations is by no means a new idea.\\nThe earliest example we are aware of is Wilbur (2001), who\\nattempted to learn global term weights using TREC data, but\\nthe idea likely dates back even further.arXiv:2106.14807v1  [cs.IR]  28 Jun 2021parts of the BM25 scoring function via the gen-\\neration of pseudo-documents. This approach also\\nhas a weakness: it only assigns weights to terms\\nthat are already present in the document, which\\nlimits retrieval to exact match. This is an impor-\\ntant limitation that is addressed by the use of dense\\nrepresentations, which are capable of capturing se-\\nmantic matches.\\nThese two issues were resolved by the recently\\nproposed DeepImpact model (Mallia et al., 2021),\\nwhich also belongs in the family of learned sparse\\nrepresentations. DeepImpact brought together two\\nkey ideas: the use of document expansion to iden-\\ntify dimensions in the sparse vector that should\\nhave non-zero weights and a term weighting model\\nbased on a pairwise loss between relevant and non-\\nrelevant texts with respect to a query. Expansion\\nterms were identiﬁed by doc2query–T5 (Nogueira\\nand Lin, 2019), a sequence-to-sequence model for\\ndocument expansion that predicts queries for which\\na text would be relevant. Since the DeepImpact\\nscoring model directly predicts term weights that\\nare then quantized, it would be more accurate to\\ncall these weights learned impacts, since query–\\ndocument scores are simply the sum of weights of\\ndocument terms that are found in the query. Calling\\nthese impact scores draws an explicit connection to\\na thread of research in information retrieval dating\\nback two decades (Anh et al., 2001).\\nThe recently proposed COIL architecture (Gao\\net al., 2021a) presents an interesting case for this\\nconceptual framework. Where does it belong? The\\nauthors themselves describe COIL as “a new ex-\\nact lexical match retrieval architecture armed with\\ndeep LM representations”. COIL produces repre-\\nsentations for each document token that are then\\ndirectly stored in the inverted index, where the\\nterm frequency usually goes in an inverted list.\\nAlthough COIL is perhaps best described as the\\nintellectual descendant of ColBERT (Khattab and\\nZaharia, 2020), another way to think about it within\\nour conceptual framework is that instead of assign-\\ningscalar weights to terms in a query, the “scoring”\\nmodel assigns each term a vector “weight”. Query\\nevaluation in COIL involves accumulating inner\\nproducts instead of scalar weights.\\nOur conceptual framework highlights a ﬁnal\\nclass of techniques: unsupervised dense represen-\\ntations. While there is little work in this space of\\nlate, it does describe techniques such as LSI (Deer-\\nwester et al., 1990; Atreya and Elkan, 2010) andLDA (Wei and Croft, 2006), which have been previ-\\nously explored. Thus, all quadrants in our proposed\\nconceptual framework are populated with known\\nexamples from the literature.\\n2 Comments and Observations\\nBased on this framework, we can make a number of\\ninteresting observations that highlight obvious next\\nsteps in the development of retrieval techniques.\\nWe discuss as follows:\\nChoice of bases. Retrieval techniques using learned\\ndense representations and learned sparse represen-\\ntations present an interesting contrast. Nearly all\\nrecent proposals take advantage of transformers, so\\nthat aspect of the design is not a salient difference.\\nThe critical contrast is the basis of the vector rep-\\nresentations: In sparse approaches, the basis of the\\nvector space remains ﬁxed to the corpus vocabulary,\\nand thus techniques such as DeepCT, COIL, and\\nDeepImpact can be understood as term weighting\\nmodels. In dense approaches, the model is given\\nthe freedom to choose a new basis derived from\\ntransformer representations. This change in basis\\nallows the encoder to represent the “meaning” of\\ntexts in relatively small ﬁxed-width vectors (com-\\npared to sparse vectors that may have millions of\\ndimensions). This leads us to the next important\\nobservation:\\nExpansions for sparse representation. Without\\nsome form of expansion, learned sparse represen-\\ntations remain limited to (better) exact matching\\nbetween queries and documents. The nature of\\nsparse representations means that it is impractical\\nto consider non-zero weights for allelements in\\nthe vector (i.e., the vocabulary space). Thus, docu-\\nment expansion serves the critical role of proposing\\na set of candidate terms that should receive non-\\nzero weights; since the number of candidate terms\\nis small compared to the vocabulary size, the re-\\nsulting vector remains sparse. Without expansion,\\nlearned sparse representations cannot address the\\nvocabulary mismatch problem (Furnas et al., 1987),\\nbecause document terms not present in the query\\ncannot contribute any score. For DeepImpact, this\\nexpansion is performed by doc2query–T5, but in\\nprinciple we can imagine other methods also. This\\nleads us to the next important observation:\\nRelating DeepCT, DeepImpact, and COIL. The up-\\nshot of the above analysis is that retrieval tech-\\nniques based on learned sparse representations\\nshould be divided into an expansion model andSparse Representations MRR@10 Notes\\nTerm Weighting Expansion\\n(1a) BM25 None 0.184 copied from (Nogueira and Lin, 2019)\\n(1b) BM25 doc2query–T5 0.277 copied from (Nogueira and Lin, 2019)\\n(2a) DeepCT None 0.243 copied from (Dai and Callan, 2019)\\n(2b) DeepCT doc2query–T5 ? no publicly reported ﬁgure\\n(2c) DeepImpact None ? no publicly reported ﬁgure\\n(2d) DeepImpact doc2query–T5 0.326 copied from (Mallia et al., 2021)\\n(2e) COIL-tok ( d= 32 ) None 0.341 copied from (Gao et al., 2021a)\\n(2f) COIL-tok ( d= 32 ) doc2query–T5 0.361 our experiment\\n(2g) uniCOIL None 0.315 our experiment\\n(2h) uniCOIL doc2query–T5 0.352 our experiment\\nDense Representations MRR@10 Notes\\n(3a) ColBERT 0.360 copied from (Khattab and Zaharia, 2020)\\n(3b) ANCE 0.330 copied from (Xiong et al., 2021)\\n(3c) DistillBERT 0.323 copied from (Hofstätter et al., 2020)\\n(3d) RocketQA 0.370 copied from (Qu et al., 2021)\\n(3e) TAS-B 0.347 copied from (Hofstätter et al., 2021)\\n(3f) TCT-ColBERTv2 0.359 copied from (Lin et al., 2021)\\nDense–Sparse Hybrids MRR@10 Notes\\n(4a) CLEAR 0.338 copied from (Gao et al., 2021b)\\n(4b) COIL-full 0.355 copied from (Gao et al., 2021a)\\n(4c) TCT-ColBERTv2 + BM25 (1a) 0.369 copied from (Lin et al., 2021)\\n(4d) TCT-ColBERTv2 + doc2query–T5 (1b) 0.375 copied from (Lin et al., 2021)\\n(4e) TCT-ColBERTv2 + DeepImpact (2d) 0.378 our experiment\\n(4f) TCT-ColBERTv2 + uniCOIL (2h) 0.378 our experiment\\n(4g) TCT-ColBERTv2 + COIL (2f) 0.382 our experiment\\nTable 2: Results on the development queries of the MS MARCO passage ranking task.\\na term weighting model. For example, DeepCT\\nperforms no expansion and uses a regression-based\\nscoring model. DeepImpact performs document ex-\\npansion and uses a pairwise scoring model. COIL\\nperforms no expansion and uses a “scoring” model\\nthat generates a contextualized “weight vector” (in-\\nstead of a scalar weight). This breakdown suggests\\na number of obvious experiments that help us un-\\nderstand the contributions of these components,\\nwhich we report next.\\n3 Experiments\\nOur proposed conceptual framework can be used\\nto organize results from the literature, which are\\nshown in Table 2 on the development queries of\\nthe MS MARCO passage ranking task (Bajaj et al.,\\n2018). Some of these entries represent ﬁgures di-\\nrectly copied from previous papers (with references\\nshown), while others are novel experimental condi-\\ntions that we report.\\nThe ﬁrst main block of the table shows retrieval\\nwith sparse representations. Row (1a) shows the\\nBM25 baseline, and row (1b) provides the effective-\\nness of doc2query–T5 expansion. In both cases, the\\nterm weights are from the BM25 scoring function,and hence unsupervised. Learned sparse retrieval\\ntechniques are shown in row group (2). Separat-\\ning the term weighting component from the ex-\\npansion component allows us to identify gaps in\\nmodel conﬁgurations that would be interesting to\\nexplore. For example, in row (2a), DeepCT pro-\\nposed a regression-based term weighting model,\\nbut performed no expansion. However, the term\\nweighting model can be applied to expanded doc-\\numents, as in row (2b); to our knowledge, this\\nconﬁguration has not been publicly reported.\\nSimilarly, DeepImpact combined doc2query–T5\\nas an expansion model and a term weighting model\\ntrained with pairwise loss. To better understand\\nthe contributions of each component, we could\\nrun the term weighting model without document\\nexpansion, as outlined in row (2c). This ablation\\nexperiment was not reported in Mallia et al. (2021),\\nbut would be interesting to conduct.\\nIn row (2e) we report the published results of\\nCOIL-tok (token dimension d= 32 ), which is the\\nsparse component in the full COIL model (which\\nis a dense–sparse hybrid). Through the lens of\\nour conceptual framework, a number of extensions\\nbecome immediately obvious. COIL can be com-bined with doc2query–T5. Using source code pro-\\nvided by the authors,3we trained such a model\\nfrom scratch, using the same hyperparameters as\\nthe authors. This variant leads to a nearly two-point\\ngain in effectiveness, as shown in row (2f).\\nIn another interesting extension, if we reduce the\\ntoken dimension of COIL to one, the model degen-\\nerates into producing scalar weights, which then\\nbecomes directly comparable to DeepCT, row (2a)\\nand the “no-expansion” variant of DeepImpact, row\\n(2c). These comparisons isolate the effects of differ-\\nent term weighting models. We dub this variant of\\nCOIL “uniCOIL”, on top of which we can also add\\ndoc2query–T5, which produces a fair comparison\\nto DeepImpact, row (2d). The original formulation\\nof COIL, even with a token dimension of one, is\\nnot directly amenable to retrieval using inverted\\nindexes because weights can be negative. To ad-\\ndress this issue, we added a ReLU operation on\\nthe output term weights of the base COIL model to\\nforce the model to generate non-negative weights.\\nOnce again, we retrained the model from scratch\\nusing the same hyperparameters provided by the\\nauthors. When encoding the corpus, we quantized\\nthese weights into 8 bits to obtain impact scores;\\nquery weights are similarly quantized. After these\\nmodiﬁcations, uniCOIL is directly compatible with\\ninverted indexes. Our experimental results are re-\\nported with the Anserini toolkit (Yang et al., 2017,\\n2018), which is built on Lucene.\\nIt is no surprise that uniCOIL without doc2query–\\nT5, row (2g), is less effective than COIL-tok ( d=\\n32), row (2e). However, uniCOIL with doc2query–\\nT5, row (2h), outperforms COIL-tok without need-\\ning any specialized retrieval infrastructure—the\\nweights are just impact scores, like in DeepImpact.\\nThese results suggest that contextualized “weight\\nvectors” in COIL aren’t necessary to achieve good\\neffectiveness—adding expansion appears sufﬁcient\\nto make up for the lost expressivity of weight vec-\\ntors, as shown in row (2h) vs. row (2e). To our\\nknowledge, our uniCOIL model, row (2h), repre-\\nsents the state of the art in sparse retrieval using\\nlearned impact weights, beating DeepImpact by\\naround two points.\\nThe second main block of Table 2 provides a\\nnumber of comparable dense retrieval results from\\nthe literature. The highest score that we are aware\\nof is RocketQA (Qu et al., 2021), whose effective-\\nness beats all known sparse conﬁgurations. Note\\n3https://github.com/luyug/COILthat ColBERT (Khattab and Zaharia, 2020) uses\\nthe more expressive MaxSim operator to compare\\nquery and document representations; all other tech-\\nniques use inner products.\\nThe ﬁnal block of Table 2 presents the results of\\ndense–sparse hybrids. Lin et al. (2021) reported\\nthe results of dense–sparse hybrids when TCT-\\nColBERTv2, row (3f), is combined with BM25,\\nrow (1a), and doc2query–T5, row (1b). To this,\\nwe added fusion with DeepImpact, uniCOIL, and\\nCOIL-tok (d= 32 ). For a fair comparison, we fol-\\nlowed the same technique for combining dense and\\nsparse results as Lin et al. (2021), which is from Ma\\net al. (2021). For each query q, we used the corre-\\nsponding dense and sparse techniques to retrieve\\ntop-1k documents. The ﬁnal fusion score of each\\ndocument is calculated by sdense +\\x0b\\x01ssparse . Since\\nthe range of the two different scores are quite differ-\\nent, we ﬁrst normalized the scores into range(0, 1).\\nThe\\x0bwas tuned in the range(0, 2) with a simple\\nline search on a subset of the MS MARCO passage\\ntraining set.\\nWith these hybrid combinations, we are able\\nto achieve, to our knowledge, the highest reported\\nscores on the MS MARCO passage ranking task for\\nsingle-stage techniques (i.e., no reranking). Note\\nthat, as before, uniCOIL is compatible with stan-\\ndard inverted indexes, unlike COIL-tok, which re-\\nquires custom infrastructure.\\n4 Next Steps\\nIn most recent work, dense retrieval techniques are\\ncompared to BM25 and experiments show that they\\nhandily win. However, this is not a fair compari-\\nson, since BM25 is unsupervised, whereas dense\\nretrieval techniques exploit supervised relevance\\nsignals from large datasets. A more appropriate\\ncomparison would be between learned dense vs.\\nsparse representations—and there, no clear win-\\nner emerges at present. However, it seems clear\\nthat they are complementary, as hybrid approaches\\nappear to be more effective than either alone.\\nAn important point to make here is that neu-\\nral networks, particularly transformers, have not\\nmade sparse representations obsolete. Both dense\\nand sparse learned representations clearly exploit\\ntransformers—the trick is that the latter class of\\ntechniques then “projects” the learned knowledge\\nback into the sparse vocabulary space. This al-\\nlows us to reuse decades of innovation in inverted\\nindexes (e.g., integer coding techniques to com-press inverted lists) and efﬁcient query evaluation\\nalgorithms (e.g., smart skipping to reduce query\\nlatency): for example, the Lucene index used in\\nour uniCOIL experiments is only 1.3 GB, com-\\npared to \\x1840 GB for COIL-tok, 26 GB for TCT-\\nColBERTv2, and 154 GB for ColBERT. We note,\\nhowever, that with dense retrieval techniques, ﬁxed-\\nwidth vectors can be approximated with binary\\nhash codes, yielding far more compact representa-\\ntions with sacriﬁcing much effectiveness (Yamada\\net al., 2021). Once again, no clear winner emerges\\nat present.\\nThe complete design space of modern informa-\\ntion retrieval techniques requires proper accounting\\nof the tradeoffs between output quality (effective-\\nness), time (query latency), and space (index size).\\nHere, we have only focused on the ﬁrst aspect.\\nLearned representations for information retrieval\\nare clearly the future, but the advantages and dis-\\nadvantages of dense vs. sparse approaches along\\nthese dimensions are not yet fully understood. It’ll\\nbe exciting to see what comes next!\\n5 Acknowledgments\\nThis research was supported in part by the Canada\\nFirst Research Excellence Fund and the Natural Sci-\\nences and Engineering Research Council (NSERC)\\nof Canada. Computational resources were provided\\nby Compute Ontario and Compute Canada.\\nReferences\\nV o Ngoc Anh, Owen de Kretser, and Alistair Moffat.\\n2001. Vector-space ranking with effective early ter-\\nmination. In Proceedings of the 24th Annual Inter-\\nnational ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval (SIGIR 2001) ,\\npages 35–42, New Orleans, Louisiana.\\nAvinash Atreya and Charles Elkan. 2010. Latent se-\\nmantic indexing (LSI) fails for TREC collections.\\nSIGKDD Explorations , 12(2):5–10.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\\nMir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-\\nwary, and Tong Wang. 2018. MS MARCO: A Hu-\\nman Generated MAchine Reading COmprehension\\nDataset. arXiv:1611.09268v3 .\\nZhuyun Dai and Jamie Callan. 2019. Context-aware\\nsentence/passage term importance estimation for\\nﬁrst stage retrieval. arXiv:1910.10687 .\\nScott Deerwester, Susan T. Dumais, George W. Furnas,\\nThomas K. Landauer, and Richard Harshman. 1990.Indexing by latent semantic analysis. Journal of\\nthe Association for Information Science , 41(6):391–\\n407.\\nGeorge W. Furnas, Thomas K. Landauer, Louis M.\\nGomez, and Susan T. Dumais. 1987. The vo-\\ncabulary problem in human-system communication.\\nCommunications of the ACM , 30(11):964–971.\\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2021a.\\nCOIL: Revisit exact lexical match in information\\nretrieval with contextualized inverted list. In Pro-\\nceedings of the 2021 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies , pages\\n3030–3042.\\nLuyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Ben-\\njamin Van Durme, and Jamie Callan. 2021b. Com-\\nplementing lexical retrieval with semantic residual\\nembedding. In Proceedings of the 43rd European\\nConference on Information Retrieval (ECIR 2021),\\nPart I , pages 146–160.\\nSebastian Hofstätter, Sophia Althammer, Michael\\nSchröder, Mete Sertkan, and Allan Hanbury.\\n2020. Improving efﬁcient neural ranking mod-\\nels with cross-architecture knowledge distillation.\\narXiv:2010.02666 .\\nSebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong\\nYang, Jimmy Lin, and Allan Hanbury. 2021. Ef-\\nﬁciently teaching an effective dense retriever with\\nbalanced topic aware sampling. In Proceedings of\\nthe 44th Annual International ACM SIGIR Confer-\\nence on Research and Development in Information\\nRetrieval (SIGIR 2021) .\\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 6769–\\n6781.\\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Ef-\\nﬁcient and effective passage search via contextual-\\nized late interaction over BERT. In Proceedings of\\nthe 43rd International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval\\n(SIGIR 2020) , pages 39–48.\\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates.\\n2020. Pretrained transformers for text ranking:\\nBERT and beyond. arXiv:2010.06467 .\\nSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\\n2021. In-batch negatives for knowledge distillation\\nwith tightly-coupled teachers for dense retrieval. In\\nProceedings of the 6th Workshop on Representation\\nLearning for NLP .\\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy\\nLin. 2021. A replication study of dense passage re-\\ntriever. arXiv:2104.05740 .Antonio Mallia, Omar Khattab, Torsten Suel, and\\nNicola Tonellotto. 2021. Learning passage impacts\\nfor inverted indexes. In Proceedings of the 44th An-\\nnual International ACM SIGIR Conference on Re-\\nsearch and Development in Information Retrieval\\n(SIGIR 2021) .\\nRodrigo Nogueira and Jimmy Lin. 2019. From\\ndoc2query to docTTTTTquery.\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\\nand Haifeng Wang. 2021. RocketQA: An opti-\\nmized training approach to dense passage retrieval\\nfor open-domain question answering. In Proceed-\\nings of the 2021 Conference of the North Ameri-\\ncan Chapter of the Association for Computational\\nLinguistics: Human Language Technologies , pages\\n5835–5847.\\nXing Wei and W. Bruce Croft. 2006. LDA-based doc-\\nument models for ad-hoc retrieval. In Proceedings\\nof the 29th Annual International ACM SIGIR Con-\\nference on Research and Development in Informa-\\ntion Retrieval (SIGIR 2006) , pages 178–185, Seattle,\\nWashington.\\nW. John Wilbur. 2001. Global term weights for docu-\\nment retrieval learned from TREC data. Journal of\\nInformation Science , 27(5):303–310.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2021. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval. In Proceedings of the 9th International Con-\\nference on Learning Representations (ICLR 2021) .\\nIkuya Yamada, Akari Asai, and Hannaneh Ha-\\njishirzi. 2021. Efﬁcient passage retrieval with\\nhashing for open-domain question answering.\\narXiv:2106.00882 .\\nPeilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:\\nenabling the use of Lucene for information retrieval\\nresearch. In Proceedings of the 40th Annual Inter-\\nnational ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval (SIGIR 2017) ,\\npages 1253–1256, Tokyo, Japan.\\nPeilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini:\\nreproducible ranking baselines using Lucene. Jour-\\nnal of Data and Information Quality , 10(4):Article\\n16.\\n3, 4, https://arxiv.org/pdf/2301.03266.pdf, Doc2Query--: When Less is More\\nMitko Gospodinov1, Sean MacAvaney2, and Craig Macdonald2\\nUniversity of Glasgow\\n12024810G@student.gla.ac.uk\\n2{first}.{last}@glasgow.ac.uk\\nAbstract. Doc2Query — the process of expanding the content of a\\ndocument before indexing using a sequence-to-sequence model — has\\nemerged as a prominent technique for improving the ﬁrst-stage retrieval\\neﬀectivenessofsearchengines.However,sequence-to-sequencemodelsare\\nknown to be prone to “hallucinating” content that is not present in the\\nsource text. We argue that Doc2Query is indeed prone to hallucination,\\nwhich ultimately harms retrieval eﬀectiveness and inﬂates the index size.\\nIn this work, we explore techniques for ﬁltering out these harmful queries\\nprior to indexing. We ﬁnd that using a relevance model to remove poor-\\nquality queries can improve the retrieval eﬀectiveness of Doc2Query by\\nup to 16%, while simultaneously reducing mean query execution time by\\n23% and cutting the index size by 33%. We release the code, data, and\\na live demonstration to facilitate reproduction and further exploration.1\\n1 Introduction\\nNeural network models, particularly those based on contextualised language\\nmodels, have been shown to improve search eﬀectiveness [3]. While some ap-\\nproaches focus on re-ranking document sets from a ﬁrst-stage retrieval function\\nto improve precision [27], others aim to improve the ﬁrst stage itself [4]. In this\\nwork, we focus on one of these ﬁrst-stage approaches: Doc2Query [29]. This ap-\\nproach trains a sequence-to-sequence model (e.g., T5 [33]) to predict queries that\\nmay be relevant to a particular text. Then, when indexing, this model is used\\ntoexpandthe document by generating a collection of queries and appending\\nthem to the document. Though computationally expensive at index time [34],\\nthis approach has been shown to be remarkably eﬀective even when retrieving\\nusing simple lexical models like BM25 [28]. Numerous works have shown that\\nthe approach can produce a high-quality pool of results that are eﬀective for\\nsubsequent stages in the ranking pipeline [19,20,23,40].\\nHowever, sequence-to-sequence models are well-known to be prone to gener-\\nate content that does not reﬂect the input text – a defect known in literature\\nas “hallucination” [25]. We ﬁnd that existing Doc2Query models are no excep-\\ntion. Figure 1 provides example generated queries from the state-of-the-art T5\\nDoc2Query model [28]. In this example, we see that many of the generated\\nqueries cannot actually be answered by the source passage (score \\x141).\\n1https://github.com/terrierteam/pyterrier_doc2queryarXiv:2301.03266v3  [cs.IR]  27 Feb 20232 Gospodinov et al.\\nOriginal Passage: Barley (Hordeum vulgare L.), a\\nmember of the grass family, is a major cereal grain. It\\nwas one of the ﬁrst cultivated grains and is now grown\\nwidely. Barley grain is a staple in Tibetan cuisine and\\nwaseatenwidelybypeasantsinMedievalEurope.Bar-\\nley has also been used as animal fodder, as a source\\nof fermentable material for beer and certain distilled\\nbeverages,andasacomponentofvarioushealthfoods.Generated Queries: (1) where does barley originate\\nfrom\\x01(2) what is the name of the cereal grain used\\nin tibetan cooking? \\x01(3) what is barley used for \\x01(1)\\nwhat is barley in food \\x01(0) what is bare wheat \\x01(3)\\nwhat family of organisms is barley in \\x01(1) why is bar-\\nley important in tibetan diet \\x01(3) what is barley \\x01\\n(2) where is barley grown \\x01(1) where was barley ﬁrst\\ngrown and eaten \\x01(1) where was barley ﬁrst used ...\\nFig. 1.Example passage from MS MARCO and generated queries using the T5\\nDoc2Query model. The relevance of each query to the passage is scored by the au-\\nthors on a scale of 0–3 using the TREC Deep Learning passage relevance criteria.\\nBased on this observation, we hypothesise that retrieval performance of\\nDoc2Querywouldimproveifhallucinatedquerieswereremoved.Inthispaper,we\\nconduct experiments where we apply a new ﬁltering phase that aims to remove\\npoor queries prior to indexing. Given that this approach removes queries, we\\ncall the approach Doc2Query-- (Doc2Query-minus-minus). Rather than training\\na new model for this task, we identify that relevance models are already ﬁt for\\nthis purpose: they estimate how relevant a passage is to a query. We therefore\\nexplore ﬁltering strategies that make use of existing neural relevance models.\\nThrough experimentation on the MS MARCO dataset, we ﬁnd that our ﬁl-\\ntering approach can improve the retrieval eﬀectiveness of indexes built using\\nDoc2Query-- by up to 16%; less can indeed be more. Meanwhile, ﬁltering nat-\\nurally reduces the index size, lowering storage and query-time computational\\ncosts. Finally, we conduct an exploration of the index-time overheads introduced\\nbytheﬁlteringprocessandconcludethatthegainsfromﬁlteringmorethanmake\\nup for the additional time spent generating more queries. The approach also has\\na positive impact on the environmental costs of applying Doc2Query; the same\\nretrieval eﬀectiveness can be achieved with only about a third of the compu-\\ntational cost when indexing. To facilitate last-metre, last-mile, and complete\\nreproduction eﬀorts [36], we release the code, indices, and ﬁltering scores.1In\\nsummary, we contribute a technique to improve the eﬀectiveness and eﬃciency\\nof Doc2Query by ﬁltering out queries that do not reﬂect the original passage.\\n2 Related Work\\nThe classical lexical mismatch problem is a key one in information retrieval -\\ndocuments that do not contain the query terms may not be retrieved. In the\\nliterature, various approaches have addressed this: query reformulation – includ-\\ning stemming, query expansion models (e.g. Rocchio, Bo1 [1], RM3 [12]) – and\\ndocument expansion [9,30,35]. Classically, query expansion models have been\\npopular, as they avoid the costs associated with making additional processing\\nfor each document needed for document expansion. However, query expansion\\nmay result in reduced performance [11], as queries are typically short and the\\nnecessary evidence to understand the context of the user is limited.Doc2Query--: When Less is More 3\\nThe application of latent representations of queries and documents, such\\nas using latent semantic indexing [8] allow retrieval to not be driven directly\\nby lexical signals. More recently, transformer-based language models (such as\\nBERT [6]) have resulted in representations of text where the contextualised\\nmeaning of words are accounted for. In particular, in dense retrieval, queries\\nand documents are represented in embeddings spaces [14,37], often facilitated\\nby Approximate Nearest Neighbour (ANN) data structures [13]. However, even\\nwhen using ANN, retrieval can still be ineﬃcient or insuﬃciently eﬀective [15].\\nOthers have explored approaches for augmenting lexical representations with\\nadditional terms that may be relevant. In this work, we explore Doc2Query [29],\\nwhich uses a sequence-to-sequence model that maps a document to queries that\\nit might be able to answer. By appending these generated queries to a docu-\\nment’s content before indexing, the document is more likely to be retrieved for\\nuser queries when using a model like BM25. An alternative style of document\\nexpansion, proposed by MacAvaney et al. [19] and since used by several other\\nmodels (e.g., [10,39,40]), uses the built-in Masked Language Modelling (MLM)\\nmechanism. MLM expansion generates individual tokens to append to the docu-\\nment as a bag of words (rather than as a sequence). Although MLM expansion is\\nalso prone to hallucination,2the bag-of-words nature of MLM expansion means\\nthat individual expansion tokens may not have suﬃcient context to apply ﬁl-\\ntering eﬀectively. We therefore focus only on sequence-style expansion and leave\\nthe exploration of MLM expansion for future work.\\n3 Doc2Query--\\nDoc2Query-- consists of two phases: a generation phrase and a ﬁltering phase.\\nIn the generation phase, a Doc2Query model generates a set of nqueries that\\neach document might be able to answer. However, as shown in Figure 1, not\\nall of the queries are necessarily relevant to the document. To mitigate this\\nproblem, Doc2Query-- then proceeds to a ﬁltering phase, which is responsible\\nfor eliminating the generated queries that are least relevant to the source doc-\\nument. Because hallucinated queries contain details not present in the original\\ntext (by deﬁnition), we argue that hallucinated queries are less useful for re-\\ntrieval than non-hallucinated ones. Filtering is accomplished by retaining only\\nthe most relevant pproportion of generated queries over the entire corpus. The\\nretained queries are then concatenated to their corresponding documents prior\\nto indexing, as per the existing Doc2Query approach.\\nMore formally, consider an expansion function ethat maps a document to n\\nqueries: e:D7!Qn. In Doc2Query, each document in corpus Dare concate-\\nnatedwiththeirexpansionqueries,forminganewcorpus D0=fConcat (d; e(d))j\\nd2Dg,whichisthen indexedbya retrievalsystem.Doc2Query--addsaﬁltering\\nmechanism that uses a relevance model that maps a query and document to a\\nreal-valued relevance score s:Q\\x02D7!R(with larger values indicating higher\\n2For instance, we ﬁnd that SPLADE [10] generates the following seemingly-unrelated\\nterms for the passage in Figure 1 in the top 20 expansion terms: reed,herb, and troy.4 Gospodinov et al.\\nrelevance). The relevance scoring function is used to ﬁlter down the queries to\\nthose that meet a certain score threshold tas follows:\\nD0=n\\nConcat\\n4, 5, https://arxiv.org/pdf/2303.07678.pdf, Query2doc: Query Expansion with Large Language Models\\nLiang Wang and Nan Yang and Furu Wei\\nMicrosoft Research\\n{wangliang,nanya,fuwei}@microsoft.com\\nAbstract\\nThis paper introduces a simple yet effec-\\ntive query expansion approach, denoted as\\nquery2doc , to improve both sparse and dense\\nretrieval systems. The proposed method\\nﬁrst generates pseudo-documents by few-shot\\nprompting large language models (LLMs), and\\nthen expands the query with generated pseudo-\\ndocuments. LLMs are trained on web-scale\\ntext corpora and are adept at knowledge mem-\\norization. The pseudo-documents from LLMs\\noften contain highly relevant information that\\ncan aid in query disambiguation and guide\\nthe retrievers. Experimental results demon-\\nstrate that query2doc boosts the performance\\nof BM25 by 3% to 15% on ad-hoc IR datasets,\\nsuch as MS-MARCO and TREC DL, with-\\nout any model ﬁne-tuning. Furthermore, our\\nmethod also beneﬁts state-of-the-art dense re-\\ntrievers in terms of both in-domain and out-of-\\ndomain results.\\n1 Introduction\\nInformation retrieval (IR) aims to locate relevant\\ndocuments from a large corpus given a user is-\\nsued query. It is a core component in modern\\nsearch engines and researchers have invested for\\ndecades in this ﬁeld. There are two mainstream\\nparadigms for IR: lexical-based sparse retrieval,\\nsuch as BM25, and embedding-based dense re-\\ntrieval (Xiong et al., 2021; Qu et al., 2021). Al-\\nthough dense retrievers perform better when large\\namounts of labeled data are available (Karpukhin\\net al., 2020), BM25 remains competitive on out-of-\\ndomain datasets (Thakur et al., 2021).\\nQuery expansion (Rocchio, 1971; Lavrenko\\nand Croft, 2001) is a long-standing technique\\nthat rewrites the query based on pseudo-relevance\\nfeedback or external knowledge sources such as\\nWordNet. For sparse retrieval, it can help bridge\\nthe lexical gap between the query and the docu-\\nments. However, query expansion methods like\\nRM3 (Lavrenko and Croft, 2001; Lv and Zhai,2009) have only shown limited success on popular\\ndatasets (Campos et al., 2016), and most state-of-\\nthe-art dense retrievers do not adopt this technique.\\nIn the meantime, document expansion methods like\\ndoc2query (Nogueira et al., 2019) have proven to\\nbe effective for sparse retrieval.\\nIn this paper, we demonstrate the effectiveness\\nof LLMs (Brown et al., 2020) as query expan-\\nsion models by generating pseudo-documents con-\\nditioned on few-shot prompts. Given that search\\nqueries are often short, ambiguous, or lack neces-\\nsary background information, LLMs can provide\\nrelevant information to guide retrieval systems, as\\nthey memorize an enormous amount of knowledge\\nand language patterns by pre-training on trillions\\nof tokens.\\nOur proposed method, called query2doc , gen-\\nerates pseudo-documents by few-shot prompting\\nLLMs and concatenates them with the original\\nquery to form a new query. This method is simple\\nto implement and does not require any changes in\\ntraining pipelines or model architectures, making it\\northogonal to the progress in the ﬁeld of LLMs and\\ninformation retrieval. Future methods can easily\\nbuild upon our query expansion framework.\\nFor in-domain evaluation, we adopt the MS-\\nMARCO passage ranking (Campos et al., 2016),\\nTREC DL 2019 and 2020 datasets. Pseudo-\\ndocuments are generated by prompting an im-\\nproved version of GPT-3 text-davinci-003 from\\nOpenAI (Brown et al., 2020). Results show that\\nquery2doc substantially improves the off-the-shelf\\nBM25 algorithm without ﬁne-tuning any model,\\nparticularly for hard queries from the TREC DL\\ntrack. Strong dense retrievers, including DPR\\n(Karpukhin et al., 2020), SimLM (Wang et al.,\\n2022a), and E5 (Wang et al., 2022b) also bene-\\nﬁt from query2doc , although the gains tend to be\\ndiminishing when distilling from a strong cross-\\nencoder based re-ranker. Experiments in zero-shot\\nOOD settings demonstrate that our method out-arXiv:2303.07678v1  [cs.IR]  14 Mar 2023performs strong baselines on most datasets. Fur-\\nther analysis also reveals the importance of model\\nscales: query2doc works best when combined with\\nthe most capable LLMs while small language mod-\\nels only provide marginal improvements over base-\\nlines.\\nTo aid reproduction, we release all\\nthe generations from text-davinci-003\\nat https://huggingface.co/datasets/\\nintfloat/query2doc_msmarco .\\n2 Method\\nWrite a passage that answers the given query:\\nQuery: what state is this zip code 85282\\nPassage: Welcome to TEMPE, AZ 85282. \\n85282 is a rural zip code in Tempe, Arizona. \\nThe population is primarily white…\\n…\\nQuery: when was pokemon green released\\nPassage:LLM Prompts\\nPokemon Green was released in Japan on \\nFebruary 27th, 1996. It was the first in the \\nPokemon series of games and served as the \\nbasis for Pokemon Red and Blue, which were \\nreleased in the US in 1998. The original \\nPokemon Green remains a beloved classic \\namong fans of the series.LLM Output\\nFigure 1: Illustration of query2doc few-shot prompting.\\nWe omit some in-context examples for space reasons.\\nGiven a query q, we employ few-shot prompting\\nto generate a pseudo-document d0as depicted in\\nFigure 1. The prompt comprises a brief instruction\\n“Write a passage that answers the given query:”\\nandklabeled pairs randomly sampled from a\\ntraining set. We use k= 4throughout this paper.\\nSubsequently, we rewrite qto a new query q+\\nby concatenating with the pseudo-document d0.\\nThere are slight differences in the concatenation\\noperation for sparse and dense retrievers, which\\nwe elaborate on in the following section.\\nSparse Retrieval Since the query qis typically\\nmuch shorter than pseudo-documents, we boost the\\nquery term weights by repeating the query ntimes\\nbefore concatenating with the pseudo-document d0:q+=concat(fqg\\x02n; d0) (1)\\nHere, “concat” denotes the string concatenation\\nfunction. q+is used as the new query for\\nBM25 retrieval. We ﬁnd that n= 5 is a gener-\\nally good value and do not tune it on a dataset basis.\\nDense Retrieval The new query q+is a sim-\\nple concatenation of the original query qand the\\npseudo-document d0separated by [SEP]:\\nq+=concat(q;[SEP]; d0) (2)\\nFor training dense retrievers, several factors can\\ninﬂuence the ﬁnal performance, such as hard nega-\\ntive mining (Xiong et al., 2021), intermediate pre-\\ntraining (Gao and Callan, 2021), and knowledge\\ndistillation from a cross-encoder based re-ranker\\n(Qu et al., 2021). In this paper, we investigate two\\nsettings to gain a more comprehensive understand-\\ning of our method. The ﬁrst setting is training DPR\\n(Karpukhin et al., 2020) models initialized from\\nBERT basewith BM25 hard negatives only. The op-\\ntimization objective is a standard contrastive loss:\\nLcont=', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the docs into nodes\n",
    "# Node parsers are a simple abstraction that take a list of documents, and chunk them into Node objects\n",
    "\n",
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='3c9f3625-12ee-46b6-b7e7-6d9c3783708b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='90e2e4e9-9282-43c6-8606-e5c11b105f25', node_type=None, metadata={}, hash='79048ed80b10e318ac10184464e9e787938cb0f9d0e4db5d9f4bd218cb70bb96')}, hash='29d0a52345017d55a684f6bfd60f5b66ff09b8c81c30764e8e39742b1a1f5e5d', text='0, 1, https://dl.acm.org/doi/pdf/10.1145/3397271.3401075, ColBERT: Efficient and Effective Passage Search via\\nContextualized Late Interaction over BERT\\nOmar Khattab\\nStanford University\\nokhattab@stanford.eduMatei Zaharia\\nStanford University\\nmatei@cs.stanford.edu\\nABSTRACT\\nRecent progress in Natural Language Understanding (NLU) is driv-\\ning fast-paced advances in Information Retrieval (IR), largely owed\\nto fine-tuning deep language models (LMs) for document ranking.\\nWhile remarkably effective, the ranking models based on these LMs\\nincrease computational cost by orders of magnitude over prior ap-\\nproaches, particularly as they must feed each query–document pair\\nthrough a massive neural network to compute a single relevance\\nscore. To tackle this, we present ColBERT, a novel ranking model\\nthat adapts deep LMs (in particular, BERT) for efficient retrieval.\\nColBERT introduces a late interaction architecture that indepen-\\ndently encodes the query and the document using BERT and then\\nemploys a cheap yet powerful interaction step that models their fine-\\ngrained similarity. By delaying and yet retaining this fine-granular\\ninteraction, ColBERT can leverage the expressiveness of deep LMs\\nwhile simultaneously gaining the ability to pre-compute document\\nrepresentations offline, considerably speeding up query processing.\\nCrucially, ColBERT’s pruning-friendly interaction mechanism en-\\nables leveraging vector-similarity indexes for end-to-end retrieval\\ndirectly from millions of documents. We extensively evaluate Col-\\nBERT using two recent passage search datasets. Results show that\\nColBERT’s effectiveness is competitive with existing BERT-based\\nmodels (and outperforms every non-BERT baseline), while exe-\\ncuting two orders-of-magnitude faster and requiring up to four\\norders-of-magnitude fewer FLOPs per query.\\nKEYWORDS\\nNeural IR; Efficiency; Deep Language Models; BERT\\nACM Reference Format:\\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Pas-\\nsage Search via Contextualized Late Interaction over BERT. In Proceedings of\\nthe 43rd International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM,\\nNew York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.3401075\\n1 INTRODUCTION\\nOver the past few years, the Information Retrieval (IR) community\\nhas witnessed the introduction of a host of neural ranking models,\\nincluding DRMM [ 7], KNRM [ 4,36], and Duet [ 20,22]. In contrast\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n©2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\\nhttps://doi.org/10.1145/3397271.3401075\\n0.15 0.20 0.25 0.30 0.35 0.40\\nMRR@10101102103104105Query Latency (ms)\\nBM25doc2queryKNRMDuet\\nDeepCTfT+ConvKNRM\\ndocTTTTTqueryBERT-baseBERT-large\\nColBERT', start_char_idx=0, end_char_idx=3444, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='90e2e4e9-9282-43c6-8606-e5c11b105f25', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='3c9f3625-12ee-46b6-b7e7-6d9c3783708b', node_type=None, metadata={}, hash='29d0a52345017d55a684f6bfd60f5b66ff09b8c81c30764e8e39742b1a1f5e5d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='726ba192-4260-4e93-850a-e00a1ebac965', node_type=None, metadata={}, hash='40078fb8bc2540f744edfc82bd807e97ca18b0bc4833c7112f921bd5ca9cd341')}, hash='79048ed80b10e318ac10184464e9e787938cb0f9d0e4db5d9f4bd218cb70bb96', text='(re-rank)ColBERT (full retrieval)Bag-of-Words (BoW) Model\\nBoW Model with NLU Augmentation\\nNeural Matching Model\\nDeep Language Model\\nColBERT (ours)Figure 1: Effectiveness (MRR@10) versus Mean Query La-\\ntency (log-scale) for a number of representative ranking\\nmodels on MS MARCO Ranking [24]. The figure also shows\\nColBERT. Neural re-rankers run on top of the official BM25\\ntop-1000 results and use a Tesla V100 GPU. Methodology and\\ndetailed results are in §4.\\nto prior learning-to-rank methods that rely on hand-crafted fea-\\ntures, these models employ embedding-based representations of\\nqueries and documents and directly model local interactions (i.e.,\\nfine-granular relationships) between their contents. Among them,\\na recent approach has emerged that fine-tunes deep pre-trained\\nlanguage models (LMs) like ELMo [ 29] and BERT [ 5] for estimating\\nrelevance. By computing deeply-contextualized semantic repre-\\nsentations of query–document pairs, these LMs help bridge the\\npervasive vocabulary mismatch [ 21,42] between documents and\\nqueries [ 30]. Indeed, in the span of just a few months, a number\\nof ranking models based on BERT have achieved state-of-the-art\\nresults on various retrieval benchmarks [ 3,18,25,39] and have\\nbeen proprietarily adapted for deployment by Google1and Bing2.\\nHowever, the remarkable gains delivered by these LMs come\\nat a steep increase in computational cost. Hofstätter et al. [9] and\\nMacAvaney et al. [18] observe that BERT-based models in the lit-\\nerature are 100-1000 ×more computationally expensive than prior\\nmodels—some of which are arguably notinexpensive to begin with\\n[13]. This quality–cost tradeoff is summarized by Figure 1, which\\ncompares two BERT-based rankers [ 25,27] against a representative\\nset of ranking models. The figure uses MS MARCO Ranking [ 24],\\na recent collection of 9M passages and 1M queries from Bing’s\\nlogs. It reports retrieval effectiveness (MRR@10) on the official\\nvalidation set as well as average query latency (log-scale) using a\\nhigh-end server that dedicates one Tesla V100 GPU per query for\\nneural re-rankers. Following the re-ranking setup of MS MARCO,\\nColBERT (re-rank), the Neural Matching Models, and the Deep LMs\\nre-rank MS MARCO’s official top-1000 documents per query. Other\\n1https://blog.google/products/search/search-language-understanding-bert/\\n2https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-\\nin-search-experience-using-azure-gpus/\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n39\\nQuery Document\\nMaxSim∑\\nMaxSim MaxSims\\nQueryCNN  /  Match KernelsCNN  /  Match Kernels / MLPMLPs\\nDocument\\n(c) All-to-all Interaction\\n(e.g., BERT)(b) Query-Document Interaction\\n(e.g., DRMM, KNRM, Conv-KNRM)(d) Late Interaction\\n(i.e., the proposed ColBERT)(a) Representation-based Similarity\\n(e.g., DSSM, SNRM)Query Document\\ns\\nQuery Document\\nsFigure 2: Schematic diagrams illustrating query–document matching paradigms in neural IR. The figure contrasts existing\\napproaches (sub-figures (a), (b), and (c)) with the proposed late interaction paradigm (sub-figure (d)).\\nmethods, including ColBERT (full retrieval), directly retrieve the\\ntop-1000 results from the entire collection.\\nAs the figure shows, BERT considerably improves search preci-\\nsion, raising MRR@10 by almost 7%', start_char_idx=3445, end_char_idx=6757, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='726ba192-4260-4e93-850a-e00a1ebac965', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='90e2e4e9-9282-43c6-8606-e5c11b105f25', node_type=None, metadata={}, hash='79048ed80b10e318ac10184464e9e787938cb0f9d0e4db5d9f4bd218cb70bb96'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='51cff5f0-bbe6-470a-9639-98bb61e855e3', node_type=None, metadata={}, hash='c29452d71326a5bd212b6e156b012f63698e2ec93f46ff28c6b62314a2a2ddfa')}, hash='40078fb8bc2540f744edfc82bd807e97ca18b0bc4833c7112f921bd5ca9cd341', text='improves search preci-\\nsion, raising MRR@10 by almost 7% against the best previous meth-\\nods; simultaneously, it increases latency by up to tens of thousands\\nof milliseconds even with a high-end GPU. This poses a challenging\\ntradeoff since raising query response times by as little as 100ms is\\nknown to impact user experience and even measurably diminish\\nrevenue [ 17]. To tackle this problem, recent work has started explor-\\ning using Natural Language Understanding (NLU) techniques to\\naugment traditional retrieval models like BM25 [ 32]. For example,\\nNogueira et al. [26,28] expand documents with NLU-generated\\nqueries before indexing with BM25 scores and Dai & Callan [ 2] re-\\nplace BM25’s term frequency with NLU-estimated term importance.\\nDespite successfully reducing latency, these approaches generally\\nreduce precision substantially relative to BERT.\\nTo reconcile efficiency and contextualization in IR, we propose\\nColBERT , a ranking model based on contextualized late interac-\\ntion over BERT . As the name suggests, ColBERT proposes a novel\\nlate interaction paradigm for estimating relevance between a query\\n𝑞and a document 𝑑. Under late interaction, 𝑞and𝑑are separately\\nencoded into two sets of contextual embeddings, and relevance is\\nevaluated using cheap and pruning-friendly computations between\\nboth sets—that is, fast computations that enable ranking without\\nexhaustively evaluating every possible candidate.\\nFigure 2 contrasts our proposed late interaction approach with\\nexisting neural matching paradigms. On the left, Figure 2 (a) illus-\\ntrates representation-focused rankers, which independently compute\\nan embedding for 𝑞and another for 𝑑and estimate relevance as\\na single similarity score between two vectors [ 12,41]. Moving to\\nthe right, Figure 2 (b) visualizes typical interaction-focused rankers.\\nInstead of summarizing 𝑞and𝑑into individual embeddings, these\\nrankers model word- and phrase-level relationships across 𝑞and𝑑\\nand match them using a deep neural network (e.g., with CNNs/MLPs\\n[22] or kernels [ 36]). In the simplest case, they feed the neural net-\\nwork an interaction matrix that reflects the similiarity between\\nevery pair of words across 𝑞and𝑑. Further right, Figure 2 (c) illus-\\ntrates a more powerful interaction-based paradigm, which models\\nthe interactions between words within as well as across𝑞and𝑑at\\nthe same time, as in BERT’s transformer architecture [25].These increasingly expressive architectures are in tension. While\\ninteraction-based models (i.e., Figure 2 (b) and (c)) tend to be su-\\nperior for IR tasks [ 8,21], a representation-focused model—by iso-\\nlating the computations among 𝑞and𝑑—makes it possible to pre-\\ncompute document representations offline [ 41], greatly reducing\\nthe computational load per query. In this work, we observe that\\nthe fine-grained matching in interaction-based models and the pre-\\ncomputation in representation-based models can be combined by\\nretaining yet judiciously delaying the query–document interaction.\\nFigure 2 (d) illustrates an architecture that precisely does so. As\\nillustrated, every query embedding interacts with all document\\nembeddings via a MaxSim operator, which computes maximum\\nsimilarity (e.g., cosine), and the scalar outputs of these operators\\nare summed across query terms. This paradigm allows ColBERT to\\nexploit deep LM-based representations while shifting the cost of\\nencoding documents offline and amortizing the cost of encoding\\nthe query once across all ranked documents. Crucially, it enables\\nColBERT to leverage vector-similarity search indexes (e.g., [ 1,15])\\nto retrieve the top- 𝑘results', start_char_idx=6709, end_char_idx=10317, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='51cff5f0-bbe6-470a-9639-98bb61e855e3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='726ba192-4260-4e93-850a-e00a1ebac965', node_type=None, metadata={}, hash='40078fb8bc2540f744edfc82bd807e97ca18b0bc4833c7112f921bd5ca9cd341'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='28e1191d-e1e5-418a-8ca6-1a9566f56a35', node_type=None, metadata={}, hash='a81d3239179d4f9c5e4eb1c3c214ac9b65cb70fcb388dbbb30ca7643d4c775f9')}, hash='c29452d71326a5bd212b6e156b012f63698e2ec93f46ff28c6b62314a2a2ddfa', text='[ 1,15])\\nto retrieve the top- 𝑘results directly from a large document collec-\\ntion. This ability substantially improves recall over existing models,\\nwhich only re-rank the output of term-based retrieval.\\nAs Figure 1 illustrates, ColBERT can serve queries in tens or, for\\nend-to-end retrieval from millions of documents, few hundreds of\\nmilliseconds. For instance, when used for re-ranking as in “ColBERT\\n(re-rank)”, it delivers over 170 ×speedup (and requires 14,000 ×fewer\\nFLOPs) relative to existing BERT-based models [ 25,27], while be-\\ning more effective than every non-BERT baseline (§4.2 & 4.3). Col-\\nBERT’s indexing—the only time it needs to feed documents through\\nBERT—is also practical: it can index the MS MARCO collection of\\n9M passages in about 3 hours using a single server with four GPUs\\n(§4.5), retaining its effectiveness with a space footprint of as little\\nas few tens of GiBs. Our ablation study (§4.4) shows that late in-\\nteraction, its implementation via MaxSim operations, and crucial\\ndesign choices within our BERT-based encoders are all essential to\\nColBERT’s effectiveness.\\nOur main contributions are as follows.\\n(1)We propose late interaction (§3.1) as a paradigm for efficient\\nand effective neural ranking.\\n(2)We present ColBERT (§3.2 & 3.3), a highly-effective model\\nthat employs novel BERT-based query and document en-\\ncoders within the late interaction paradigm.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n40(3)We show how to leverage ColBERT both for re-ranking on\\ntop of a term-based retrieval model (§3.5) and for searching\\na full collection using vector similarity indexes (§3.6).\\n(4)We evaluate ColBERT on MS MARCO and TREC CAR, two\\nrecent passage search collections.\\nWe release our reference implementation as open source.3\\n2 RELATED WORK\\nNeural Matching Models. Over the past few years, IR researchers\\nhave introduced numerous neural architectures for ranking. In\\nthis work, we compare against KNRM [ 4,36], Duet [ 20,22], Con-\\nvKNRM [ 4], and fastText+ConvKNRM [ 10]. KNRM proposes a dif-\\nferentiable kernel-pooling technique for extracting matching sig-\\nnals from an interaction matrix, while Duet combines signals from\\nexact-match-based as well as embedding-based similarities for rank-\\ning. Introduced in 2018, ConvKNRM learns to match 𝑛-grams in the\\nquery and the document. Lastly, fastText+ConvKNRM (abbreviated\\nfT+ConvKNRM) tackles the absence of rare words from typical\\nword embeddings lists by adopting sub-word token embeddings.\\nIn 2018, Zamani et al. [41] introduced SNRM, a representation-\\nfocused IR model that encodes each query and each document as\\na single, sparse high-dimensional vector of “latent terms”. By pro-\\nducing a sparse-vector representation for each document, SNRM\\nis able to use a traditional IR inverted index for representing docu-\\nments, allowing fast end-to-end retrieval. Despite highly promising\\nresults and insights, SNRM’s effectiveness is substantially outper-\\nformed by the state of the art on the datasets with which it was\\nevaluated (e.g., see [ 18,38]). While SNRM employs sparsity to al-\\nlow using inverted indexes, we relax this assumption and compare\\na (dense) BERT-based representation-focused model against our\\nlate-interaction ColBERT in our ablation experiments in §4.4. For a\\ndetailed overview of existing', start_char_idx=10334, end_char_idx=13676, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='28e1191d-e1e5-418a-8ca6-1a9566f56a35', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='51cff5f0-bbe6-470a-9639-98bb61e855e3', node_type=None, metadata={}, hash='c29452d71326a5bd212b6e156b012f63698e2ec93f46ff28c6b62314a2a2ddfa'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ddcfb8b1-af34-4954-aa0d-3a5e1412d2ba', node_type=None, metadata={}, hash='a3dc295a61756bdfcdd1eacdb08e2928a0b98bdd811c40001f3caad71f5319fc')}, hash='a81d3239179d4f9c5e4eb1c3c214ac9b65cb70fcb388dbbb30ca7643d4c775f9', text='our ablation experiments in §4.4. For a\\ndetailed overview of existing neural ranking models, we refer the\\nreaders to two recent surveys of the literature [8, 21].\\nLanguage Model Pretraining for IR. Recent work in NLU\\nemphasizes the importance pre-training language representation\\nmodels in an unsupervised fashion before subsequently fine-tuning\\nthem on downstream tasks. A notable example is BERT [ 5], a bi-\\ndirectional transformer-based language model whose fine-tuning\\nadvanced the state of the art on various NLU benchmarks. Nogueira et\\nal.[25], MacAvaney et al. [18], and Dai & Callan [ 3] investigate\\nincorporating such LMs (mainly BERT, but also ELMo [ 29]) on dif-\\nferent ranking datasets. As illustrated in Figure 2 (c), the common\\napproach (and the one adopted by Nogueira et al. on MS MARCO\\nand TREC CAR) is to feed the query–document pair through BERT\\nand use an MLP on top of BERT’s [CLS] output token to produce a\\nrelevance score. Subsequent work by Nogueira et al. [27] introduced\\nduoBERT, which fine-tunes BERT to compare the relevance of a\\npair of documents given a query. Relative to their single-document\\nBERT, this gives duoBERT about 1% MRR@10 advantage on MS\\nMARCO while increasing the cost by at least 1.4×.\\nBERT Optimizations. As discussed in §1, these rankers can be\\nhighly expensive in practice. Orthogonal to our approach, there are\\nongoing efforts in the NLU literature for distilling [ 14,33], compress-\\ning [ 40], and pruning [ 19] BERT. Other optimizations may trade\\nquality and speed specifically for IR (e.g., re-ranking with a smaller\\n3https://github.com/stanford-futuredata/ColBERT\\nQuery Document\\nQuery Encoder, fQ Document Encoder, fDMaxSim MaxSim MaxSimscore\\nOffline IndexingFigure 3: The general architecture of ColBERT given a query\\n𝑞and a document 𝑑.\\ndepth𝑘(§4.2) or truncating longer documents). While these efforts\\ncan be instrumental in narrowing the efficiency gap, they generally\\nachieve much smaller speedups than our re-designed architecture\\nfor IR, due to their generic nature, and the more aggressive ones\\noften come at the cost of noticeably lower quality.\\nEfficient NLU-based Models. Recently, a direction emerged\\nthat employs expensive NLU computation offline. This includes\\ndoc2query [ 28] and DeepCT [ 2]. The doc2query model expands\\neach document with a pre-defined number of synthetic queries, gen-\\nerated by a seq2seq transformer model trained to generate queries\\ngiven a document. It then relies on a BM25 index for retrieval from\\nthe (expanded) documents. DeepCT uses BERT to produce the term\\nfrequency component of BM25 in a context-aware manner, essen-\\ntially representing a feasible realization of the term-independence\\nassumption with neural networks [ 23]. Lastly, docTTTTTquery [ 26]\\nis identical to doc2query except that it fine-tunes a pre-trained\\nmodel (namely, T5 [31]) for generating the predicted queries.\\nConcurrently with the drafting of this paper, Hofstätter et al. [11]\\npublished their Transformer-Kernel (TK) model. At a high level, TK\\nimproves the KNRM architecture described earlier: while KNRM\\nemploys kernel pooling on top of word-embedding-based inter-\\naction, TK uses a Transformer [ 34] component for contextually\\nencoding queries and documents before kernel pooling. TK estab-\\nlishes a new state-of-the-art for non-BERT models on MS MARCO\\n(Dev); however, the best non-ensemble MRR@10 it achieves is 31%\\nwhile ColBERT reaches up to 36%. Moreover, due to indexing docu-\\nment representations', start_char_idx=13650, end_char_idx=17124, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='ddcfb8b1-af34-4954-aa0d-3a5e1412d2ba', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='28e1191d-e1e5-418a-8ca6-1a9566f56a35', node_type=None, metadata={}, hash='a81d3239179d4f9c5e4eb1c3c214ac9b65cb70fcb388dbbb30ca7643d4c775f9'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='38042519-735a-4578-8b1d-66e895c2e1e5', node_type=None, metadata={}, hash='23bfddf08bd707d394459fbee63168a69ce1bbf3ebf6818b4266da1f90d52b57')}, hash='a3dc295a61756bdfcdd1eacdb08e2928a0b98bdd811c40001f3caad71f5319fc', text='reaches up to 36%. Moreover, due to indexing docu-\\nment representations offline and employing a MaxSim-based late\\ninteraction mechanism, ColBERT is much more scalable, enabling\\nend-to-end retrieval which is not supported by TK.\\n3 COLBERT\\nColBERT prescribes a simple framework for balancing the quality\\nand cost of neural IR, particularly deep language models like BERT.\\nAs introduced earlier, delaying the query–document interaction can\\nfacilitate cheap neural re-ranking (i.e., through pre-computation)\\nand even support practical end-to-end neural retrieval (i.e., through\\npruning via vector-similarity search). ColBERT addresses how to\\ndo so while still preserving the effectiveness of state-of-the-art\\nmodels, which condition the bulk of their computations on the joint\\nquery–document pair.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n41Even though ColBERT’s late-interaction framework can be ap-\\nplied to a wide variety of architectures (e.g., CNNs, RNNs, transform-\\ners, etc.), we choose to focus this work on bi-directional transformer-\\nbased encoders (i.e., BERT) owing to their state-of-the-art effective-\\nness yet very high computational cost.\\n3.1 Architecture\\nFigure 3 depicts the general architecture of ColBERT, which com-\\nprises: (a) a query encoder 𝑓𝑄, (b) a document encoder 𝑓𝐷, and (c)\\nthe late interaction mechanism. Given a query 𝑞and document 𝑑,\\n𝑓𝑄encodes𝑞into a bag of fixed-size embeddings 𝐸𝑞while𝑓𝐷en-\\ncodes𝑑into another bag 𝐸𝑑. Crucially, each embeddings in 𝐸𝑞and\\n𝐸𝑑iscontextualized based on the other terms in 𝑞or𝑑, respectively.\\nWe describe our BERT-based encoders in §3.2.\\nUsing𝐸𝑞and𝐸𝑑, ColBERT computes the relevance score be-\\ntween𝑞and𝑑via late interaction, which we define as a summation\\nof maximum similarity (MaxSim) operators. In particular, we find\\nthe maximum cosine similarity of each 𝑣∈𝐸𝑞with vectors in 𝐸𝑑,\\nand combine the outputs via summation. Besides cosine, we also\\nevaluate squared L2 distance as a measure of vector similarity. In-\\ntuitively, this interaction mechanism softly searches for each query\\nterm𝑡𝑞—in a manner that reflects its context in the query—against\\nthe document’s embeddings, quantifying the strength of the “match”\\nvia the largest similarity score between 𝑡𝑞and a document term 𝑡𝑑.\\nGiven these term scores, it then estimates the document relevance\\nby summing the matching evidence across all query terms.\\nWhile more sophisticated matching is possible with other choices\\nsuch as deep convolution and attention layers (i.e., as in typical\\ninteraction-focused models), a summation of maximum similarity\\ncomputations has two distinctive characteristics. First, it stands\\nout as a particularly cheap interaction mechanism, as we examine\\nits FLOPs in §4.2. Second, and more importantly, it is amenable\\nto highly-efficient pruning for top- 𝑘retrieval, as we evaluate in\\n§4.3. This enables using vector-similarity algorithms for skipping\\ndocuments without materializing the full interaction matrix or even\\nconsidering each document in isolation. Other cheap choices (e.g.,\\na summation of average similarity scores, instead of maximum) are\\npossible; however, many are less amenable to pruning. In §4.4,', start_char_idx=17123, end_char_idx=20327, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='38042519-735a-4578-8b1d-66e895c2e1e5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ddcfb8b1-af34-4954-aa0d-3a5e1412d2ba', node_type=None, metadata={}, hash='a3dc295a61756bdfcdd1eacdb08e2928a0b98bdd811c40001f3caad71f5319fc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='eae735f4-41ac-4517-804c-efe667e81607', node_type=None, metadata={}, hash='231d2f58cbc4dcfbfaa9de31d768f870fccd97f77213b9caa72bc1a9f1e90e62')}, hash='23bfddf08bd707d394459fbee63168a69ce1bbf3ebf6818b4266da1f90d52b57', text='however, many are less amenable to pruning. In §4.4, we\\nconduct an extensive ablation study that empirically verifies the ad-\\nvantage of our MaxSim-based late interaction against alternatives.\\n3.2 Query & Document Encoders\\nPrior to late interaction, ColBERT encodes each query or document\\ninto a bag of embeddings, employing BERT-based encoders. We\\nshare a single BERT model among our query and document en-\\ncoders but distinguish input sequences that correspond to queries\\nand documents by prepending a special token [Q]to queries and\\nanother token [D]to documents.\\nQuery Encoder. Given a textual query 𝑞, we tokenize it into its\\nBERT-based WordPiece [ 35] tokens𝑞1𝑞2...𝑞𝑙. We prepend the token\\n[Q]to the query. We place this token right after BERT’s sequence-\\nstart token [CLS] . If the query has fewer than a pre-defined number\\nof tokens𝑁𝑞, we pad it with BERT’s special [mask] tokens up\\nto length𝑁𝑞(otherwise, we truncate it to the first 𝑁𝑞tokens).\\nThis padded sequence of input tokens is then passed into BERT’sdeep transformer architecture, which computes a contextualized\\nrepresentation of each token.\\nWe denote the padding with masked tokens as query augmen-\\ntation , a step that allows BERT to produce query-based embeddings\\nat the positions corresponding to these masks. Query augmentation\\nis intended to serve as a soft, differentiable mechanism for learning\\nto expand queries with new terms or to re-weigh existing terms\\nbased on their importance for matching the query. As we show in\\n§4.4, this operation is essential for ColBERT’s effectiveness.\\nGiven BERT’s representation of each token, our encoder passes\\nthe contextualized output representations through a linear layer\\nwith no activations. This layer serves to control the dimension of\\nColBERT’s embeddings, producing 𝑚-dimensional embeddings for\\nthe layer’s output size 𝑚. As we discuss later, we typically fix 𝑚to\\nbe much smaller than BERT’s fixed hidden dimension.\\nWhile ColBERT’s embedding dimension has limited impact on\\nthe efficiency of query encoding, this step is crucial for controlling\\nthe space footprint of documents, as we show in §4.5. In addition,\\nit can have a significant impact on query execution time, particu-\\nlarly the time taken for transferring the document representations\\nonto the GPU from system memory (where they reside before pro-\\ncessing a query). In fact, as we show in §4.2, gathering, stacking,\\nand transferring the embeddings from CPU to GPU can be the\\nmost expensive step in re-ranking with ColBERT. Finally, the out-\\nput embeddings are normalized so each has L2 norm equal to one.\\nThe result is that the dot-product of any two embeddings becomes\\nequivalent to their cosine similarity, falling in the [−1,1]range.\\nDocument Encoder. Our document encoder has a very similar\\narchitecture. We first segment a document 𝑑into its constituent\\ntokens𝑑1𝑑2...𝑑𝑚, to which we prepend BERT’s start token [CLS] fol-\\nlowed by our special token [D]that indicates a document sequence.\\nUnlike queries, we do not append [mask] tokens to documents. Af-\\nter passing this input sequence through BERT and the subsequent\\nlinear layer, the document encoder filters out the embeddings corre-\\nsponding to punctuation symbols, determined via a pre-defined list.\\nThis filtering is meant to reduce the number of embeddings per doc-\\nument, as we hypothesize that (even contextualized) embeddings\\nof punctuation are unnecessary for effectiveness.\\nIn summary, given', start_char_idx=20346, end_char_idx=23783, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='eae735f4-41ac-4517-804c-efe667e81607', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='38042519-735a-4578-8b1d-66e895c2e1e5', node_type=None, metadata={}, hash='23bfddf08bd707d394459fbee63168a69ce1bbf3ebf6818b4266da1f90d52b57'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d884956b-0130-45db-810b-31bde713044a', node_type=None, metadata={}, hash='009f3160cd0450d0a9bc191262c294087e7ea492225849e64fe3bb10ae68423d')}, hash='231d2f58cbc4dcfbfaa9de31d768f870fccd97f77213b9caa72bc1a9f1e90e62', text='embeddings\\nof punctuation are unnecessary for effectiveness.\\nIn summary, given 𝑞=𝑞0𝑞1...𝑞𝑙and𝑑=𝑑0𝑑1...𝑑𝑛, we compute\\nthe bags of embeddings 𝐸𝑞and𝐸𝑑in the following manner, where\\n#refers to the [mask] tokens:\\n𝐸𝑞:=Normalize(CNN(BERT(“[𝑄]𝑞0𝑞1...𝑞𝑙##...#”))) (1)\\n𝐸𝑑:=Filter(Normalize(CNN(BERT(“[𝐷]𝑑0𝑑1...𝑑𝑛”)))) (2)\\n3.3 Late Interaction\\nGiven the representation of a query 𝑞and a document 𝑑, the rele-\\nvance score of 𝑑to𝑞, denoted as𝑆𝑞,𝑑, is estimated via late interaction\\nbetween their bags of contextualized embeddings. As mentioned\\nbefore, this is conducted as a sum of maximum similarity computa-\\ntions, namely cosine similarity (implemented as dot-products due\\nto the embedding normalization) or squared L2 distance.\\n𝑆𝑞,𝑑:=Õ\\n𝑖∈[|𝐸𝑞|]max\\n𝑗∈[|𝐸𝑑|]𝐸𝑞𝑖·𝐸𝑇\\n𝑑𝑗(3)\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n42ColBERT is differentiable end-to-end. We fine-tune the BERT\\nencoders and train from scratch the additional parameters (i.e., the\\nlinear layer and the [Q] and [D] markers’ embeddings) using the\\nAdam [ 16] optimizer. Notice that our interaction mechanism has\\nno trainable parameters. Given a triple ⟨𝑞,𝑑+,𝑑−⟩with query 𝑞,\\npositive document 𝑑+and negative document 𝑑−, ColBERT is used\\nto produce a score for each document individually and is optimized\\nvia pairwise softmax cross-entropy loss over the computed scores\\nof𝑑+and𝑑−.\\n3.4 Offline Indexing: Computing & Storing\\nDocument Embeddings\\nBy design, ColBERT isolates almost all of the computations be-\\ntween queries and documents to enable pre-computing document\\nrepresentations offline. At a high level, our indexing procedure is\\nstraight-forward: we proceed over the documents in the collection\\nin batches, running our document encoder 𝑓𝐷on each batch and\\nstoring the output embeddings per document. Although indexing a\\nset of documents is an offline process, we incorporate a few simple\\noptimizations for enhancing its throughput. As we show in §4.5,\\nthese can considerably reduce the offline cost of indexing.\\nTo begin with, we exploit multiple GPUs, if available, for faster\\nencoding of batches of documents in parallel. When batching, we\\npad all documents to the maximum length of a document within\\nthe batch.4To make capping the sequence length on a per-batch\\nbasis effective, our indexer proceeds through documents in large\\ngroups of𝐵(e.g.,𝐵=100,000) documents. It sorts these documents\\nby length and then feeds batches of 𝑏(e.g.,𝑏=128) documents of\\ncomparable length through our encoder. Such length-based bucket-\\ning is sometimes refered to as a BucketIterator in some libraries\\n(e.g., allenNLP). Lastly, while most computations occur on the GPU,\\nwe found that a non-trivial portion of the indexing time is spent on\\npre-processing the text sequences, primarily BERT’s WordPiece to-\\nkenization.', start_char_idx=23757, end_char_idx=26562, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='d884956b-0130-45db-810b-31bde713044a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='eae735f4-41ac-4517-804c-efe667e81607', node_type=None, metadata={}, hash='231d2f58cbc4dcfbfaa9de31d768f870fccd97f77213b9caa72bc1a9f1e90e62'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ac9fa16d-0a5d-41bb-95d7-b603c58d33ae', node_type=None, metadata={}, hash='860a47839f9c47b3e772641ff6b40ac0ea6642b3c10692098a5328aa959acac2')}, hash='009f3160cd0450d0a9bc191262c294087e7ea492225849e64fe3bb10ae68423d', text='text sequences, primarily BERT’s WordPiece to-\\nkenization. Exploiting that these operations are independent across\\ndocuments in a batch, we parallelize the pre-processing across the\\navailable CPU cores.\\nOnce the document representations are produced, they are saved\\nto disk using 32-bit or 16-bit values to represent each dimension.\\nAs we describe in §3.5 and 3.6, these representations are either\\nsimply loaded from disk for ranking or are subsequently indexed\\nfor vector-similarity search, respectively.\\n3.5 Top-𝑘Re-ranking with ColBERT\\nRecall that ColBERT can be used for re-ranking the output of an-\\nother retrieval model, typically a term-based model, or directly\\nfor end-to-end retrieval from a document collection. In this sec-\\ntion, we discuss how we use ColBERT for ranking a small set of\\n𝑘(e.g.,𝑘=1000) documents given a query 𝑞. Since𝑘is small, we\\nrely on batch computations to exhaustively score each document\\n(unlike our approach in §3.6). To begin with, our query serving sub-\\nsystem loads the indexed documents representations into memory,\\nrepresenting each document as a matrix of embeddings.\\nGiven a query 𝑞, we compute its bag of contextualized embed-\\ndings𝐸𝑞(Equation 1) and, concurrently, gather the document repre-\\nsentations into a 3-dimensional tensor 𝐷consisting of 𝑘document\\n4The public BERT implementations we saw simply pad to a pre-defined length.matrices. We pad the 𝑘documents to their maximum length to\\nfacilitate batched operations, and move the tensor 𝐷to the GPU’s\\nmemory. On the GPU, we compute a batch dot-product of 𝐸𝑞and\\n𝐷, possibly over multiple mini-batches. The output materializes a\\n3-dimensional tensor that is a collection of cross-match matrices\\nbetween𝑞and each document. To compute the score of each docu-\\nment, we reduce its matrix across document terms via a max-pool\\n(i.e., representing an exhaustive implementation of our MaxSim\\ncomputation) and reduce across query terms via a summation. Fi-\\nnally, we sort the 𝑘documents by their total scores.\\nRelative to existing neural rankers (especially, but not exclusively,\\nBERT-based ones), this computation is very cheap that, in fact, the\\ncost of a simple implementation is dominated by the gathering\\nand transferring of the pre-computed embeddings. To illustrate,\\nranking𝑘documents via typical BERT rankers requires feeding\\nBERT𝑘different inputs each of length 𝑙=|𝑞|+|𝑑𝑖|for query𝑞and\\ndocuments𝑑𝑖, where attention has quadratic cost in the length of\\nthe sequence. In contrast, ColBERT feeds BERT only a single, much\\nshorter sequence of length 𝑙=|𝑞|. Consequently, ColBERT is not\\nonly cheaper, it also scales much better with 𝑘(§4.2).\\n3.6 End-to-end Top- 𝑘Retrieval with ColBERT\\nAs mentioned before, ColBERT’s late-interaction operator is specifi-\\ncally designed to enable end-to-end retrieval from a large collection,\\nlargely to improve recall relative to term-based retrieval approaches.\\nThis section is concerned with cases where the number of docu-\\nments to be ranked is too large for exhaustive evaluation of each\\npossible candidate document, particularly when we are only in-\\nterested in the highest scoring ones. Concretely, we focus here on\\nretrieving the top- 𝑘results directly from a large document collec-\\ntion with𝑁(e.g.,𝑁=10,000,000) documents, where 𝑘≪𝑁.\\nTo do so, we leverage the pruning-friendly nature of the MaxSim\\noperations at the backbone of late', start_char_idx=26581, end_char_idx=29944, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='ac9fa16d-0a5d-41bb-95d7-b603c58d33ae', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d884956b-0130-45db-810b-31bde713044a', node_type=None, metadata={}, hash='009f3160cd0450d0a9bc191262c294087e7ea492225849e64fe3bb10ae68423d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='56847c1c-f41e-4165-a93b-bad600a34abf', node_type=None, metadata={}, hash='a4486893e3c7a8567dadcf20b9f6e480a7a207d6d39869335ae178a8ba2833ba')}, hash='860a47839f9c47b3e772641ff6b40ac0ea6642b3c10692098a5328aa959acac2', text='the pruning-friendly nature of the MaxSim\\noperations at the backbone of late interaction. Instead of applying\\nMaxSim between one of the query embeddings and all of one docu-\\nment’s embeddings, we can use fast vector-similarity data structures\\nto efficiently conduct this search between the query embedding\\nandalldocument embeddings across the full collection. For this,\\nwe employ an off-the-shelf library for large-scale vector-similarity\\nsearch, namely faiss [15] from Facebook.5In particular, at the\\nend of offline indexing (§3.4), we maintain a mapping from each\\nembedding to its document of origin and then index all document\\nembeddings into faiss.\\nSubsequently, when serving queries, we use a two-stage pro-\\ncedure to retrieve the top- 𝑘documents from the entire collection.\\nBoth stages rely on ColBERT’s scoring: the first is an approximate\\nstage aimed at filtering while the second is a refinement stage. For\\nthe first stage, we concurrently issue 𝑁𝑞vector-similarity queries\\n(corresponding to each of the embeddings in 𝐸𝑞) onto our faiss in-\\ndex. This retrieves the top- 𝑘′(e.g.,𝑘′=𝑘/2) matches for that vector\\nover all document embeddings. We map each of those to its docu-\\nment of origin, producing 𝑁𝑞×𝑘′document IDs, only 𝐾≤𝑁𝑞×𝑘′\\nof which are unique. These 𝐾documents likely contain one or more\\nembeddings that are highly similar to the query embeddings. For\\nthe second stage, we refine this set by exhaustively re-ranking only\\nthose𝐾documents in the usual manner described in §3.5.\\n5https://github.com/facebookresearch/faiss\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n43In our faiss -based implementation, we use an IVFPQ index\\n(“inverted file with product quantization”). This index partitions\\nthe embedding space into 𝑃(e.g.,𝑃=1000) cells based on 𝑘-means\\nclustering and then assigns each document embedding to its nearest\\ncell based on the selected vector-similarity metric. For serving\\nqueries, when searching for the top- 𝑘′matches for a single query\\nembedding, only the nearest 𝑝(e.g.,𝑝=10) partitions are searched.\\nTo improve memory efficiency, every embedding is divided into 𝑠\\n(e.g.,𝑠=16) sub-vectors, each represented using one byte. Moreover,\\nthe index conducts the similarity computations in this compressed\\ndomain, leading to cheaper computations and thus faster search.\\n4 EXPERIMENTAL EVALUATION\\nWe now turn our attention to empirically testing ColBERT, address-\\ning the following research questions.\\nRQ1: In a typical re-ranking setup, how well can ColBERT bridge\\nthe existing gap (highlighted in §1) between highly-efficient and\\nhighly-effective neural models? (§4.2)\\nRQ2: Beyond re-ranking, can ColBERT effectively support end-\\nto-end retrieval directly from a large collection? (§4.3)\\nRQ3: What does each component of ColBERT (e.g., late interac-\\ntion, query augmentation) contribute to its quality? (§4.4)\\nRQ4: What are ColBERT’s indexing-related costs in terms of\\noffline computation and memory overhead? (§4.5)\\n4.1 Methodology\\n4.1.1 Datasets & Metrics. Similar to related work [ 2,27,28], we\\nconduct our experiments on the MS MARCO Ranking [ 24] (hence-\\nforth, MS MARCO) and TREC Complex Answer Retrieval (TREC-\\nCAR) [ 6] datasets. Both of these recent datasets provide large', start_char_idx=29932, end_char_idx=33190, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='56847c1c-f41e-4165-a93b-bad600a34abf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ac9fa16d-0a5d-41bb-95d7-b603c58d33ae', node_type=None, metadata={}, hash='860a47839f9c47b3e772641ff6b40ac0ea6642b3c10692098a5328aa959acac2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d87d146d-c0e9-48f6-b546-ccc02f694bca', node_type=None, metadata={}, hash='8923b8ce313b482ce5c662fd4df537978ca46260f6474221ec975631dabba160')}, hash='a4486893e3c7a8567dadcf20b9f6e480a7a207d6d39869335ae178a8ba2833ba', text='[ 6] datasets. Both of these recent datasets provide large train-\\ning data of the scale that facilitates training and evaluating deep\\nneural networks. We describe both in detail below.\\nMS MARCO. MS MARCO is a dataset (and a corresponding\\ncompetition) introduced by Microsoft in 2016 for reading compre-\\nhension and adapted in 2018 for retrieval. It is a collection of 8.8M\\npassages from Web pages, which were gathered from Bing’s re-\\nsults to 1M real-world queries. Each query is associated with sparse\\nrelevance judgements of one (or very few) documents marked as\\nrelevant and no documents explicitly indicated as irrelevant. Per\\nthe official evaluation, we use MRR@10 to measure effectiveness.\\nWe use three query sets in our evaluation. The official develop-\\nment and evaluation sets contain roughly 7k queries. The relevance\\njudgements of the evaluation set are held-out by Microsoft and\\nMRR@10 results can only be obtained by submitting to the com-\\npetition’s organizers. We submitted our main re-ranking ColBERT\\nmodel for §4.2. In addition, the collection includes roughly 55k\\nqueries (with labels) that are provided as additional validation data.\\nWe re-purpose a random sample of 5k queries among those (i.e.,\\nones not in our development or training sets) as a “local” evaluation\\nset. Along with the official development set, we use this held-out\\nset for testing our models as well as baselines in §4.3. We do so to\\navoid submitting multiple variants of the same model at once, as\\nthe organizers discourage too many submissions by the same team.\\nTREC CAR. Introduced by Dietz [ 6]et al. in 2017, TREC CAR is\\na synthetic dataset based on Wikipedia that consists of about 29M\\npassages. Similar to related work [ 25], we dedicate the first fourof five pre-defined folds for training (and the fifth for validation),\\nwhich amounts to roughly 3M queries generated by concatenating\\nthe title of a Wikipedia page with the heading of one of its sections.\\nThat section’s passages are marked as relevant to the corresponding\\nquery. Our evaluation is conducted on the test set used in TREC\\n2017 CAR, which contains 2,254 queries.\\n4.1.2 Implementation. Our ColBERT models are implemented us-\\ning Python 3 and PyTorch 1. We use the popular transformers6\\nlibrary for pre-trained BERT. Similar to [ 25], we fine-tune all Col-\\nBERT models with learning rate 3×10−6with a batch size 32. We fix\\nthe number of embeddings per query at 𝑁𝑞=32. Unless otherwise\\nstated, we set our ColBERT embedding dimension 𝑚to 128; §4.5\\ndemonstrates ColBERT’s robustness to a wide range of dimensions.\\nFor MS MARCO, we initialize the BERT components of the Col-\\nBERT query and document encoders using Google’s official pre-\\ntrained BERT basemodel and train all models for 200k iterations.\\nFor TREC CAR, we follow related work [ 2,25] and use a different\\npre-trained model to the official ones. To explain, the official BERT\\nmodels were pre-trained on Wikipedia, which is the source of TREC\\nCAR’s training and test sets. To avoid leaking test data into train,\\nNogueira and Cho [ 25] pre-train a BERT model on the Wiki pages\\ncorresponding to training subset of TREC CAR. They release their\\nBERT large pre-trained model, which we fine-tune for ColBERT’s ex-\\nperiments on TREC CAR. As BERT large embeddings are larger, we\\nset𝑚to 200, and since fine-tuning this model is significantly slower\\nthan BERT base, we train on TREC CAR for only 125k iterations.\\nIn our re-ranking results, unless otherwise stated, we use 4 bytes\\nper dimension in our embeddings and employ cosine as our vector-\\nsimilarity function. For end-to-end ranking, we use (squared)', start_char_idx=33208, end_char_idx=36819, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='d87d146d-c0e9-48f6-b546-ccc02f694bca', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='56847c1c-f41e-4165-a93b-bad600a34abf', node_type=None, metadata={}, hash='a4486893e3c7a8567dadcf20b9f6e480a7a207d6d39869335ae178a8ba2833ba'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f200d048-6d2a-4c05-8d4b-7b25d1318203', node_type=None, metadata={}, hash='850be28394b50d804e6e6b0a0e93d94ae429cd867a17d43714453b9c3d119ce5')}, hash='8923b8ce313b482ce5c662fd4df537978ca46260f6474221ec975631dabba160', text='function. For end-to-end ranking, we use (squared) L2\\ndistance, as we found our faiss index was faster at L2-based re-\\ntrieval. For our faiss index, we set the number of partitions to\\n𝑃=2,000, and search the nearest 𝑝=10to each query embedding to\\nretrieve𝑘′=𝑘=1000 document vectors per query embedding. We\\ndivide each embedding into 𝑠=16sub-vectors, each encoded using\\none byte. To represent the index used for the second stage of our\\nend-to-end retrieval procedure, we use 16-bit values per dimension.\\n4.1.3 Hardware & Time Measurements. To evaluate the latency of\\nneural re-ranking models in §4.2, we use a single Tesla V100 GPU\\nthat has 32 GiBs of memory on a server with two Intel Xeon Gold\\n6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469\\nGiBs of RAM. For the mostly CPU-based retrieval experiments in\\n§4.3 and the indexing experiments in §4.5, we use another server\\nwith the same CPU and system memory specifications but which\\nhas four Titan V GPUs attached, each with 12 GiBs of memory.\\nAcross all experiments, only one GPU is dedicated per query for\\nretrieval (i.e., for methods with neural computations) but we use\\nup to all four GPUs during indexing.\\n4.2 Quality–Cost Tradeoff: Top- 𝑘Re-ranking\\nIn this section, we examine ColBERT’s efficiency and effectiveness\\nat re-ranking the top- 𝑘results extracted by a bag-of-words retrieval\\nmodel, which is the most typical setting for testing and deploying\\nneural ranking models. We begin with the MS MARCO dataset. We\\n6https://github.com/huggingface/transformers\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n44Method MRR@10 (Dev) MRR@10 (Eval) Re-ranking Latency (ms) FLOPs/query\\nBM25 (official) 16.7 16.5 - -\\nKNRM 19.8 19.8 3 592M (0.085×)\\nDuet 24.3 24.5 22 159B (23×)\\nfastText+ConvKNRM 29.0 27.7 28 78B (11×)\\nBERT base[25] 34.7 - 10,700 97T (13,900×)\\nBERT base(our training) 36.0 - 10,700 97T (13,900×)\\nBERT large [25] 36.5 35.9 32,900 340T (48,600×)\\nColBERT (over BERT base) 34.9 34.9 61 7B (1×)\\nTable 1: “Re-ranking” results on MS MARCO. Each neural model re-ranks the official top-1000 results produced by BM25.\\nLatency is reported for re-ranking only. To obtain the end-to-end latency in Figure 1, we add the BM25 latency from Table 2.\\nMethod MRR@10 (Dev) MRR@10 (Local Eval) Latency (ms) Recall@50 Recall@200 Recall@1000\\nBM25 (official) 16.7 - - - - 81.4\\nBM25 (Anserini) 18.7 19.5 62 59.2 73.8 85.7\\ndoc2query 21.5 22.8 85 64.4 77.9 89.1\\nDeepCT 24.3 - 62(est.) 69 [2] 82 [2] 91 [2]\\ndocTTTTTquery 27.7 28.4 87 75.6 86.9 94.7\\nColBERT L2(re-rank) 34.8 36.4 - 75.3 80.5 81.4\\nColBERT L2(end-to-end) 36.0 36.7 458 82.9 92.3 96.8\\nTable 2: End-to-end retrieval results on MS MARCO. Each model retrieves the top-1000 documents per query directly from the\\nentire 8.8M document collection.\\ncompare against KNRM, Duet, and', start_char_idx=36825, end_char_idx=39659, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='f200d048-6d2a-4c05-8d4b-7b25d1318203', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d87d146d-c0e9-48f6-b546-ccc02f694bca', node_type=None, metadata={}, hash='8923b8ce313b482ce5c662fd4df537978ca46260f6474221ec975631dabba160'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0f74995e-4170-4bae-8090-3cf5b906d9ba', node_type=None, metadata={}, hash='6bcb31e9fa6cdd0f6842b81f881d81626feb40f3dcac8de3e06861f3172e3586')}, hash='850be28394b50d804e6e6b0a0e93d94ae429cd867a17d43714453b9c3d119ce5', text='8.8M document collection.\\ncompare against KNRM, Duet, and fastText+ConvKNRM, a repre-\\nsentative set of neural matching models that have been previously\\ntested on MS MARCO. In addition, we compare against the adapta-\\ntion of BERT for ranking by Nogueira and Cho [ 25], in particular,\\ntheir BERT baseand its deeper counterpart BERT large.7\\nWe report the competition’s official metric, namely MRR@10, on\\nthe validation set (Dev) and the evaluation set (Eval). We also report\\nthe re-ranking latency, which we measure using a single Tesla V100\\nGPU, and the FLOPs per query for each neural ranking model. To do\\nso, we adapt the baselines’ publicly-available reference implemen-\\ntations into our pytorch testbed. For ColBERT, our reported latency\\nsubsumes the entire computation from gathering the document\\nrepresentations, moving them to the GPU, tokenizing then encod-\\ning the query, and applying late interaction to compute document\\nscores. For the baselines, we measure the scoring computations on\\nthe GPU and exclude the CPU-based text preprocessing (similar\\nto [9]). In principle, the baselines can pre-compute most of this\\npreprocessing (e.g., document tokenization) offline. We estimate\\nthe FLOPs using the torchprofile8library.\\nWe now proceed to study the results, which are reported in Ta-\\nble 1. To begin with, we notice the fast progress from KNRM in\\n2017 to the BERT-based models in 2019, manifesting itself in over\\n16% increase in MRR@10. As described in §1, the simultaneous\\nincrease in computational cost is difficult to miss. Judging by their\\nrather monotonic pattern of increasingly larger cost and higher ef-\\nfectiveness, these results appear to paint a picture where expensive\\nmodels are necessary for high-quality ranking.\\nIn contrast with this trend, ColBERT (which employs late in-\\nteraction over BERT base) performs competitively with the original\\n7https://github.com/nyu-dl/dl4marco-bert/\\n8https://github.com/mit-han-lab/torchprofileadaptation of BERT baseand BERT large for ranking by Nogueira\\nand Cho [ 25,27]. Interestingly, ColBERT appears no worse than\\nBERT basein MRR@10—although the latter uses a different loss func-\\ntion to ColBERT’s (§3.3). To confirm the intuition that ColBERT’s\\nlate interaction does trade away some of BERT’s quality, the table\\nalso reports results of “BERT base(our training)”, which is based on\\nNogueira and Cho’s model of the same size but is optimized with\\npairwise softmax cross-entropy loss. We train it with learning rate\\n3×10−6and batch size 16 for 200k iterations. Unlike the original\\nBERT baseranker, results show that this model does in fact have an\\nedge over ColBERT’s effectiveness.\\nWhile highly competitive in retrieval quality, ColBERT is orders\\nof magnitude cheaper than BERT base, in particular, by over 170 ×in\\nlatency and 13,900×in FLOPs. This highlights the expressiveness\\nof our proposed late interaction mechanism when coupled with a\\npowerful pre-trained LM like BERT. While ColBERT’s re-ranking\\nlatency is slightly higher than the non-BERT models shown (i.e., by\\n10s of milliseconds), this difference is explained by the time it takes\\na simple Python implementation to gather, stack, and transfer the\\ndocument embeddings to the GPU. In particular, the query encoding\\nand interaction in ColBERT consume only 13 milliseconds of its\\ntotal execution time.\\nDiving deeper into the quality–cost tradeoff between BERT and\\nColBERT, Figure 4 demonstrates the relationships between FLOPs\\nand effectiveness (MRR@10) as a function of the re-ranking depth\\n𝑘when re-ranking the top- 𝑘results by BM25, comparing', start_char_idx=39653, end_char_idx=43219, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='0f74995e-4170-4bae-8090-3cf5b906d9ba', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f200d048-6d2a-4c05-8d4b-7b25d1318203', node_type=None, metadata={}, hash='850be28394b50d804e6e6b0a0e93d94ae429cd867a17d43714453b9c3d119ce5'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='adc9d968-9e1e-4560-bbeb-48c7a5d697a6', node_type=None, metadata={}, hash='ceb1035fe4453cc25113b8ca2b51ae4d4d80378a18ed31bcd71811ed3389ad06')}, hash='6bcb31e9fa6cdd0f6842b81f881d81626feb40f3dcac8de3e06861f3172e3586', text='re-ranking the top- 𝑘results by BM25, comparing ColBERT\\nand BERT base(our training). We conduct this experiment on MS\\nMARCO (Dev). We note here that as the official top-1000 ranking\\ndoes not provide the BM25 order (and also lacks documents beyond\\nthe top-1000 per query), the models in this experiment re-rank the\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n450.27 0.29 0.31 0.33 0.35 0.37\\nMRR@10103104105106107108109Million FLOPs (log-scale)\\nk=10205010020050010002000\\nk=10 20 50 100\\n200500\\n10002000\\nBERTbase (our training)\\nColBERTFigure 4: FLOPs (in millions) and MRR@10 as functions\\nof the re-ranking depth 𝑘. Since the official BM25 ranking\\nis not ordered, the initial top- 𝑘retrieval is conducted with\\nAnserini’s BM25.\\nAnserini [ 37] toolkit’s BM25 output. Consequently, both MRR@10\\nvalues at𝑘=1000 are slightly higher from those reported in Table 1.\\nStudying the results in Figure 4, we notice that not only is Col-\\nBERT much cheaper than BERT for the same model size (i.e., 12-\\nlayer “base” transformer encoder), it also scales better with the\\nnumber of ranked documents. In part, this is because ColBERT\\nonly needs to process the query once, irrespective of the number of\\ndocuments evaluated. For instance, at 𝑘=10, BERT requires nearly\\n180×more FLOPs than ColBERT; at 𝑘=1000, BERT’s overhead\\njumps to 13,900×. It then reaches 23,000× at𝑘=2000.\\nWe observe that this orders-of-magnitude reduction in FLOPs\\nmakes it practical to run ColBERT entirely on the CPU. In fact, sub-\\nsequent informal experimentation suggests that ColBERT’s latency\\nand FLOPs can be considerably reduced further by a number of\\noptimizations, some entailing a controllable quality tradeoff. These\\ninclude using smaller vector dimensions (whose MRR@10 is tested\\nin §4.5), padding queries to shorter 𝑁𝑞, processing documents in\\na lengths-aware fashion, and distilling/quantizing the encoder(s)\\n(§2), the final two of which are also applicable to the baseline BERT\\nreference implementation by Nogueira and Cho [ 25]. Addition-\\nally, caching the document embeddings on the GPU(s)—if sufficient\\nGPU memory exists—can significantly reduce ColBERT’s latency.\\nLastly, batch-processing of multiple queries can enhance ColBERT’s\\nthroughput by improving the GPU utilization of query encoding.\\nWe leave exploring these opportunities for future work.\\nMethod MAP MRR@10\\nBM25 (Anserini) 15.3 -\\ndoc2query 18.1 -\\nDeepCT 24.6 33.2\\nBM25 + BERT base 31.0 -\\nBM25 + BERT large 33.5 -\\nBM25 + ColBERT 31.3 44.2\\nTable 3: Results on TREC CAR.\\nHaving studied our results on MS MARCO, we now consider\\nTREC CAR, whose official metric is MAP. Similar to Table 1, we\\nalso report MRR@10. The results are summarized in Table 3, which\\nincludes a number of important baselines (BM25, doc2query, and\\nDeepCT) in addition to re-ranking baselines that have previouslybeen tested on this dataset. As the table shows, the results mirror\\nthose seen with MS MARCO.\\n4.3 End-to-end Top- 𝑘Retrieval\\nBeyond cheap re-ranking, ColBERT is amenable to top- 𝑘retrieval di-\\nrectly from a full collection. Table 2 considers full retrieval, wherein\\neach model retrieves the top-1000 documents directly from MS\\nMARCO’s 8.8M documents per', start_char_idx=43230, end_char_idx=46439, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='adc9d968-9e1e-4560-bbeb-48c7a5d697a6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0f74995e-4170-4bae-8090-3cf5b906d9ba', node_type=None, metadata={}, hash='6bcb31e9fa6cdd0f6842b81f881d81626feb40f3dcac8de3e06861f3172e3586'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='af975e6f-2135-498d-8c97-ede7497f8320', node_type=None, metadata={}, hash='c6a12db7726e7a5032639456f22d54fcd7558124bbf41a640691bb2ce888b3cf')}, hash='ceb1035fe4453cc25113b8ca2b51ae4d4d80378a18ed31bcd71811ed3389ad06', text='documents directly from MS\\nMARCO’s 8.8M documents per query. In addition to MRR@10 and\\nlatency in milliseconds, the table reports Recall@50, Recall@200,\\nand Recall@1000, important metrics for a full-retrieval model that\\nessentially filters down a large collection on a per-query basis.\\nWe compare against BM25, in particular MS MARCO’s official\\nBM25 ranking as well as a well-tuned baseline based on the Anserini\\ntoolkit.9While many other traditional models exist, we are not\\naware of any that substantially outperform Anserini’s BM25 im-\\nplementation (e.g., see RM3 in [ 28], LMDir in [ 2], or Microsoft’s\\nproprietary feature-based RankSVM on the leaderboard).\\nWe also compare against doc2query, DeepCT, and docTTTTT-\\nquery. All three rely on a traditional bag-of-words model (primarily\\nBM25) for retrieval. Crucially, however, they re-weigh the frequency\\nof terms per document and/or expand the set of terms in each doc-\\nument before building the BM25 index. In particular, doc2query\\nexpands each document with a pre-defined number of synthetic\\nqueries generated by a seq2seq transformer model (which docTTT-\\nTquery replaced with a pre-trained language model, T5 [ 31]). In\\ncontrast, DeepCT uses BERT to produce the term frequency com-\\nponent of BM25 in a context-aware manner.\\nFor the latency of Anserini’s BM25, doc2query, and docTTTT-\\nquery, we use the authors’ [ 26,28] Anserini-based implementation.\\nWhile this implementation supports multi-threading, it only utilizes\\nparallelism across different queries. We thus report single-threaded\\nlatency for these models, noting that simply parallelizing their com-\\nputation over shards of the index can substantially decrease their\\nalready-low latency. For DeepCT, we only estimate its latency us-\\ning that of BM25 (as denoted by (est.) in the table), since DeepCT\\nre-weighs BM25’s term frequency without modifying the index\\notherwise.10As discussed in §4.1, we use ColBERT L2for end-to-\\nend retrieval, which employs negative squared L2 distance as its\\nvector-similarity function. For its latency, we measure the time for\\nfaiss -based candidate filtering and the subsequent re-ranking. In\\nthis experiment, faiss uses all available CPU cores.\\nLooking at Table 2, we first see Anserini’s BM25 baseline at 18.7\\nMRR@10, noticing its very low latency as implemented in Anserini\\n(which extends the well-known Lucene system), owing to both\\nvery cheap operations and decades of bag-of-words top- 𝑘retrieval\\noptimizations. The three subsequent baselines, namely doc2query,\\nDeepCT, and docTTTTquery, each brings a decisive enhancement\\nto effectiveness. These improvements come at negligible overheads\\nin latency, since these baselines ultimately rely on BM25-based\\nretrieval. The most effective among these three, docTTTTquery,\\ndemonstrates a massive 9% gain over vanilla BM25 by fine-tuning\\nthe recent language model T5.\\n9http://anserini.io/\\n10In practice, a myriad of reasons could still cause DeepCT’s latency to differ\\nslightly from BM25’s. For instance, the top- 𝑘pruning strategy employed, if any, could\\ninteract differently with a changed distribution of scores.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n46Shifting our attention to ColBERT’s end-to-end retrieval effec-\\ntiveness, we see its major gains in MRR@10 over all of these end-\\nto-end models. In fact, using ColBERT in the end-to-end setup is', start_char_idx=46434, end_char_idx=49833, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='af975e6f-2135-498d-8c97-ede7497f8320', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='adc9d968-9e1e-4560-bbeb-48c7a5d697a6', node_type=None, metadata={}, hash='ceb1035fe4453cc25113b8ca2b51ae4d4d80378a18ed31bcd71811ed3389ad06'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='35e7d8b3-c1c2-41a0-8826-b0e76b2ef38a', node_type=None, metadata={}, hash='db37ea28c0e4a3531612496c8671128cdf9565ae1797e49b5d0dd51f1379950b')}, hash='c6a12db7726e7a5032639456f22d54fcd7558124bbf41a640691bb2ce888b3cf', text='models. In fact, using ColBERT in the end-to-end setup is su-\\nperior in terms of MRR@10 to re-ranking with the same model due\\nto the improved recall. Moving beyond MRR@10, we also see large\\ngains in Recall@ 𝑘for𝑘equals to 50, 200, and 1000. For instance, its\\nRecall@50 actually exceeds the official BM25’s Recall@1000 and\\neven all but docTTTTTquery’s Recall@200, emphasizing the value\\nof end-to-end retrieval (instead of just re-ranking) with ColBERT.\\n4.4 Ablation Studies\\n0.220.240.260.280.300.320.340.36\\nMRR@10BERT [CLS]-based dot-product (5-layer)  [A]\\nColBERT via average similarity (5-layer)  [B]\\nColBERT without query augmentation (5-layer)  [C]\\nColBERT (5-layer)  [D]\\nColBERT (12-layer)  [E]\\nColBERT + e2e retrieval (12-layer)  [F]\\nFigure 5: Ablation results on MS MARCO (Dev). Between\\nbrackets is the number of BERT layers used in each model.\\nThe results from §4.2 indicate that ColBERT is highly effective\\ndespite the low cost and simplicity of its late interaction mechanism.\\nTo better understand the source of this effectiveness, we examine a\\nnumber of important details in ColBERT’s interaction and encoder\\narchitecture. For this ablation, we report MRR@10 on the validation\\nset of MS MARCO in Figure 5, which shows our main re-ranking\\nColBERT model [E], with MRR@10 of 34.9%.\\nDue to the cost of training all models, we train a copy of our\\nmain model that retains only the first 5 layers of BERT out of 12\\n(i.e., model [D]) and similarly train all our ablation models for\\n200k iterations with five BERT layers. To begin with, we ask if the\\nfine-granular interaction in late interaction is necessary. Model [A]\\ntackles this question: it uses BERT to produce a single embedding\\nvector for the query and another for the document, extracted from\\nBERT’s [CLS] contextualized embedding and expanded through a\\nlinear layer to dimension 4096 (which equals 𝑁𝑞×128=32×128).\\nRelevance is estimated as the inner product of the query’s and the\\ndocument’s embeddings, which we found to perform better than\\ncosine similarity for single-vector re-ranking. As the results show,\\nthis model is considerably less effective than ColBERT, reinforcing\\nthe importance of late interaction.\\nSubsequently, we ask if our MaxSim-based late interaction is bet-\\nter than other simple alternatives. We test a model [B] that replaces\\nColBERT’s maximum similarity with average similarity. The results\\nsuggest the importance of individual terms in the query paying\\nspecial attention to particular terms in the document. Similarly,\\nthe figure emphasizes the importance of our query augmentation\\nmechanism: without query augmentation [C], ColBERT has a no-\\nticeably lower MRR@10. Lastly, we see the impact of end-to-end\\nretrieval not only on recall but also on MRR@10. By retrieving\\ndirectly from the full collection, ColBERT is able to retrieve to the\\ntop-10 documents missed entirely from BM25’s top-1000.\\n4.5 Indexing Throughput & Footprint\\nLastly, we examine the indexing throughput and space footprint\\nof ColBERT. Figure 6 reports indexing throughput on MS MARCO\\n0 10000 20000 30000 40000 50000\\nThroughput (documents/minute)Basic ColBERT Indexing\\n+multi-GPU document processing\\n+per-batch maximum sequence length\\n+length-based bucketing\\n+multi-core pre-processingFigure 6: Effect of ColBERT’s indexing optimizations on the\\noffline indexing throughput.\\ndocuments with ColBERT and four other ablation settings,', start_char_idx=49833, end_char_idx=53221, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='35e7d8b3-c1c2-41a0-8826-b0e76b2ef38a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='af975e6f-2135-498d-8c97-ede7497f8320', node_type=None, metadata={}, hash='c6a12db7726e7a5032639456f22d54fcd7558124bbf41a640691bb2ce888b3cf'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ff606784-9596-4e6d-bc1a-3658bcaf5ca3', node_type=None, metadata={}, hash='22ba03638b0127bbd3b84fe19f0662e3b6028c3048e5fca645169f4a9d3fccac')}, hash='db37ea28c0e4a3531612496c8671128cdf9565ae1797e49b5d0dd51f1379950b', text='indexing throughput.\\ndocuments with ColBERT and four other ablation settings, which\\nindividually enable optimizations described in §3.4 on top of basic\\nbatched indexing. Based on these throughputs, ColBERT can index\\nMS MARCO in about three hours. Note that any BERT-based model\\nmust incur the computational cost of processing each document\\nat least once. While ColBERT encodes each document with BERT\\nexactly once, existing BERT-based rankers would repeat similar\\ncomputations on possibly hundreds of documents for each query.\\nSetting Dimension( 𝑚) Bytes/Dim Space(GiBs) MRR@10\\nRe-rank Cosine 128 4 286 34.9\\nEnd-to-end L2 128 2 154 36.0\\nRe-rank L2 128 2 143 34.8\\nRe-rank Cosine 48 4 54 34.4\\nRe-rank Cosine 24 2 27 33.9\\nTable 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.\\nTable 4 reports the space footprint of ColBERT under various\\nsettings as we reduce the embeddings dimension and/or the bytes\\nper dimension. Interestingly, the most space-efficient setting, that\\nis, re-ranking with cosine similarity with 24-dimensional vectors\\nstored as 2-byte floats, is only 1% worse in MRR@10 than the most\\nspace-consuming one, while the former requires only 27 GiBs to\\nrepresent the MS MARCO collection.\\n5 CONCLUSIONS\\nIn this paper, we introduced ColBERT, a novel ranking model that\\nemploys contextualized late interaction over deep LMs (in particular,\\nBERT) for efficient retrieval. By independently encoding queries\\nand documents into fine-grained representations that interact via\\ncheap and pruning-friendly computations, ColBERT can leverage\\nthe expressiveness of deep LMs while greatly speeding up query\\nprocessing. Crucially, doing so allows scaling ColBERT to end-\\nto-end neural retrieval directly from a large document collection,\\nwhich can greatly improve recall over existing models. Our results\\nshow that ColBERT is two orders-of-magnitude faster than existing\\nBERT-based models, all while only minimally impacting re-ranking\\nquality and while outperforming every non-BERT baseline.\\nAcknowledgments. OK was supported by the Eltoukhy Family\\nGraduate Fellowship at the Stanford School of Engineering. This\\nresearch was supported in part by affiliate members and other\\nsupporters of the Stanford DAWN project—Ant Financial, Facebook,\\nGoogle, Infosys, NEC, and VMware—as well as Cisco, SAP, and the\\nNSF under CAREER grant CNS-1651570. Any opinions, findings,\\nand conclusions or recommendations expressed in this material are\\nthose of the authors and do not necessarily reflect the views of the\\nNational Science Foundation.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n47REFERENCES\\n[1]Firas Abuzaid, Geet Sethi, Peter Bailis, and Matei Zaharia. 2019. To Index or Not\\nto Index: Optimizing Exact Maximum Inner Product Search. In 2019 IEEE 35th\\nInternational Conference on Data Engineering (ICDE). IEEE, 1250–1261.\\n[2]Zhuyun Dai and Jamie Callan. 2019. Context-Aware Sentence/Passage Term\\nImportance Estimation For First Stage Retrieval. arXiv preprint arXiv:1910.10687\\n(2019).\\n[3]Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with\\nContextual Neural Language Modeling. arXiv preprint arXiv:1905.09217 (2019).\\n[4]Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional\\nneural networks for soft-matching n-grams in ad-hoc search. In Proceedings of the\\neleventh ACM international conference on web search and data mining. 126–134.\\n[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.', start_char_idx=53201, end_char_idx=56685, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='ff606784-9596-4e6d-bc1a-3658bcaf5ca3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='35e7d8b3-c1c2-41a0-8826-b0e76b2ef38a', node_type=None, metadata={}, hash='db37ea28c0e4a3531612496c8671128cdf9565ae1797e49b5d0dd51f1379950b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2081813c-0494-42a5-8726-11b66f19a933', node_type=None, metadata={}, hash='fca1c81bf83f34c71b5812a6e17fae011093bca12791adc5fbe71df5e351c809')}, hash='22ba03638b0127bbd3b84fe19f0662e3b6028c3048e5fca645169f4a9d3fccac', text='Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\\nPre-training of deep bidirectional transformers for language understanding. arXiv\\npreprint arXiv:1810.04805 (2018).\\n[6]Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. 2017. TREC\\nComplex Answer Retrieval Overview.. In TREC.\\n[7]Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance\\nmatching model for ad-hoc retrieval. In Proceedings of the 25th ACM International\\non Conference on Information and Knowledge Management. ACM, 55–64.\\n[8]Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen\\nWu, W Bruce Croft, and Xueqi Cheng. 2019. A deep look into neural ranking\\nmodels for information retrieval. arXiv preprint arXiv:1903.06902 (2019).\\n[9]Sebastian Hofstätter and Allan Hanbury. 2019. Let’s measure run time! Extending\\nthe IR replicability infrastructure to include performance aspects. arXiv preprint\\narXiv:1907.04614 (2019).\\n[10] Sebastian Hofstätter, Navid Rekabsaz, Carsten Eickhoff, and Allan Hanbury. 2019.\\nOn the effect of low-frequency terms on neural-IR models. In Proceedings of\\nthe 42nd International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval. 1137–1140.\\n[11] Sebastian Hofstätter, Markus Zlabinger, and Allan Hanbury. 2019. TU Wien@\\nTREC Deep Learning’19–Simple Contextualization for Re-ranking. arXiv preprint\\narXiv:1912.01385 (2019).\\n[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry\\nHeck. 2013. Learning deep structured semantic models for web search using\\nclickthrough data. In Proceedings of the 22nd ACM international conference on\\nInformation & Knowledge Management. 2333–2338.\\n[13] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Efficient Interaction-based Neural\\nRanking with Locality Sensitive Hashing. In The World Wide Web Conference.\\nACM, 2858–2864.\\n[14] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\\nand Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.\\narXiv preprint arXiv:1909.10351 (2019).\\n[15] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity\\nsearch with GPUs. arXiv preprint arXiv:1702.08734 (2017).\\n[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\\nmization. arXiv preprint arXiv:1412.6980 (2014).\\n[17] Ron Kohavi, Alex Deng, Brian Frasca, Toby Walker, Ya Xu, and Nils Pohlmann.\\n2013. Online controlled experiments at large scale. In SIGKDD.\\n[18] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. Cedr:\\nContextualized embeddings for document ranking. In Proceedings of the 42nd\\nInternational ACM SIGIR Conference on Research and Development in Information\\nRetrieval. ACM, 1101–1104.\\n[19] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really\\nBetter than One?. In Advances in Neural Information Processing Systems . 14014–\\n14024.\\n[20] Bhaskar Mitra and Nick Craswell. 2019. An Updated Duet Model', start_char_idx=56710, end_char_idx=59680, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='2081813c-0494-42a5-8726-11b66f19a933', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ff606784-9596-4e6d-bc1a-3658bcaf5ca3', node_type=None, metadata={}, hash='22ba03638b0127bbd3b84fe19f0662e3b6028c3048e5fca645169f4a9d3fccac'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='11928e8d-af3d-41ca-a56a-1e738d42d612', node_type=None, metadata={}, hash='58c5983b9e47bb371fed0bfa45b3283ec6433598740ce747a6910666b58eb776')}, hash='fca1c81bf83f34c71b5812a6e17fae011093bca12791adc5fbe71df5e351c809', text='Bhaskar Mitra and Nick Craswell. 2019. An Updated Duet Model for Passage\\nRe-ranking. arXiv preprint arXiv:1903.07666 (2019).\\n[21] Bhaskar Mitra, Nick Craswell, et al .2018. An introduction to neural information\\nretrieval. Foundations and Trends® in Information Retrieval 13, 1 (2018), 1–126.\\n[22] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using\\nlocal and distributed representations of text for web search. In Proceedings of the26th International Conference on World Wide Web. International World Wide Web\\nConferences Steering Committee, 1291–1299.\\n[23] Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz,\\nand Emine Yilmaz. 2019. Incorporating query term independence assumption\\nfor efficient retrieval and ranking using deep neural networks. arXiv preprint\\narXiv:1907.03693 (2019).\\n[24] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A Human-Generated MAchine\\nReading COmprehension Dataset. (2016).\\n[25] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\\narXiv preprint arXiv:1901.04085 (2019).\\n[26] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to\\ndocTTTTTquery. (2019).\\n[27] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-Stage\\nDocument Ranking with BERT. arXiv preprint arXiv:1910.14424 (2019).\\n[28] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\\nExpansion by Query Prediction. arXiv preprint arXiv:1904.08375 (2019).\\n[29] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word\\nrepresentations. arXiv preprint arXiv:1802.05365 (2018).\\n[30] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding\\nthe Behaviors of BERT in Ranking. arXiv preprint arXiv:1904.07531 (2019).\\n[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim-\\nits of transfer learning with a unified text-to-text transformer. arXiv preprint\\narXiv:1910.10683 (2019).\\n[32] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\\nMike Gatford, et al. 1995. Okapi at TREC-3. NIST Special Publication (1995).\\n[33] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.\\n2019. Distilling task-specific knowledge from BERT into simple neural networks.\\narXiv preprint arXiv:1903.12136 (2019).\\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in neural information processing systems. 5998–6008.\\n[35] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,\\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al', start_char_idx=59675, end_char_idx=62569, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='11928e8d-af3d-41ca-a56a-1e738d42d612', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2081813c-0494-42a5-8726-11b66f19a933', node_type=None, metadata={}, hash='fca1c81bf83f34c71b5812a6e17fae011093bca12791adc5fbe71df5e351c809'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='73f1ea17-7a4f-4031-b23e-b644fea64432', node_type=None, metadata={}, hash='88bfa6a571bf431acc99e2bba8ab361556a87fbebcfcb1b6c298eb192ff56edc')}, hash='58c5983b9e47bb371fed0bfa45b3283ec6433598740ce747a6910666b58eb776', text='Yuan Cao, Qin Gao, Klaus Macherey, et al .\\n2016. Google’s neural machine translation system: Bridging the gap between\\nhuman and machine translation. arXiv preprint arXiv:1609.08144 (2016).\\n[36] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017.\\nEnd-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th\\nInternational ACM SIGIR conference on research and development in information\\nretrieval. 55–64.\\n[37] Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini: Reproducible ranking\\nbaselines using Lucene. Journal of Data and Information Quality (JDIQ) 10, 4\\n(2018), 1–20.\\n[38] Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin. 2019. Critically Examining\\nthe\" Neural Hype\" Weak Baselines and the Additivity of Effectiveness Gains\\nfrom Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR\\nConference on Research and Development in Information Retrieval. 1129–1132.\\n[39] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019.\\nCross-domain modeling of sentence-level evidence for document retrieval. In\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\\ncessing and the 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP). 3481–3487.\\n[40] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert:\\nQuantized 8bit bert. arXiv preprint arXiv:1910.06188 (2019).\\n[41] Hamed Zamani, Mostafa Dehghani, W Bruce Croft, Erik Learned-Miller, and\\nJaap Kamps. 2018. From neural re-ranking to neural ranking: Learning a sparse\\nrepresentation for inverted indexing. In Proceedings of the 27th ACM International\\nConference on Information and Knowledge Management. ACM, 497–506.\\n[42] Le Zhao. 2012. Modeling and solving term mismatch for full-text retrieval. Ph.D.\\nDissertation. Carnegie Mellon University.\\nSession 1A: NeuIR and Semantic Matching \\n \\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n48\\n1, 2, https://arxiv.org/pdf/2104.07186.pdf, COIL: Revisit Exact Lexical Match in Information Retrieval\\nwith Contextualized Inverted List\\nLuyu Gao, Zhuyun Dai, Jamie Callan\\nLanguage Technologies Institute\\nCarnegie Mellon University\\n{luyug, zhuyund, callan}@cs.cmu.edu\\nAbstract\\nClassical information retrieval systems such as\\nBM25 rely on exact lexical match and carry\\nout search efﬁciently with inverted list index.\\nRecent neural IR models shifts towards soft\\nsemantic matching all query document terms,\\nbut they lose the computation efﬁciency of\\nexact match systems. This paper presents\\nCOIL, a contextualized exact match retrieval\\narchitecture that brings semantic lexical match-\\ning. COIL scoring is based on overlapping\\nquery document tokens’ contextualized repre-\\nsentations. The new architecture stores con-\\ntextualized token representations in inverted\\nlists, bringing together the efﬁciency of exact\\nmatch and the representation power of deep\\nlanguage models. Our experimental results\\nshow COIL outperforms classical lexical re-\\ntrievers and state-of-the-art deep LM retrievers\\nwith similar or smaller latency.1\\n1 Introduction\\nWidely used, bag-of-words (BOW) information re-\\ntrieval (IR) systems such as BM25 rely on exact\\nlexical match2between query and document terms.\\nRecent study in neural IR takes a different approach\\nand compute soft', start_char_idx=62588, end_char_idx=65879, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='73f1ea17-7a4f-4031-b23e-b644fea64432', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='11928e8d-af3d-41ca-a56a-1e738d42d612', node_type=None, metadata={}, hash='58c5983b9e47bb371fed0bfa45b3283ec6433598740ce747a6910666b58eb776'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='aba7c4ca-2457-4635-8dbd-8f81304cc552', node_type=None, metadata={}, hash='37d9af4967c31d3ab9e51de2d696abf5262b90f82c4965623a36bd999764393f')}, hash='88bfa6a571bf431acc99e2bba8ab361556a87fbebcfcb1b6c298eb192ff56edc', text='terms.\\nRecent study in neural IR takes a different approach\\nand compute soft matching between all query and\\ndocument terms to model complex matching.\\nThe shift to soft matching in neural IR models\\nattempts to address vocabulary mismatch problems,\\nthat query and the relevant documents use differ-\\nent terms, e.g. cat v.s. kitty, for the same con-\\ncept (Huang et al., 2013; Guo et al., 2016; Xiong\\net al., 2017). Later introduction of contextualized\\nrepresentations (Peters et al., 2018) from deep lan-\\nguage models (LM) further address semantic mis-\\nmatch , that the same term can refer to different\\nconcepts, e.g., bank of river vs. bank in ﬁnance.\\nFine-tuned deep LM rerankers produce token rep-\\nresentations based on context and achieve state-of-\\n1Our code is available at https://github.com/\\nluyug/COIL .\\n2Exact match up to morphological changes.the-art in text ranking with huge performance leap\\n(Nogueira and Cho, 2019; Dai and Callan, 2019b).\\nThough the idea of soft matching all tokens is\\ncarried through the development of neural IR mod-\\nels, seeing the success brought by deep LMs, we\\ntake a step back and ask: how much gain can we get\\nif we introduce contextualized representations back\\nto lexical exact match systems? In other words, can\\nwe build a system that still performs exact query-\\ndocument token matching but compute matching\\nsignals with contextualized token representations\\ninstead of heuristics? This may seem a constraint\\non the model, but exact lexical match produce more\\nexplainable and controlled patterns than soft match-\\ning. It also allows search to focus on only the\\nsubset of documents that have overlapping terms\\nwith query, which can be done efﬁciently with in-\\nverted list index. Meanwhile, using dense contex-\\ntualized token representations enables the model\\nto handle semantic mismatch, which has been a\\nlong-standing problem in classic lexical systems.\\nTo answer the question, we propose a new lexi-\\ncal matching scheme that uses vector similarities\\nbetween query-document overlapping term contex-\\ntualized representations to replace heuristic scor-\\ning used in classical systems. We present COn-\\ntextualized Inverted List (COIL), a new exact lex-\\nical match retrieval architecture armed with deep\\nLM representations. COIL processes documents\\nwith deep LM ofﬂine and produces representations\\nfor each document token. The representations are\\ngrouped by their surface tokens into inverted lists.\\nAt search time, we build representation vectors\\nfor query tokens and perform contextualized ex-\\nact match: use each query token to look up its\\nown inverted list and compute vector similarity\\nwith document vectors stored in the inverted list\\nas matching scores. COIL enables efﬁcient search\\nwith rich-in-semantic matching between query and\\ndocument.\\nOur contributions include 1) introduce a novelarXiv:2104.07186v1  [cs.IR]  15 Apr 2021retrieval architecture, contextualized inverted\\nlists (COIL) that brings semantic matching into\\nlexical IR systems, 2) show matching signals in-\\nduced from exact lexical match can capture com-\\nplicated matching patterns, 3) demonstrate COIL\\nsigniﬁcantly outperform classical and deep LM\\naugmented lexical retrievers as well as state-of-the-\\nart dense retrievers on two retrieval tasks.\\n2 Related Work\\nLexical Retriever Classical IR systems rely on\\nexact lexical match retrievers such as Boolean\\nRetrieval, BM25 (Robertson and Walker, 1994)\\nand statistical language models (Lafferty and Zhai,\\n2001). This type of retrieval model can process\\nqueries very quickly by organizing the documents\\ninto inverted index, where each distinct term has\\nan inverted list that stores information about docu-\\nments it', start_char_idx=65847, end_char_idx=69514, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='aba7c4ca-2457-4635-8dbd-8f81304cc552', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='73f1ea17-7a4f-4031-b23e-b644fea64432', node_type=None, metadata={}, hash='88bfa6a571bf431acc99e2bba8ab361556a87fbebcfcb1b6c298eb192ff56edc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='71916901-659b-40c1-9f5d-8f7c231b736c', node_type=None, metadata={}, hash='5926c880fd5c3fee64db462c80af6bfd1935ece1132f0af9efa68d39251f9c31')}, hash='37d9af4967c31d3ab9e51de2d696abf5262b90f82c4965623a36bd999764393f', text='each distinct term has\\nan inverted list that stores information about docu-\\nments it appears in. Nowadays, they are still widely\\nused in production systems. However, these re-\\ntrieval models fall short of matching related terms\\n(vocabulary mismatch) or modeling context of the\\nterms (semantic mismatch). Much early effort\\nwas put into improving exact lexical match retriev-\\ners, such as matching n-grams (Metzler and Croft,\\n2005) or expanding queries with terms from related\\ndocuments (Lavrenko and Croft, 2001). However,\\nthese methods still use BOW framework and have\\nlimited capability of modeling human languages.\\nNeural Ranker In order to deal with vocab-\\nulary mismatch, neural retrievers that rely on\\nsoft matching between numerical text represen-\\ntations are introduced. Early attempts compute\\nsimilarity between pre-trained word embedding\\nsuch as word2vec (Mikolov et al., 2013) and\\nGLoVe (Pennington et al., 2014) to produce match-\\ning score (Ganguly et al., 2015; Diaz et al., 2016).\\nOne more recent approach encodes query and doc-\\nument each into a vector and computes vector sim-\\nilarity (Huang et al., 2013). Later researches real-\\nized the limited capacity of a single vector to en-\\ncode ﬁne-grained information and introduced full\\ninteraction models to perform soft matching be-\\ntween all term vectors (Guo et al., 2016; Xiong\\net al., 2017). In these approaches, scoring is\\nbased on learned neural networks and the hugely\\nincreased computation cost limited their use to\\nreranking a top candidate list generated by a lexical\\nretriever.Deep LM Based Ranker and Retriever Deep\\nLM made a huge impact on neural IR. Fine-\\ntuned Transformer (Vaswani et al., 2017) LM\\nBERT (Devlin et al., 2019) achieved state-of-the-\\nart reranking performance for passages and docu-\\nments (Nogueira and Cho, 2019; Dai and Callan,\\n2019b). As illustrated in Figure 1a, the common\\napproach is to feed the concatenated query docu-\\nment text through BERT and use BERT’s [CLS]\\noutput token to produce a relevance score. The\\ndeep LM rerankers addressed both vocabulary and\\nsemantic mismatch by computing full cross atten-\\ntion between contextualized token representations.\\nLighter deep LM rankers are developed (MacA-\\nvaney et al., 2020; Gao et al., 2020), but their cross\\nattention operations are still too expensive for full-\\ncollection retrieval.\\nLater research therefore resorted to augment-\\ning lexical retrieval with deep LMs by expanding\\nthe document surface form to narrow the vocab-\\nulary gap, e.g., DocT5Query (Nogueira and Lin,\\n2019), or altering term weights to emphasize impor-\\ntant terms, e.g., DeepCT (Dai and Callan, 2019a).\\nSmartly combining deep LM retriever and reranker\\ncan offer additive gain for end performance (Gao\\net al., 2021a). These retrievers however still suffer\\nfrom vocabulary and semantic mismatch as tradi-\\ntional lexical retrievers.\\nAnother line of research continues the work on\\nsingle vector representation and build dense retriev-\\ners, as illustrated in Figure 1b. They store docu-\\nment vectors in a dense index and retrieve them\\nthrough Nearest Neighbours search. Using deep\\nLMs, dense retrievers have achieved promising re-\\nsults on several retrieval tasks (Karpukhin et al.,\\n2020). Later researches show that dense retrieval\\nsystems can be further improved by better train-\\ning (Xiong et al., 2020; Gao et al., 2021b).\\nSingle vector systems have also been extended\\nto multi-vector representation systems. Poly-\\nencoder (Humeau et al., 2020) encodes queries\\ninto a set of vectors. Similarly,', start_char_idx=69508, end_char_idx=73023, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='71916901-659b-40c1-9f5d-8f7c231b736c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='aba7c4ca-2457-4635-8dbd-8f81304cc552', node_type=None, metadata={}, hash='37d9af4967c31d3ab9e51de2d696abf5262b90f82c4965623a36bd999764393f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8f6b4ff5-7756-470a-84a3-3b208662bb7f', node_type=None, metadata={}, hash='52f7a75468e206bb3e4dc5778fe9a00d216f0daf6fd2cd5d972c3ce8a852e704')}, hash='5926c880fd5c3fee64db462c80af6bfd1935ece1132f0af9efa68d39251f9c31', text='et al., 2020) encodes queries\\ninto a set of vectors. Similarly, Me-BERT (Luan\\net al., 2020) represents documents with a set of vec-\\ntors. A concurrent work ColBERT (Figure 1c) use\\nmultiple vectors to encode both queries and docu-\\nments (Khattab and Zaharia, 2020). In particular, it\\nrepresents a documents with all its terms’ vectors\\nand a query with an expanded set of term vectors.\\nIt then computes all-to-all (Cartesian) soft match\\nbetween the tokens. ColBERT performs interaction\\nas dot product followed pooling operations, whichCLS bank account SEP bank river bankCLS bank account SEP bank river bankCLS bank account SEP bank river bankCLS bank account SEP bank river bankscore(a) Cross-Attention Model (e.g., BERT reranker)\\nCLS bank account CLS bank river bankCLS bank accountCLS bank account\\nCLS bank river bankCLS bank river bankscore (b) Dense Retrievers (e.g., DPR)\\nCLS bank account CLS bank river bankCLS bank accountCLS bank account\\nCLS bank river bankCLS bank river bank\\nEXP EXPscore\\nmax max max\\nEXP\\nEXPEXP\\nEXPmax max\\n(c) ColBERT: All-to-All Match\\nCLS bank account CLS bank river bankCLS bank accountCLS bank account\\nCLS bank river bankCLS bank river bankdot maxsum (d) COIL: Contextualized Exact Match\\nFigure 1: An illustration of reranking/retrieval mechanisms with deep LM, including our proposed model, COIL.\\nBank\\nRiver\\nAccountBank\\nAccount\\nTraditional Inverted Lists Querydocid: 3\\ntf: 2docid: 9\\ntf: 1docid: 1\\ntf: 1docid: 2\\ntf: 1docid: 4\\ntf: 1docid: 5\\ntf: 2docid: 1\\ntf: 1docid: 3\\ntf: 1docid: 6\\ntf: 1....\\n....BM25\\xa0\\nscoringBM25\\xa0\\nscoring\\nBM25\\xa0\\nscoring\\nFigure 2: An illustration of traditional inverted lists.\\nThe inverted list maps a term to the list of documents\\nwhere the term occurs. Retriever looks up query terms’\\ninverted lists and scores those documents with stored\\nstatistics such as term frequency (tf).\\nallows it to also leverage a dense index to do full\\ncorpus retrieval. However, since ColBERT encodes\\na document with all tokens, it adds another order\\nof magnitude of index complexity to all aforemen-\\ntioned methods: document tokens in the collection\\nneed to be stored in a single huge index and con-\\nsidered at query time. Consequently, ColBERT is\\nengineering and hardware demanding.\\n3 Methodologies\\nIn this section, we ﬁrst provide some preliminaries\\non exact lexical match systems. Then we discuss\\nCOIL’s contextualized exact match design and how\\nits search index is organized. We also give a com-\\nparison between COIL and other popular retrievers.\\nBank\\nRiver\\nAccountBank\\nAccount\\nContextualized Inverted Lists Querydocid\\xa0 [1 3 6 7]\\ndocid\\xa0 [1 2 4 5 5 9]\\ndocid\\xa0 [3 3 9]vectors\\nvectors\\nvectorsCLSdocid\\xa0 [1 2 3 4 .............C]\\nvectors ...CLSmatrix\\nproduct\\nmatrix\\nproduct\\nmatrix\\nproductFigure 3: COIL’s index and retrieval architecture.\\nCOIL-tok relies on the exact token matching (lower).\\nCOIL-full includes in addition CLS matching (upper).\\n3.1 Preliminaries\\nClassic lexical retrieval system relies on overlap-\\nping query document terms under morphological\\ngeneralization like stemming, in other words, exact\\nlexical match , to score query document pair. A\\nscoring function is deﬁned as a sum of matched\\nterm scores. The scores are usually based on statis-\\ntics like term frequency ( tf). Generally, we', start_char_idx=73043, end_char_idx=76273, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='8f6b4ff5-7756-470a-84a3-3b208662bb7f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='71916901-659b-40c1-9f5d-8f7c231b736c', node_type=None, metadata={}, hash='5926c880fd5c3fee64db462c80af6bfd1935ece1132f0af9efa68d39251f9c31'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f121978d-88eb-48e3-8010-7fc87f6f2456', node_type=None, metadata={}, hash='853392e68385c30bb45cd5c89b5767a9a46622397e79a3a40ba646918f2b69fe')}, hash='52f7a75468e206bb3e4dc5778fe9a00d216f0daf6fd2cd5d972c3ce8a852e704', text='scores are usually based on statis-\\ntics like term frequency ( tf). Generally, we can\\nwrite,\\ns=X\\nt2q\\\\d\\x1bt(hq(q;t);hd(d;t)) (1)\\nwhere for each overlapping term tbetween query q\\nand document d, functionshqandhdextract terminformation and a term scoring function \\x1btcom-\\nbines them. A popular example is BM25, which\\ncomputes,\\nsBM25 =X\\nt2q\\\\didf(t)hBM25\\nq(q;t)hBM25\\nd(d;t)\\nhBM25\\nq(q;t) =tft;q(1 +k2)\\ntft;q+k2\\nhBM25\\nd(d;t) =tft;d(1 +k1)\\ntft;d+k1(1\\n2, 3, https://arxiv.org/pdf/2106.14807.pdf, A Few Brief Notes on DeepImpact, COIL, and a Conceptual\\nFramework for Information Retrieval Techniques\\nJimmy Lin andXueguang Ma\\nDavid R. Cheriton School of Computer Science\\nUniversity of Waterloo\\nAbstract\\nRecent developments in representational learn-\\ning for information retrieval can be organized\\nin a conceptual framework that establishes two\\npairs of contrasts: sparse vs. dense representa-\\ntions and unsupervised vs. learned representa-\\ntions. Sparse learned representations can fur-\\nther be decomposed into expansion and term\\nweighting components. This framework al-\\nlows us to understand the relationship between\\nrecently proposed techniques such as DPR,\\nANCE, DeepCT, DeepImpact, and COIL, and\\nfurthermore, gaps revealed by our analysis\\npoint to “low hanging fruit” in terms of tech-\\nniques that have yet to be explored. We present\\na novel technique dubbed “uniCOIL”, a simple\\nextension of COIL that achieves to our knowl-\\nedge the current state-of-the-art in sparse re-\\ntrieval on the popular MS MARCO passage\\nranking dataset. Our implementation using\\nthe Anserini IR toolkit is built on the Lucene\\nsearch library and thus fully compatible with\\nstandard inverted indexes.\\n1 Introduction\\nWe present a novel conceptual framework for un-\\nderstanding recent developments in information re-\\ntrieval that organizes techniques along two dimen-\\nsions. The ﬁrst dimension establishes the contrast\\nbetween sparse and dense vector representations\\nfor queries and documents.1The second dimen-\\nsion establishes the contrast between unsupervised\\nand learned (supervised) representations. Figure 1\\nillustrates our framework.\\nRecent proposals for dense retrieval, exempliﬁed\\nby DPR (Karpukhin et al., 2020) and ANCE (Xiong\\net al., 2021), but also encompassing many other\\ntechniques (Gao et al., 2021b; Hofstätter et al.,\\n2020; Qu et al., 2021; Hofstätter et al., 2021; Lin\\n1Consistent with parlance in information retrieval, we use\\n“document” throughout this paper in a generic sense to refer to\\nthe unit of retrieved text. To be more precise, our experiments\\nare in fact focused on passage retrieval.Dense Sparse\\nSupervised DPR, ANCE DeepImpact, COIL\\nUnsupervised LSI, LDA BM25, tf–idf\\nTable 1: Our conceptual framework for organizing re-\\ncent developments in information retrieval.\\net al., 2021), can be understood as learned dense\\nrepresentations for retrieval. This is formulated\\nas a representational learning problem where the\\ntask is to learn (transformer-based) encoders that\\nmap queries and documents into dense ﬁxed-width\\nvectors (768 dimensions is typical) in which inner\\nproducts between queries and relevant documents\\nare maximized, based on supervision signals from\\na large dataset such as the MS MARCO passage\\nranking test collection (Bajaj et al., 2018). See Lin\\net al. (2020)', start_char_idx=76259, end_char_idx=79528, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='f121978d-88eb-48e3-8010-7fc87f6f2456', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8f6b4ff5-7756-470a-84a3-3b208662bb7f', node_type=None, metadata={}, hash='52f7a75468e206bb3e4dc5778fe9a00d216f0daf6fd2cd5d972c3ce8a852e704'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='49ed6b5a-0ebb-4649-ac0c-2142be28065e', node_type=None, metadata={}, hash='4a1de18254e8fd4c75b5a6bffd5cf7cc4921e94763c94897d8a1013731d878e8')}, hash='853392e68385c30bb45cd5c89b5767a9a46622397e79a3a40ba646918f2b69fe', text='test collection (Bajaj et al., 2018). See Lin\\net al. (2020) for a survey.\\nDense retrieval techniques are typically com-\\npared against a bag-of-words exact match ranking\\nmodel such as BM25, which in this context can be\\nunderstood as unsupervised sparse retrieval. Al-\\nthough it may be unnatural to describe BM25 in\\nthis way, it is technically accurate: each document\\nis represented by a sparse vector where each dimen-\\nsion corresponds to a unique term in the vocabulary,\\nand the scoring function assigns a weight to each di-\\nmension. As with dense retrieval, query–document\\nscores are computed via inner products.\\nWhat about learned sparse retrieval? The most\\nprominent recent example of this in the literature\\nis DeepCT (Dai and Callan, 2019), which uses\\na transformer to learn term weights based on a re-\\ngression model, with the supervision signal coming\\nfrom the MS MARCO passage ranking test collec-\\ntion.2DeepCT has an interesting “quirk”: in truth,\\nit only learns the term frequency (tf) component\\nof term weights, but still relies on the remaining\\n2Learning sparse representations is by no means a new idea.\\nThe earliest example we are aware of is Wilbur (2001), who\\nattempted to learn global term weights using TREC data, but\\nthe idea likely dates back even further.arXiv:2106.14807v1  [cs.IR]  28 Jun 2021parts of the BM25 scoring function via the gen-\\neration of pseudo-documents. This approach also\\nhas a weakness: it only assigns weights to terms\\nthat are already present in the document, which\\nlimits retrieval to exact match. This is an impor-\\ntant limitation that is addressed by the use of dense\\nrepresentations, which are capable of capturing se-\\nmantic matches.\\nThese two issues were resolved by the recently\\nproposed DeepImpact model (Mallia et al., 2021),\\nwhich also belongs in the family of learned sparse\\nrepresentations. DeepImpact brought together two\\nkey ideas: the use of document expansion to iden-\\ntify dimensions in the sparse vector that should\\nhave non-zero weights and a term weighting model\\nbased on a pairwise loss between relevant and non-\\nrelevant texts with respect to a query. Expansion\\nterms were identiﬁed by doc2query–T5 (Nogueira\\nand Lin, 2019), a sequence-to-sequence model for\\ndocument expansion that predicts queries for which\\na text would be relevant. Since the DeepImpact\\nscoring model directly predicts term weights that\\nare then quantized, it would be more accurate to\\ncall these weights learned impacts, since query–\\ndocument scores are simply the sum of weights of\\ndocument terms that are found in the query. Calling\\nthese impact scores draws an explicit connection to\\na thread of research in information retrieval dating\\nback two decades (Anh et al., 2001).\\nThe recently proposed COIL architecture (Gao\\net al., 2021a) presents an interesting case for this\\nconceptual framework. Where does it belong? The\\nauthors themselves describe COIL as “a new ex-\\nact lexical match retrieval architecture armed with\\ndeep LM representations”. COIL produces repre-\\nsentations for each document token that are then\\ndirectly stored in the inverted index, where the\\nterm frequency usually goes in an inverted list.\\nAlthough COIL is perhaps best described as the\\nintellectual descendant of ColBERT (Khattab and\\nZaharia, 2020), another way to think about it within\\nour conceptual framework is that instead of assign-\\ningscalar weights to terms in a query, the “scoring”\\nmodel assigns each term a vector “weight”. Query\\nevaluation in COIL involves accumulating inner\\nproducts instead of scalar weights.\\nOur conceptual framework highlights a ﬁnal\\nclass of techniques: unsupervised dense represen-\\ntations. While there is little work in this space of\\nlate, it does describe techniques such as LSI (Deer-\\nwester et', start_char_idx=79548, end_char_idx=83291, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='49ed6b5a-0ebb-4649-ac0c-2142be28065e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f121978d-88eb-48e3-8010-7fc87f6f2456', node_type=None, metadata={}, hash='853392e68385c30bb45cd5c89b5767a9a46622397e79a3a40ba646918f2b69fe'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f20527a8-769f-4776-9e71-dd2d3bae2a40', node_type=None, metadata={}, hash='95554130094c1726c3a9ce4f56184e2da8794f4b7957b2bdc7b9a1ab3f04d69d')}, hash='4a1de18254e8fd4c75b5a6bffd5cf7cc4921e94763c94897d8a1013731d878e8', text='it does describe techniques such as LSI (Deer-\\nwester et al., 1990; Atreya and Elkan, 2010) andLDA (Wei and Croft, 2006), which have been previ-\\nously explored. Thus, all quadrants in our proposed\\nconceptual framework are populated with known\\nexamples from the literature.\\n2 Comments and Observations\\nBased on this framework, we can make a number of\\ninteresting observations that highlight obvious next\\nsteps in the development of retrieval techniques.\\nWe discuss as follows:\\nChoice of bases. Retrieval techniques using learned\\ndense representations and learned sparse represen-\\ntations present an interesting contrast. Nearly all\\nrecent proposals take advantage of transformers, so\\nthat aspect of the design is not a salient difference.\\nThe critical contrast is the basis of the vector rep-\\nresentations: In sparse approaches, the basis of the\\nvector space remains ﬁxed to the corpus vocabulary,\\nand thus techniques such as DeepCT, COIL, and\\nDeepImpact can be understood as term weighting\\nmodels. In dense approaches, the model is given\\nthe freedom to choose a new basis derived from\\ntransformer representations. This change in basis\\nallows the encoder to represent the “meaning” of\\ntexts in relatively small ﬁxed-width vectors (com-\\npared to sparse vectors that may have millions of\\ndimensions). This leads us to the next important\\nobservation:\\nExpansions for sparse representation. Without\\nsome form of expansion, learned sparse represen-\\ntations remain limited to (better) exact matching\\nbetween queries and documents. The nature of\\nsparse representations means that it is impractical\\nto consider non-zero weights for allelements in\\nthe vector (i.e., the vocabulary space). Thus, docu-\\nment expansion serves the critical role of proposing\\na set of candidate terms that should receive non-\\nzero weights; since the number of candidate terms\\nis small compared to the vocabulary size, the re-\\nsulting vector remains sparse. Without expansion,\\nlearned sparse representations cannot address the\\nvocabulary mismatch problem (Furnas et al., 1987),\\nbecause document terms not present in the query\\ncannot contribute any score. For DeepImpact, this\\nexpansion is performed by doc2query–T5, but in\\nprinciple we can imagine other methods also. This\\nleads us to the next important observation:\\nRelating DeepCT, DeepImpact, and COIL. The up-\\nshot of the above analysis is that retrieval tech-\\nniques based on learned sparse representations\\nshould be divided into an expansion model andSparse Representations MRR@10 Notes\\nTerm Weighting Expansion\\n(1a) BM25 None 0.184 copied from (Nogueira and Lin, 2019)\\n(1b) BM25 doc2query–T5 0.277 copied from (Nogueira and Lin, 2019)\\n(2a) DeepCT None 0.243 copied from (Dai and Callan, 2019)\\n(2b) DeepCT doc2query–T5 ? no publicly reported ﬁgure\\n(2c) DeepImpact None ? no publicly reported ﬁgure\\n(2d) DeepImpact doc2query–T5 0.326 copied from (Mallia et al., 2021)\\n(2e) COIL-tok ( d= 32 ) None 0.341 copied from (Gao et al., 2021a)\\n(2f) COIL-tok ( d= 32 ) doc2query–T5 0.361 our experiment\\n(2g) uniCOIL None 0.315 our experiment\\n(2h) uniCOIL doc2query–T5 0.352 our experiment\\nDense Representations MRR@10 Notes\\n(3a) ColBERT 0.360 copied from (Khattab and Zaharia, 2020)\\n(3b) ANCE 0.330 copied from (Xiong et al., 2021)\\n(3c) DistillBERT 0.323 copied from (Hofstätter et al., 2020)\\n(3d) RocketQA 0.370 copied from', start_char_idx=83294, end_char_idx=86629, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='f20527a8-769f-4776-9e71-dd2d3bae2a40', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='49ed6b5a-0ebb-4649-ac0c-2142be28065e', node_type=None, metadata={}, hash='4a1de18254e8fd4c75b5a6bffd5cf7cc4921e94763c94897d8a1013731d878e8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='680e3760-ff90-49eb-a5c9-e3d62328cba6', node_type=None, metadata={}, hash='b3c7e1ea242743558a68c25bbaa12e1b45ed363776b94d3bfe524b99557cb7cb')}, hash='95554130094c1726c3a9ce4f56184e2da8794f4b7957b2bdc7b9a1ab3f04d69d', text='et al., 2020)\\n(3d) RocketQA 0.370 copied from (Qu et al., 2021)\\n(3e) TAS-B 0.347 copied from (Hofstätter et al., 2021)\\n(3f) TCT-ColBERTv2 0.359 copied from (Lin et al., 2021)\\nDense–Sparse Hybrids MRR@10 Notes\\n(4a) CLEAR 0.338 copied from (Gao et al., 2021b)\\n(4b) COIL-full 0.355 copied from (Gao et al., 2021a)\\n(4c) TCT-ColBERTv2 + BM25 (1a) 0.369 copied from (Lin et al., 2021)\\n(4d) TCT-ColBERTv2 + doc2query–T5 (1b) 0.375 copied from (Lin et al., 2021)\\n(4e) TCT-ColBERTv2 + DeepImpact (2d) 0.378 our experiment\\n(4f) TCT-ColBERTv2 + uniCOIL (2h) 0.378 our experiment\\n(4g) TCT-ColBERTv2 + COIL (2f) 0.382 our experiment\\nTable 2: Results on the development queries of the MS MARCO passage ranking task.\\na term weighting model. For example, DeepCT\\nperforms no expansion and uses a regression-based\\nscoring model. DeepImpact performs document ex-\\npansion and uses a pairwise scoring model. COIL\\nperforms no expansion and uses a “scoring” model\\nthat generates a contextualized “weight vector” (in-\\nstead of a scalar weight). This breakdown suggests\\na number of obvious experiments that help us un-\\nderstand the contributions of these components,\\nwhich we report next.\\n3 Experiments\\nOur proposed conceptual framework can be used\\nto organize results from the literature, which are\\nshown in Table 2 on the development queries of\\nthe MS MARCO passage ranking task (Bajaj et al.,\\n2018). Some of these entries represent ﬁgures di-\\nrectly copied from previous papers (with references\\nshown), while others are novel experimental condi-\\ntions that we report.\\nThe ﬁrst main block of the table shows retrieval\\nwith sparse representations. Row (1a) shows the\\nBM25 baseline, and row (1b) provides the effective-\\nness of doc2query–T5 expansion. In both cases, the\\nterm weights are from the BM25 scoring function,and hence unsupervised. Learned sparse retrieval\\ntechniques are shown in row group (2). Separat-\\ning the term weighting component from the ex-\\npansion component allows us to identify gaps in\\nmodel conﬁgurations that would be interesting to\\nexplore. For example, in row (2a), DeepCT pro-\\nposed a regression-based term weighting model,\\nbut performed no expansion. However, the term\\nweighting model can be applied to expanded doc-\\numents, as in row (2b); to our knowledge, this\\nconﬁguration has not been publicly reported.\\nSimilarly, DeepImpact combined doc2query–T5\\nas an expansion model and a term weighting model\\ntrained with pairwise loss. To better understand\\nthe contributions of each component, we could\\nrun the term weighting model without document\\nexpansion, as outlined in row (2c). This ablation\\nexperiment was not reported in Mallia et al. (2021),\\nbut would be interesting to conduct.\\nIn row (2e) we report the published results of\\nCOIL-tok (token dimension d= 32 ), which is the\\nsparse component in the full COIL model (which\\nis a dense–sparse hybrid). Through the lens of\\nour conceptual framework, a number of extensions\\nbecome immediately obvious. COIL can be com-bined with doc2query–T5. Using source code pro-\\nvided by the authors,3we trained such a model\\nfrom scratch, using the same hyperparameters as\\nthe authors. This variant leads to a nearly two-point\\ngain in effectiveness, as shown in row (2f).\\nIn another interesting', start_char_idx=86639, end_char_idx=89872, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='680e3760-ff90-49eb-a5c9-e3d62328cba6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f20527a8-769f-4776-9e71-dd2d3bae2a40', node_type=None, metadata={}, hash='95554130094c1726c3a9ce4f56184e2da8794f4b7957b2bdc7b9a1ab3f04d69d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='dcfd296d-cd96-4256-990c-a7c548389675', node_type=None, metadata={}, hash='3819d651b4335957281fe912e0913144fc2c05c6e0c5d5fe28cc7df7f68d2c27')}, hash='b3c7e1ea242743558a68c25bbaa12e1b45ed363776b94d3bfe524b99557cb7cb', text='in effectiveness, as shown in row (2f).\\nIn another interesting extension, if we reduce the\\ntoken dimension of COIL to one, the model degen-\\nerates into producing scalar weights, which then\\nbecomes directly comparable to DeepCT, row (2a)\\nand the “no-expansion” variant of DeepImpact, row\\n(2c). These comparisons isolate the effects of differ-\\nent term weighting models. We dub this variant of\\nCOIL “uniCOIL”, on top of which we can also add\\ndoc2query–T5, which produces a fair comparison\\nto DeepImpact, row (2d). The original formulation\\nof COIL, even with a token dimension of one, is\\nnot directly amenable to retrieval using inverted\\nindexes because weights can be negative. To ad-\\ndress this issue, we added a ReLU operation on\\nthe output term weights of the base COIL model to\\nforce the model to generate non-negative weights.\\nOnce again, we retrained the model from scratch\\nusing the same hyperparameters provided by the\\nauthors. When encoding the corpus, we quantized\\nthese weights into 8 bits to obtain impact scores;\\nquery weights are similarly quantized. After these\\nmodiﬁcations, uniCOIL is directly compatible with\\ninverted indexes. Our experimental results are re-\\nported with the Anserini toolkit (Yang et al., 2017,\\n2018), which is built on Lucene.\\nIt is no surprise that uniCOIL without doc2query–\\nT5, row (2g), is less effective than COIL-tok ( d=\\n32), row (2e). However, uniCOIL with doc2query–\\nT5, row (2h), outperforms COIL-tok without need-\\ning any specialized retrieval infrastructure—the\\nweights are just impact scores, like in DeepImpact.\\nThese results suggest that contextualized “weight\\nvectors” in COIL aren’t necessary to achieve good\\neffectiveness—adding expansion appears sufﬁcient\\nto make up for the lost expressivity of weight vec-\\ntors, as shown in row (2h) vs. row (2e). To our\\nknowledge, our uniCOIL model, row (2h), repre-\\nsents the state of the art in sparse retrieval using\\nlearned impact weights, beating DeepImpact by\\naround two points.\\nThe second main block of Table 2 provides a\\nnumber of comparable dense retrieval results from\\nthe literature. The highest score that we are aware\\nof is RocketQA (Qu et al., 2021), whose effective-\\nness beats all known sparse conﬁgurations. Note\\n3https://github.com/luyug/COILthat ColBERT (Khattab and Zaharia, 2020) uses\\nthe more expressive MaxSim operator to compare\\nquery and document representations; all other tech-\\nniques use inner products.\\nThe ﬁnal block of Table 2 presents the results of\\ndense–sparse hybrids. Lin et al. (2021) reported\\nthe results of dense–sparse hybrids when TCT-\\nColBERTv2, row (3f), is combined with BM25,\\nrow (1a), and doc2query–T5, row (1b). To this,\\nwe added fusion with DeepImpact, uniCOIL, and\\nCOIL-tok (d= 32 ). For a fair comparison, we fol-\\nlowed the same technique for combining dense and\\nsparse results as Lin et al. (2021), which is from Ma\\net al. (2021). For each query q, we used the corre-\\nsponding dense and sparse techniques to retrieve\\ntop-1k documents. The ﬁnal fusion score of each\\ndocument is calculated by sdense +\\x0b\\x01ssparse . Since\\nthe range of the two different scores are quite differ-\\nent, we ﬁrst normalized the scores into range(0, 1).\\nThe\\x0bwas tuned in the range(0, 2) with a simple\\nline search on a subset of the MS MARCO passage\\ntraining set.\\nWith these hybrid combinations, we are able\\nto achieve, to our knowledge, the', start_char_idx=89858, end_char_idx=93211, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='dcfd296d-cd96-4256-990c-a7c548389675', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='680e3760-ff90-49eb-a5c9-e3d62328cba6', node_type=None, metadata={}, hash='b3c7e1ea242743558a68c25bbaa12e1b45ed363776b94d3bfe524b99557cb7cb'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='dd097f21-ea74-4b5b-a700-875c85769bca', node_type=None, metadata={}, hash='c93773b6df502f993fecc902eab654a270291c3ef4af16acdc334faaf0a7eed3')}, hash='3819d651b4335957281fe912e0913144fc2c05c6e0c5d5fe28cc7df7f68d2c27', text='these hybrid combinations, we are able\\nto achieve, to our knowledge, the highest reported\\nscores on the MS MARCO passage ranking task for\\nsingle-stage techniques (i.e., no reranking). Note\\nthat, as before, uniCOIL is compatible with stan-\\ndard inverted indexes, unlike COIL-tok, which re-\\nquires custom infrastructure.\\n4 Next Steps\\nIn most recent work, dense retrieval techniques are\\ncompared to BM25 and experiments show that they\\nhandily win. However, this is not a fair compari-\\nson, since BM25 is unsupervised, whereas dense\\nretrieval techniques exploit supervised relevance\\nsignals from large datasets. A more appropriate\\ncomparison would be between learned dense vs.\\nsparse representations—and there, no clear win-\\nner emerges at present. However, it seems clear\\nthat they are complementary, as hybrid approaches\\nappear to be more effective than either alone.\\nAn important point to make here is that neu-\\nral networks, particularly transformers, have not\\nmade sparse representations obsolete. Both dense\\nand sparse learned representations clearly exploit\\ntransformers—the trick is that the latter class of\\ntechniques then “projects” the learned knowledge\\nback into the sparse vocabulary space. This al-\\nlows us to reuse decades of innovation in inverted\\nindexes (e.g., integer coding techniques to com-press inverted lists) and efﬁcient query evaluation\\nalgorithms (e.g., smart skipping to reduce query\\nlatency): for example, the Lucene index used in\\nour uniCOIL experiments is only 1.3 GB, com-\\npared to \\x1840 GB for COIL-tok, 26 GB for TCT-\\nColBERTv2, and 154 GB for ColBERT. We note,\\nhowever, that with dense retrieval techniques, ﬁxed-\\nwidth vectors can be approximated with binary\\nhash codes, yielding far more compact representa-\\ntions with sacriﬁcing much effectiveness (Yamada\\net al., 2021). Once again, no clear winner emerges\\nat present.\\nThe complete design space of modern informa-\\ntion retrieval techniques requires proper accounting\\nof the tradeoffs between output quality (effective-\\nness), time (query latency), and space (index size).\\nHere, we have only focused on the ﬁrst aspect.\\nLearned representations for information retrieval\\nare clearly the future, but the advantages and dis-\\nadvantages of dense vs. sparse approaches along\\nthese dimensions are not yet fully understood. It’ll\\nbe exciting to see what comes next!\\n5 Acknowledgments\\nThis research was supported in part by the Canada\\nFirst Research Excellence Fund and the Natural Sci-\\nences and Engineering Research Council (NSERC)\\nof Canada. Computational resources were provided\\nby Compute Ontario and Compute Canada.\\nReferences\\nV o Ngoc Anh, Owen de Kretser, and Alistair Moffat.\\n2001. Vector-space ranking with effective early ter-\\nmination. In Proceedings of the 24th Annual Inter-\\nnational ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval (SIGIR 2001) ,\\npages 35–42, New Orleans, Louisiana.\\nAvinash Atreya and Charles Elkan. 2010. Latent se-\\nmantic indexing (LSI) fails for TREC collections.\\nSIGKDD Explorations , 12(2):5–10.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\\nMir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-\\nwary, and Tong Wang. 2018. MS MARCO: A Hu-\\nman Generated MAchine Reading COmprehension\\nDataset. arXiv:1611.09268v3 .\\nZhuyun Dai and Jamie Callan. 2019. Context-aware\\nsentence/passage term importance estimation', start_char_idx=93204, end_char_idx=96638, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='dd097f21-ea74-4b5b-a700-875c85769bca', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='dcfd296d-cd96-4256-990c-a7c548389675', node_type=None, metadata={}, hash='3819d651b4335957281fe912e0913144fc2c05c6e0c5d5fe28cc7df7f68d2c27'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2f4e0b80-6c3d-4e2c-a35b-5dc002ed3670', node_type=None, metadata={}, hash='ce60645b24a6968ba207a33749a183669c22bafb812aaf664cb16d9b198f145e')}, hash='c93773b6df502f993fecc902eab654a270291c3ef4af16acdc334faaf0a7eed3', text='Jamie Callan. 2019. Context-aware\\nsentence/passage term importance estimation for\\nﬁrst stage retrieval. arXiv:1910.10687 .\\nScott Deerwester, Susan T. Dumais, George W. Furnas,\\nThomas K. Landauer, and Richard Harshman. 1990.Indexing by latent semantic analysis. Journal of\\nthe Association for Information Science , 41(6):391–\\n407.\\nGeorge W. Furnas, Thomas K. Landauer, Louis M.\\nGomez, and Susan T. Dumais. 1987. The vo-\\ncabulary problem in human-system communication.\\nCommunications of the ACM , 30(11):964–971.\\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2021a.\\nCOIL: Revisit exact lexical match in information\\nretrieval with contextualized inverted list. In Pro-\\nceedings of the 2021 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies , pages\\n3030–3042.\\nLuyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Ben-\\njamin Van Durme, and Jamie Callan. 2021b. Com-\\nplementing lexical retrieval with semantic residual\\nembedding. In Proceedings of the 43rd European\\nConference on Information Retrieval (ECIR 2021),\\nPart I , pages 146–160.\\nSebastian Hofstätter, Sophia Althammer, Michael\\nSchröder, Mete Sertkan, and Allan Hanbury.\\n2020. Improving efﬁcient neural ranking mod-\\nels with cross-architecture knowledge distillation.\\narXiv:2010.02666 .\\nSebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong\\nYang, Jimmy Lin, and Allan Hanbury. 2021. Ef-\\nﬁciently teaching an effective dense retriever with\\nbalanced topic aware sampling. In Proceedings of\\nthe 44th Annual International ACM SIGIR Confer-\\nence on Research and Development in Information\\nRetrieval (SIGIR 2021) .\\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 6769–\\n6781.\\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Ef-\\nﬁcient and effective passage search via contextual-\\nized late interaction over BERT. In Proceedings of\\nthe 43rd International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval\\n(SIGIR 2020) , pages 39–48.\\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates.\\n2020. Pretrained transformers for text ranking:\\nBERT and beyond. arXiv:2010.06467 .\\nSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\\n2021. In-batch negatives for knowledge distillation\\nwith tightly-coupled teachers for dense retrieval. In\\nProceedings of the 6th Workshop on Representation\\nLearning for NLP .\\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy\\nLin. 2021. A replication study of dense passage re-\\ntriever. arXiv:2104.05740 .Antonio Mallia, Omar Khattab, Torsten Suel, and\\nNicola Tonellotto. 2021. Learning passage impacts\\nfor inverted indexes. In Proceedings of the 44th An-\\nnual International ACM SIGIR Conference on Re-\\nsearch and Development in Information Retrieval\\n(SIGIR 2021) .\\nRodrigo Nogueira and Jimmy Lin. 2019. From\\ndoc2query to docTTTTTquery.\\nYingqi Qu, Yuchen Ding, Jing', start_char_idx=96630, end_char_idx=99668, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='2f4e0b80-6c3d-4e2c-a35b-5dc002ed3670', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='dd097f21-ea74-4b5b-a700-875c85769bca', node_type=None, metadata={}, hash='c93773b6df502f993fecc902eab654a270291c3ef4af16acdc334faaf0a7eed3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='fceaa65c-3d06-470e-903d-9a5141950d75', node_type=None, metadata={}, hash='63f0751d62354d875aa6eebcfd3437f5902ca037030954b8b0672e23e5143ce8')}, hash='ce60645b24a6968ba207a33749a183669c22bafb812aaf664cb16d9b198f145e', text='docTTTTTquery.\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\\nand Haifeng Wang. 2021. RocketQA: An opti-\\nmized training approach to dense passage retrieval\\nfor open-domain question answering. In Proceed-\\nings of the 2021 Conference of the North Ameri-\\ncan Chapter of the Association for Computational\\nLinguistics: Human Language Technologies , pages\\n5835–5847.\\nXing Wei and W. Bruce Croft. 2006. LDA-based doc-\\nument models for ad-hoc retrieval. In Proceedings\\nof the 29th Annual International ACM SIGIR Con-\\nference on Research and Development in Informa-\\ntion Retrieval (SIGIR 2006) , pages 178–185, Seattle,\\nWashington.\\nW. John Wilbur. 2001. Global term weights for docu-\\nment retrieval learned from TREC data. Journal of\\nInformation Science , 27(5):303–310.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2021. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval. In Proceedings of the 9th International Con-\\nference on Learning Representations (ICLR 2021) .\\nIkuya Yamada, Akari Asai, and Hannaneh Ha-\\njishirzi. 2021. Efﬁcient passage retrieval with\\nhashing for open-domain question answering.\\narXiv:2106.00882 .\\nPeilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:\\nenabling the use of Lucene for information retrieval\\nresearch. In Proceedings of the 40th Annual Inter-\\nnational ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval (SIGIR 2017) ,\\npages 1253–1256, Tokyo, Japan.\\nPeilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini:\\nreproducible ranking baselines using Lucene. Jour-\\nnal of Data and Information Quality , 10(4):Article\\n16.\\n3, 4, https://arxiv.org/pdf/2301.03266.pdf, Doc2Query--: When Less is More\\nMitko Gospodinov1, Sean MacAvaney2, and Craig Macdonald2\\nUniversity of Glasgow\\n12024810G@student.gla.ac.uk\\n2{first}.{last}@glasgow.ac.uk\\nAbstract. Doc2Query — the process of expanding the content of a\\ndocument before indexing using a sequence-to-sequence model — has\\nemerged as a prominent technique for improving the ﬁrst-stage retrieval\\neﬀectivenessofsearchengines.However,sequence-to-sequencemodelsare\\nknown to be prone to “hallucinating” content that is not present in the\\nsource text. We argue that Doc2Query is indeed prone to hallucination,\\nwhich ultimately harms retrieval eﬀectiveness and inﬂates the index size.\\nIn this work, we explore techniques for ﬁltering out these harmful queries\\nprior to indexing. We ﬁnd that using a relevance model to remove poor-\\nquality queries can improve the retrieval eﬀectiveness of Doc2Query by\\nup to 16%, while simultaneously reducing mean query execution time by\\n23% and cutting the index size by 33%. We release the code, data, and\\na live demonstration to facilitate reproduction and further exploration.1\\n1 Introduction\\nNeural network models, particularly those based on contextualised language\\nmodels, have been shown to improve search eﬀectiveness [3]. While some ap-\\nproaches focus on re-ranking document sets from a ﬁrst-stage retrieval function\\nto improve precision [27], others aim to improve the ﬁrst stage itself [4]. In this\\nwork, we focus on', start_char_idx=99701, end_char_idx=102889, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='fceaa65c-3d06-470e-903d-9a5141950d75', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2f4e0b80-6c3d-4e2c-a35b-5dc002ed3670', node_type=None, metadata={}, hash='ce60645b24a6968ba207a33749a183669c22bafb812aaf664cb16d9b198f145e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0dcff9e0-a9b4-4b18-83da-4a378980faec', node_type=None, metadata={}, hash='a324a0557e937d62d61d9fcca373a54a5e81799ac51b8831eb40febc02fbdfbd')}, hash='63f0751d62354d875aa6eebcfd3437f5902ca037030954b8b0672e23e5143ce8', text='the ﬁrst stage itself [4]. In this\\nwork, we focus on one of these ﬁrst-stage approaches: Doc2Query [29]. This ap-\\nproach trains a sequence-to-sequence model (e.g., T5 [33]) to predict queries that\\nmay be relevant to a particular text. Then, when indexing, this model is used\\ntoexpandthe document by generating a collection of queries and appending\\nthem to the document. Though computationally expensive at index time [34],\\nthis approach has been shown to be remarkably eﬀective even when retrieving\\nusing simple lexical models like BM25 [28]. Numerous works have shown that\\nthe approach can produce a high-quality pool of results that are eﬀective for\\nsubsequent stages in the ranking pipeline [19,20,23,40].\\nHowever, sequence-to-sequence models are well-known to be prone to gener-\\nate content that does not reﬂect the input text – a defect known in literature\\nas “hallucination” [25]. We ﬁnd that existing Doc2Query models are no excep-\\ntion. Figure 1 provides example generated queries from the state-of-the-art T5\\nDoc2Query model [28]. In this example, we see that many of the generated\\nqueries cannot actually be answered by the source passage (score \\x141).\\n1https://github.com/terrierteam/pyterrier_doc2queryarXiv:2301.03266v3  [cs.IR]  27 Feb 20232 Gospodinov et al.\\nOriginal Passage: Barley (Hordeum vulgare L.), a\\nmember of the grass family, is a major cereal grain. It\\nwas one of the ﬁrst cultivated grains and is now grown\\nwidely. Barley grain is a staple in Tibetan cuisine and\\nwaseatenwidelybypeasantsinMedievalEurope.Bar-\\nley has also been used as animal fodder, as a source\\nof fermentable material for beer and certain distilled\\nbeverages,andasacomponentofvarioushealthfoods.Generated Queries: (1) where does barley originate\\nfrom\\x01(2) what is the name of the cereal grain used\\nin tibetan cooking? \\x01(3) what is barley used for \\x01(1)\\nwhat is barley in food \\x01(0) what is bare wheat \\x01(3)\\nwhat family of organisms is barley in \\x01(1) why is bar-\\nley important in tibetan diet \\x01(3) what is barley \\x01\\n(2) where is barley grown \\x01(1) where was barley ﬁrst\\ngrown and eaten \\x01(1) where was barley ﬁrst used ...\\nFig. 1.Example passage from MS MARCO and generated queries using the T5\\nDoc2Query model. The relevance of each query to the passage is scored by the au-\\nthors on a scale of 0–3 using the TREC Deep Learning passage relevance criteria.\\nBased on this observation, we hypothesise that retrieval performance of\\nDoc2Querywouldimproveifhallucinatedquerieswereremoved.Inthispaper,we\\nconduct experiments where we apply a new ﬁltering phase that aims to remove\\npoor queries prior to indexing. Given that this approach removes queries, we\\ncall the approach Doc2Query-- (Doc2Query-minus-minus). Rather than training\\na new model for this task, we identify that relevance models are already ﬁt for\\nthis purpose: they estimate how relevant a passage is to a query. We therefore\\nexplore ﬁltering strategies that make use of existing neural relevance models.\\nThrough experimentation on the MS MARCO dataset, we ﬁnd that our ﬁl-\\ntering approach can improve the retrieval eﬀectiveness of indexes built using\\nDoc2Query-- by up to 16%; less can indeed be more. Meanwhile, ﬁltering nat-\\nurally reduces the index size, lowering storage and query-time computational\\ncosts. Finally, we conduct an exploration of the index-time overheads', start_char_idx=102886, end_char_idx=106205, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='0dcff9e0-a9b4-4b18-83da-4a378980faec', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='fceaa65c-3d06-470e-903d-9a5141950d75', node_type=None, metadata={}, hash='63f0751d62354d875aa6eebcfd3437f5902ca037030954b8b0672e23e5143ce8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9db6b10c-23cd-4642-bf9a-3f8b3c044485', node_type=None, metadata={}, hash='c908391e8274cc7e29cb0e0ae01297f428b790e78d08bde44808c8742ea7c73b')}, hash='a324a0557e937d62d61d9fcca373a54a5e81799ac51b8831eb40febc02fbdfbd', text='Finally, we conduct an exploration of the index-time overheads introduced\\nbytheﬁlteringprocessandconcludethatthegainsfromﬁlteringmorethanmake\\nup for the additional time spent generating more queries. The approach also has\\na positive impact on the environmental costs of applying Doc2Query; the same\\nretrieval eﬀectiveness can be achieved with only about a third of the compu-\\ntational cost when indexing. To facilitate last-metre, last-mile, and complete\\nreproduction eﬀorts [36], we release the code, indices, and ﬁltering scores.1In\\nsummary, we contribute a technique to improve the eﬀectiveness and eﬃciency\\nof Doc2Query by ﬁltering out queries that do not reﬂect the original passage.\\n2 Related Work\\nThe classical lexical mismatch problem is a key one in information retrieval -\\ndocuments that do not contain the query terms may not be retrieved. In the\\nliterature, various approaches have addressed this: query reformulation – includ-\\ning stemming, query expansion models (e.g. Rocchio, Bo1 [1], RM3 [12]) – and\\ndocument expansion [9,30,35]. Classically, query expansion models have been\\npopular, as they avoid the costs associated with making additional processing\\nfor each document needed for document expansion. However, query expansion\\nmay result in reduced performance [11], as queries are typically short and the\\nnecessary evidence to understand the context of the user is limited.Doc2Query--: When Less is More 3\\nThe application of latent representations of queries and documents, such\\nas using latent semantic indexing [8] allow retrieval to not be driven directly\\nby lexical signals. More recently, transformer-based language models (such as\\nBERT [6]) have resulted in representations of text where the contextualised\\nmeaning of words are accounted for. In particular, in dense retrieval, queries\\nand documents are represented in embeddings spaces [14,37], often facilitated\\nby Approximate Nearest Neighbour (ANN) data structures [13]. However, even\\nwhen using ANN, retrieval can still be ineﬃcient or insuﬃciently eﬀective [15].\\nOthers have explored approaches for augmenting lexical representations with\\nadditional terms that may be relevant. In this work, we explore Doc2Query [29],\\nwhich uses a sequence-to-sequence model that maps a document to queries that\\nit might be able to answer. By appending these generated queries to a docu-\\nment’s content before indexing, the document is more likely to be retrieved for\\nuser queries when using a model like BM25. An alternative style of document\\nexpansion, proposed by MacAvaney et al. [19] and since used by several other\\nmodels (e.g., [10,39,40]), uses the built-in Masked Language Modelling (MLM)\\nmechanism. MLM expansion generates individual tokens to append to the docu-\\nment as a bag of words (rather than as a sequence). Although MLM expansion is\\nalso prone to hallucination,2the bag-of-words nature of MLM expansion means\\nthat individual expansion tokens may not have suﬃcient context to apply ﬁl-\\ntering eﬀectively. We therefore focus only on sequence-style expansion and leave\\nthe exploration of MLM expansion for future work.\\n3 Doc2Query--\\nDoc2Query-- consists of two phases: a generation phrase and a ﬁltering phase.\\nIn the generation phase, a Doc2Query model generates a set of nqueries that\\neach document might be able to answer. However, as shown in Figure 1, not\\nall of the queries are necessarily relevant to the document. To mitigate this\\nproblem, Doc2Query-- then proceeds to a ﬁltering phase, which is responsible\\nfor eliminating the generated queries that are least relevant to the source doc-\\nument. Because hallucinated queries contain details not present in the original\\ntext (by deﬁnition), we argue that hallucinated', start_char_idx=106195, end_char_idx=109900, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='9db6b10c-23cd-4642-bf9a-3f8b3c044485', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0dcff9e0-a9b4-4b18-83da-4a378980faec', node_type=None, metadata={}, hash='a324a0557e937d62d61d9fcca373a54a5e81799ac51b8831eb40febc02fbdfbd'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6288780d-f39f-41cb-a1b3-bd1b0686192a', node_type=None, metadata={}, hash='b4bcd80c9edc0dc17d82475ab235c32808a1c2a7d96c58337daaaba8306848ef')}, hash='c908391e8274cc7e29cb0e0ae01297f428b790e78d08bde44808c8742ea7c73b', text='the original\\ntext (by deﬁnition), we argue that hallucinated queries are less useful for re-\\ntrieval than non-hallucinated ones. Filtering is accomplished by retaining only\\nthe most relevant pproportion of generated queries over the entire corpus. The\\nretained queries are then concatenated to their corresponding documents prior\\nto indexing, as per the existing Doc2Query approach.\\nMore formally, consider an expansion function ethat maps a document to n\\nqueries: e:D7!Qn. In Doc2Query, each document in corpus Dare concate-\\nnatedwiththeirexpansionqueries,forminganewcorpus D0=fConcat (d; e(d))j\\nd2Dg,whichisthen indexedbya retrievalsystem.Doc2Query--addsaﬁltering\\nmechanism that uses a relevance model that maps a query and document to a\\nreal-valued relevance score s:Q\\x02D7!R(with larger values indicating higher\\n2For instance, we ﬁnd that SPLADE [10] generates the following seemingly-unrelated\\nterms for the passage in Figure 1 in the top 20 expansion terms: reed,herb, and troy.4 Gospodinov et al.\\nrelevance). The relevance scoring function is used to ﬁlter down the queries to\\nthose that meet a certain score threshold tas follows:\\nD0=n\\nConcat\\n4, 5, https://arxiv.org/pdf/2303.07678.pdf, Query2doc: Query Expansion with Large Language Models\\nLiang Wang and Nan Yang and Furu Wei\\nMicrosoft Research\\n{wangliang,nanya,fuwei}@microsoft.com\\nAbstract\\nThis paper introduces a simple yet effec-\\ntive query expansion approach, denoted as\\nquery2doc , to improve both sparse and dense\\nretrieval systems. The proposed method\\nﬁrst generates pseudo-documents by few-shot\\nprompting large language models (LLMs), and\\nthen expands the query with generated pseudo-\\ndocuments. LLMs are trained on web-scale\\ntext corpora and are adept at knowledge mem-\\norization. The pseudo-documents from LLMs\\noften contain highly relevant information that\\ncan aid in query disambiguation and guide\\nthe retrievers. Experimental results demon-\\nstrate that query2doc boosts the performance\\nof BM25 by 3% to 15% on ad-hoc IR datasets,\\nsuch as MS-MARCO and TREC DL, with-\\nout any model ﬁne-tuning. Furthermore, our\\nmethod also beneﬁts state-of-the-art dense re-\\ntrievers in terms of both in-domain and out-of-\\ndomain results.\\n1 Introduction\\nInformation retrieval (IR) aims to locate relevant\\ndocuments from a large corpus given a user is-\\nsued query. It is a core component in modern\\nsearch engines and researchers have invested for\\ndecades in this ﬁeld. There are two mainstream\\nparadigms for IR: lexical-based sparse retrieval,\\nsuch as BM25, and embedding-based dense re-\\ntrieval (Xiong et al., 2021; Qu et al., 2021). Al-\\nthough dense retrievers perform better when large\\namounts of labeled data are available (Karpukhin\\net al., 2020), BM25 remains competitive on out-of-\\ndomain datasets (Thakur et al., 2021).\\nQuery expansion (Rocchio, 1971; Lavrenko\\nand Croft, 2001) is a long-standing technique\\nthat rewrites the query based on pseudo-relevance\\nfeedback or external knowledge sources such as\\nWordNet. For sparse retrieval, it can help bridge\\nthe lexical gap between the query and the docu-\\nments. However, query expansion methods like\\nRM3 (Lavrenko and Croft, 2001; Lv and Zhai,2009) have only shown limited success on popular\\ndatasets (Campos et al., 2016), and most state-of-\\nthe-art dense retrievers do not adopt', start_char_idx=109902, end_char_idx=113189, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='6288780d-f39f-41cb-a1b3-bd1b0686192a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9db6b10c-23cd-4642-bf9a-3f8b3c044485', node_type=None, metadata={}, hash='c908391e8274cc7e29cb0e0ae01297f428b790e78d08bde44808c8742ea7c73b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d45ea90b-bb41-4d13-8a85-2e97854466d8', node_type=None, metadata={}, hash='3b989533d3c983d2d72d3d67ae32ff7047ca85b1deb3a5b85903518caecf833d')}, hash='b4bcd80c9edc0dc17d82475ab235c32808a1c2a7d96c58337daaaba8306848ef', text='and most state-of-\\nthe-art dense retrievers do not adopt this technique.\\nIn the meantime, document expansion methods like\\ndoc2query (Nogueira et al., 2019) have proven to\\nbe effective for sparse retrieval.\\nIn this paper, we demonstrate the effectiveness\\nof LLMs (Brown et al., 2020) as query expan-\\nsion models by generating pseudo-documents con-\\nditioned on few-shot prompts. Given that search\\nqueries are often short, ambiguous, or lack neces-\\nsary background information, LLMs can provide\\nrelevant information to guide retrieval systems, as\\nthey memorize an enormous amount of knowledge\\nand language patterns by pre-training on trillions\\nof tokens.\\nOur proposed method, called query2doc , gen-\\nerates pseudo-documents by few-shot prompting\\nLLMs and concatenates them with the original\\nquery to form a new query. This method is simple\\nto implement and does not require any changes in\\ntraining pipelines or model architectures, making it\\northogonal to the progress in the ﬁeld of LLMs and\\ninformation retrieval. Future methods can easily\\nbuild upon our query expansion framework.\\nFor in-domain evaluation, we adopt the MS-\\nMARCO passage ranking (Campos et al., 2016),\\nTREC DL 2019 and 2020 datasets. Pseudo-\\ndocuments are generated by prompting an im-\\nproved version of GPT-3 text-davinci-003 from\\nOpenAI (Brown et al., 2020). Results show that\\nquery2doc substantially improves the off-the-shelf\\nBM25 algorithm without ﬁne-tuning any model,\\nparticularly for hard queries from the TREC DL\\ntrack. Strong dense retrievers, including DPR\\n(Karpukhin et al., 2020), SimLM (Wang et al.,\\n2022a), and E5 (Wang et al., 2022b) also bene-\\nﬁt from query2doc , although the gains tend to be\\ndiminishing when distilling from a strong cross-\\nencoder based re-ranker. Experiments in zero-shot\\nOOD settings demonstrate that our method out-arXiv:2303.07678v1  [cs.IR]  14 Mar 2023performs strong baselines on most datasets. Fur-\\nther analysis also reveals the importance of model\\nscales: query2doc works best when combined with\\nthe most capable LLMs while small language mod-\\nels only provide marginal improvements over base-\\nlines.\\nTo aid reproduction, we release all\\nthe generations from text-davinci-003\\nat https://huggingface.co/datasets/\\nintfloat/query2doc_msmarco .\\n2 Method\\nWrite a passage that answers the given query:\\nQuery: what state is this zip code 85282\\nPassage: Welcome to TEMPE, AZ 85282. \\n85282 is a rural zip code in Tempe, Arizona. \\nThe population is primarily white…\\n…\\nQuery: when was pokemon green released\\nPassage:LLM Prompts\\nPokemon Green was released in Japan on \\nFebruary 27th, 1996. It was the first in the \\nPokemon series of games and served as the \\nbasis for Pokemon Red and Blue, which were \\nreleased in the US in 1998. The original \\nPokemon Green remains a beloved classic \\namong fans of the series.LLM Output\\nFigure 1: Illustration of query2doc few-shot prompting.\\nWe omit some in-context examples for space reasons.\\nGiven a query q, we employ few-shot prompting\\nto generate a pseudo-document d0as depicted in\\nFigure 1. The prompt comprises a brief instruction\\n“Write a passage that answers the given query:”\\nandklabeled pairs randomly sampled from a\\ntraining set. We use k= 4throughout this paper.\\nSubsequently, we rewrite qto a new query q+\\nby concatenating with the pseudo-document d0.\\nThere are slight differences in the concatenation\\noperation for sparse and dense retrievers, which\\nwe elaborate on in the following section.\\nSparse Retrieval Since', start_char_idx=113194, end_char_idx=116659, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='d45ea90b-bb41-4d13-8a85-2e97854466d8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d3ebb839-f393-431d-b500-d1ce023cf399', node_type=None, metadata={}, hash='6b17a06aad0a7d93b9b6f3e6024417f50fc5776a9c09d8623a3137b9590786a1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6288780d-f39f-41cb-a1b3-bd1b0686192a', node_type=None, metadata={}, hash='b4bcd80c9edc0dc17d82475ab235c32808a1c2a7d96c58337daaaba8306848ef')}, hash='3b989533d3c983d2d72d3d67ae32ff7047ca85b1deb3a5b85903518caecf833d', text='which\\nwe elaborate on in the following section.\\nSparse Retrieval Since the query qis typically\\nmuch shorter than pseudo-documents, we boost the\\nquery term weights by repeating the query ntimes\\nbefore concatenating with the pseudo-document d0:q+=concat(fqg\\x02n; d0) (1)\\nHere, “concat” denotes the string concatenation\\nfunction. q+is used as the new query for\\nBM25 retrieval. We ﬁnd that n= 5 is a gener-\\nally good value and do not tune it on a dataset basis.\\nDense Retrieval The new query q+is a sim-\\nple concatenation of the original query qand the\\npseudo-document d0separated by [SEP]:\\nq+=concat(q;[SEP]; d0) (2)\\nFor training dense retrievers, several factors can\\ninﬂuence the ﬁnal performance, such as hard nega-\\ntive mining (Xiong et al., 2021), intermediate pre-\\ntraining (Gao and Callan, 2021), and knowledge\\ndistillation from a cross-encoder based re-ranker\\n(Qu et al., 2021). In this paper, we investigate two\\nsettings to gain a more comprehensive understand-\\ning of our method. The ﬁrst setting is training DPR\\n(Karpukhin et al., 2020) models initialized from\\nBERT basewith BM25 hard negatives only. The op-\\ntimization objective is a standard contrastive loss:\\nLcont=', start_char_idx=116638, end_char_idx=117811, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# Necessary to use the latest OpenAI models that support function calling API\n",
    "service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo\"))\n",
    "\n",
    "#Index the Nodes\n",
    "index = GPTVectorStoreIndex(nodes, service_context=service_context)\n",
    "\n",
    "#An Index is a data structure that allows us to quickly retrieve relevant context for a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store th Index\n",
    "index.storage_context.persist(persist_dir=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
